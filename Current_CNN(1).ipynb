{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import *\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impact_score</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>5_Wave_Geopotential_Height_isobaric</th>\n",
       "      <th>Absolute_vorticity_isobaric</th>\n",
       "      <th>Apparent_temperature_height_above_ground</th>\n",
       "      <th>Best_4_layer_Lifted_Index_surface</th>\n",
       "      <th>Cloud_mixing_ratio_isobaric</th>\n",
       "      <th>Cloud_water_entire_atmosphere_single_layer</th>\n",
       "      <th>Convective_available_potential_energy_pressure_difference_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>day_22</th>\n",
       "      <th>day_23</th>\n",
       "      <th>day_24</th>\n",
       "      <th>day_25</th>\n",
       "      <th>day_26</th>\n",
       "      <th>day_27</th>\n",
       "      <th>day_28</th>\n",
       "      <th>day_29</th>\n",
       "      <th>day_30</th>\n",
       "      <th>day_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.268081</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5493.184570</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>275.700012</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20.268081</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5514.240723</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>275.399994</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>20.268081</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5544.382324</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>274.299988</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>20.268081</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5575.850586</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>275.700012</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>16.868994</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5614.513184</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>272.899994</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5565.216309</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>270.557709</td>\n",
       "      <td>14.358902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388971</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5563.294434</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>271.070496</td>\n",
       "      <td>10.099735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389061</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5541.568848</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>273.715454</td>\n",
       "      <td>7.515501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389151</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5532.193848</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>274.989288</td>\n",
       "      <td>11.738830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389241</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>5509.431152</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>276.816498</td>\n",
       "      <td>8.995741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4321 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        impact_score   lat   lng  5_Wave_Geopotential_Height_isobaric  \\\n",
       "0          20.268081  41.5 -71.0                          5493.184570   \n",
       "78         20.268081  41.5 -71.0                          5514.240723   \n",
       "156        20.268081  41.5 -71.0                          5544.382324   \n",
       "234        20.268081  41.5 -71.0                          5575.850586   \n",
       "312        16.868994  41.5 -71.0                          5614.513184   \n",
       "...              ...   ...   ...                                  ...   \n",
       "388881           NaN  41.5 -71.0                          5565.216309   \n",
       "388971           NaN  41.5 -71.0                          5563.294434   \n",
       "389061           NaN  41.5 -71.0                          5541.568848   \n",
       "389151           NaN  41.5 -71.0                          5532.193848   \n",
       "389241           NaN  41.5 -71.0                          5509.431152   \n",
       "\n",
       "        Absolute_vorticity_isobaric  Apparent_temperature_height_above_ground  \\\n",
       "0                          0.000099                                275.700012   \n",
       "78                         0.000094                                275.399994   \n",
       "156                        0.000093                                274.299988   \n",
       "234                        0.000093                                275.700012   \n",
       "312                        0.000086                                272.899994   \n",
       "...                             ...                                       ...   \n",
       "388881                     0.000186                                270.557709   \n",
       "388971                     0.000137                                271.070496   \n",
       "389061                     0.000115                                273.715454   \n",
       "389151                     0.000189                                274.989288   \n",
       "389241                     0.000207                                276.816498   \n",
       "\n",
       "        Best_4_layer_Lifted_Index_surface  Cloud_mixing_ratio_isobaric  \\\n",
       "0                               11.800000                          0.0   \n",
       "78                              10.700000                          0.0   \n",
       "156                             11.100000                          0.0   \n",
       "234                             19.100000                          0.0   \n",
       "312                             22.900000                          0.0   \n",
       "...                                   ...                          ...   \n",
       "388881                          14.358902                          0.0   \n",
       "388971                          10.099735                          0.0   \n",
       "389061                           7.515501                          0.0   \n",
       "389151                          11.738830                          0.0   \n",
       "389241                           8.995741                          0.0   \n",
       "\n",
       "        Cloud_water_entire_atmosphere_single_layer  \\\n",
       "0                                             0.19   \n",
       "78                                            0.65   \n",
       "156                                           0.00   \n",
       "234                                           0.00   \n",
       "312                                           0.00   \n",
       "...                                            ...   \n",
       "388881                                        0.82   \n",
       "388971                                        0.55   \n",
       "389061                                        1.33   \n",
       "389151                                        0.04   \n",
       "389241                                        0.00   \n",
       "\n",
       "        Convective_available_potential_energy_pressure_difference_layer  ...  \\\n",
       "0                                                    19.0                ...   \n",
       "78                                                    0.0                ...   \n",
       "156                                                   0.0                ...   \n",
       "234                                                   2.0                ...   \n",
       "312                                                   0.0                ...   \n",
       "...                                                   ...                ...   \n",
       "388881                                                5.0                ...   \n",
       "388971                                                0.0                ...   \n",
       "389061                                                1.0                ...   \n",
       "389151                                                0.0                ...   \n",
       "389241                                                0.0                ...   \n",
       "\n",
       "        day_22  day_23  day_24  day_25  day_26  day_27  day_28  day_29  \\\n",
       "0            0       0       0       0       0       0       0       0   \n",
       "78           0       0       0       0       0       0       0       0   \n",
       "156          0       0       0       0       0       0       0       0   \n",
       "234          0       0       0       0       0       0       0       0   \n",
       "312          0       0       0       0       0       0       0       0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "388881       0       0       0       0       0       0       0       0   \n",
       "388971       0       0       0       0       0       0       0       0   \n",
       "389061       0       0       0       0       0       0       0       0   \n",
       "389151       0       0       0       0       0       0       0       0   \n",
       "389241       0       0       0       0       0       0       0       0   \n",
       "\n",
       "        day_30  day_31  \n",
       "0            0       0  \n",
       "78           0       0  \n",
       "156          0       0  \n",
       "234          0       0  \n",
       "312          0       0  \n",
       "...        ...     ...  \n",
       "388881       1       0  \n",
       "388971       0       1  \n",
       "389061       0       1  \n",
       "389151       0       1  \n",
       "389241       0       1  \n",
       "\n",
       "[4321 rows x 168 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('no_zip.csv')\n",
    "data = data.drop(['Unnamed: 0'], axis = 1)\n",
    "first = data[data['zip5'] == 2722]\n",
    "first = first.drop(['zip5', 'date_key'], axis =1)\n",
    "first\n",
    "#y = data['impact_score']\n",
    "#X = data.drop(['date_key', 'impact_score'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_17_18 = first[first['year'] != 2019]\n",
    "y = first_17_18['impact_score']\n",
    "X = first_17_18.drop(['impact_score'], axis = 1)\n",
    "X = np.expand_dims(X, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 140, 4)            116       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 70, 4)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 43, 4)             452       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 43, 4)             0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 172)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 173       \n",
      "=================================================================\n",
      "Total params: 741\n",
      "Trainable params: 741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2003 samples, validate on 859 samples\n",
      "Epoch 1/100\n",
      "2003/2003 [==============================] - 13s 7ms/step - loss: 1947038.2226 - mae: 250.4501 - val_loss: 1302.1718 - val_mae: 30.9781\n",
      "Epoch 2/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 99766.0995 - mae: 47.7141 - val_loss: 253.2410 - val_mae: 15.7910\n",
      "Epoch 3/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 366677.6654 - mae: 40.2663 - val_loss: 303.0391 - val_mae: 17.0368\n",
      "Epoch 4/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3415.9261 - mae: 22.2645 - val_loss: 272.3324 - val_mae: 16.3276\n",
      "Epoch 5/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 1439.5657 - mae: 18.4212 - val_loss: 242.4255 - val_mae: 15.4397\n",
      "Epoch 6/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 1291.9828 - mae: 15.7666 - val_loss: 232.4920 - val_mae: 15.0715\n",
      "Epoch 7/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 303.7244 - mae: 14.0429 - val_loss: 223.5612 - val_mae: 14.7445\n",
      "Epoch 8/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 228.3171 - mae: 13.2253 - val_loss: 220.9591 - val_mae: 14.6746\n",
      "Epoch 9/100\n",
      "2003/2003 [==============================] - 7s 3ms/step - loss: 4459.7034 - mae: 14.6614 - val_loss: 211.4269 - val_mae: 14.3365\n",
      "Epoch 10/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 478.0484 - mae: 11.3368 - val_loss: 70.4682 - val_mae: 8.1654\n",
      "Epoch 11/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 45.6604 - mae: 4.8058 - val_loss: 8.8813 - val_mae: 2.4414\n",
      "Epoch 12/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 23.9042 - mae: 3.3085 - val_loss: 5.6659 - val_mae: 1.8682\n",
      "Epoch 13/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 57.7537 - mae: 3.2315 - val_loss: 4.8269 - val_mae: 1.7145\n",
      "Epoch 14/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 31097.4630 - mae: 7.0573 - val_loss: 3.9157 - val_mae: 1.5393\n",
      "Epoch 15/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 305.5127 - mae: 3.3751 - val_loss: 3.9816 - val_mae: 1.5492\n",
      "Epoch 16/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 13.5291 - mae: 2.8796 - val_loss: 4.0362 - val_mae: 1.5574\n",
      "Epoch 17/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 17.1873 - mae: 2.9316 - val_loss: 4.0106 - val_mae: 1.5525\n",
      "Epoch 18/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 2351.0893 - mae: 4.6661 - val_loss: 4.1823 - val_mae: 1.5835\n",
      "Epoch 19/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7435.2786 - mae: 4.8948 - val_loss: 3.6733 - val_mae: 1.4998\n",
      "Epoch 20/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 12.6815 - mae: 2.8117 - val_loss: 3.6803 - val_mae: 1.5003\n",
      "Epoch 21/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 44.2010 - mae: 2.9033 - val_loss: 3.6442 - val_mae: 1.4934\n",
      "Epoch 22/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 142.7334 - mae: 3.1137 - val_loss: 3.7587 - val_mae: 1.5082\n",
      "Epoch 23/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 13.1665 - mae: 2.8575 - val_loss: 3.7021 - val_mae: 1.4959\n",
      "Epoch 24/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 12.5116 - mae: 2.8043 - val_loss: 3.6193 - val_mae: 1.4773\n",
      "Epoch 25/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 12.3172 - mae: 2.7785 - val_loss: 3.4178 - val_mae: 1.4399\n",
      "Epoch 26/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 11.5654 - mae: 2.7076 - val_loss: 3.4452 - val_mae: 1.4359\n",
      "Epoch 27/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 14.7546 - mae: 2.7416 - val_loss: 3.2084 - val_mae: 1.3902\n",
      "Epoch 28/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 10.9653 - mae: 2.6041 - val_loss: 3.2185 - val_mae: 1.3909\n",
      "Epoch 29/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 25.0035 - mae: 2.7092 - val_loss: 3.1637 - val_mae: 1.3817\n",
      "Epoch 30/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 37.5202 - mae: 2.7785 - val_loss: 2.9832 - val_mae: 1.3428\n",
      "Epoch 31/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 10.3219 - mae: 2.5645 - val_loss: 2.9481 - val_mae: 1.3347\n",
      "Epoch 32/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 14.1387 - mae: 2.5645 - val_loss: 3.0693 - val_mae: 1.3644\n",
      "Epoch 33/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 11.4352 - mae: 2.5907 - val_loss: 2.8998 - val_mae: 1.3231\n",
      "Epoch 34/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 57.4869 - mae: 2.6659 - val_loss: 3.2184 - val_mae: 1.3926\n",
      "Epoch 35/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 9.3495 - mae: 2.3997 - val_loss: 2.9447 - val_mae: 1.3263\n",
      "Epoch 36/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 1091.6599 - mae: 3.1858 - val_loss: 3.0039 - val_mae: 1.3524\n",
      "Epoch 37/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.4566 - mae: 2.4476 - val_loss: 2.9600 - val_mae: 1.3369\n",
      "Epoch 38/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.1427 - mae: 2.4016 - val_loss: 2.8458 - val_mae: 1.3072\n",
      "Epoch 39/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.7735 - mae: 2.3385 - val_loss: 2.8558 - val_mae: 1.3085\n",
      "Epoch 40/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.4410 - mae: 2.3741 - val_loss: 2.7745 - val_mae: 1.2891\n",
      "Epoch 41/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.4898 - mae: 2.3407 - val_loss: 2.7498 - val_mae: 1.2807\n",
      "Epoch 42/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.4359 - mae: 2.2926 - val_loss: 2.7366 - val_mae: 1.2748\n",
      "Epoch 43/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.8134 - mae: 2.3446 - val_loss: 2.7020 - val_mae: 1.2674\n",
      "Epoch 44/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 9.6827 - mae: 2.3326 - val_loss: 2.7354 - val_mae: 1.2733\n",
      "Epoch 45/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 8.3688 - mae: 2.3225 - val_loss: 2.5908 - val_mae: 1.2405\n",
      "Epoch 46/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.2904 - mae: 2.2755 - val_loss: 2.6331 - val_mae: 1.2554\n",
      "Epoch 47/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.5414 - mae: 2.1666 - val_loss: 2.5448 - val_mae: 1.2329\n",
      "Epoch 48/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.4160 - mae: 2.1456 - val_loss: 2.5323 - val_mae: 1.2304\n",
      "Epoch 49/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 20.8700 - mae: 2.1559 - val_loss: 2.6704 - val_mae: 1.2707\n",
      "Epoch 50/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7.5454 - mae: 2.1629 - val_loss: 2.4914 - val_mae: 1.2182\n",
      "Epoch 51/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.6671 - mae: 2.0413 - val_loss: 2.5905 - val_mae: 1.2365\n",
      "Epoch 52/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 34112.5115 - mae: 7.1185 - val_loss: 2.8885 - val_mae: 1.3226\n",
      "Epoch 53/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.5544 - mae: 2.1870 - val_loss: 3.0204 - val_mae: 1.3672\n",
      "Epoch 54/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.6083 - mae: 2.1483 - val_loss: 2.8696 - val_mae: 1.3250\n",
      "Epoch 55/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.2629 - mae: 2.1336 - val_loss: 2.7423 - val_mae: 1.2914\n",
      "Epoch 56/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.2102 - mae: 2.1000 - val_loss: 2.6439 - val_mae: 1.2649\n",
      "Epoch 57/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7.1420 - mae: 2.0831 - val_loss: 2.5274 - val_mae: 1.2291\n",
      "Epoch 58/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.6208 - mae: 2.0323 - val_loss: 2.4927 - val_mae: 1.2162\n",
      "Epoch 59/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 2036.9164 - mae: 3.2169 - val_loss: 2.9697 - val_mae: 1.3359\n",
      "Epoch 60/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.6164 - mae: 2.1513 - val_loss: 2.7253 - val_mae: 1.2836\n",
      "Epoch 61/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 6.5334 - mae: 2.0390 - val_loss: 2.5598 - val_mae: 1.2440\n",
      "Epoch 62/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 6.8148 - mae: 2.0563 - val_loss: 2.5526 - val_mae: 1.2413\n",
      "Epoch 63/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 7.0621 - mae: 2.0898 - val_loss: 2.6039 - val_mae: 1.2474\n",
      "Epoch 64/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 6.2413 - mae: 1.9517 - val_loss: 2.5867 - val_mae: 1.2415\n",
      "Epoch 65/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 6.5202 - mae: 2.0213 - val_loss: 2.5733 - val_mae: 1.2371\n",
      "Epoch 66/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 6.4942 - mae: 1.9997 - val_loss: 2.5588 - val_mae: 1.2267\n",
      "Epoch 67/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 6.0648 - mae: 1.9405 - val_loss: 2.6027 - val_mae: 1.2369\n",
      "Epoch 68/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 6.3638 - mae: 1.9797 - val_loss: 2.5128 - val_mae: 1.2135\n",
      "Epoch 69/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 5.9420 - mae: 1.9061 - val_loss: 2.3791 - val_mae: 1.1822\n",
      "Epoch 70/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 5.6238 - mae: 1.8635 - val_loss: 2.4211 - val_mae: 1.1945\n",
      "Epoch 71/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.9123 - mae: 1.9197 - val_loss: 2.4201 - val_mae: 1.1953\n",
      "Epoch 72/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 5.6598 - mae: 1.8746 - val_loss: 2.4639 - val_mae: 1.2025\n",
      "Epoch 73/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 5.2538 - mae: 1.8082 - val_loss: 2.3157 - val_mae: 1.1701\n",
      "Epoch 74/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 3880.0428 - mae: 3.1987 - val_loss: 2.3931 - val_mae: 1.2047\n",
      "Epoch 75/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.3651 - mae: 1.8369 - val_loss: 2.3727 - val_mae: 1.1894\n",
      "Epoch 76/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 5.4245 - mae: 1.8249 - val_loss: 2.3336 - val_mae: 1.1724\n",
      "Epoch 77/100\n",
      "2003/2003 [==============================] - 8s 4ms/step - loss: 5.1708 - mae: 1.7883 - val_loss: 2.3543 - val_mae: 1.1774\n",
      "Epoch 78/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 11.1285 - mae: 1.8203 - val_loss: 2.3529 - val_mae: 1.1725\n",
      "Epoch 79/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.0290 - mae: 1.7618 - val_loss: 2.3758 - val_mae: 1.1818\n",
      "Epoch 80/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 5.0865 - mae: 1.7658 - val_loss: 2.5301 - val_mae: 1.2303\n",
      "Epoch 81/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 249.9692 - mae: 2.0419 - val_loss: 2.4198 - val_mae: 1.2121\n",
      "Epoch 82/100\n",
      "2003/2003 [==============================] - 8s 4ms/step - loss: 5.1891 - mae: 1.7905 - val_loss: 2.4398 - val_mae: 1.2192\n",
      "Epoch 83/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 5.2335 - mae: 1.7925 - val_loss: 2.3818 - val_mae: 1.1933\n",
      "Epoch 84/100\n",
      "2003/2003 [==============================] - 8s 4ms/step - loss: 4.9608 - mae: 1.7425 - val_loss: 2.4320 - val_mae: 1.2084\n",
      "Epoch 85/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 415.1962 - mae: 2.2460 - val_loss: 2.5677 - val_mae: 1.2490\n",
      "Epoch 86/100\n",
      "2003/2003 [==============================] - 14s 7ms/step - loss: 5.2725 - mae: 1.8077 - val_loss: 2.4938 - val_mae: 1.2251\n",
      "Epoch 87/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 11.4763 - mae: 1.8308 - val_loss: 2.5483 - val_mae: 1.2441\n",
      "Epoch 88/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.1234 - mae: 1.7801 - val_loss: 2.4979 - val_mae: 1.2300\n",
      "Epoch 89/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.2711 - mae: 1.7972 - val_loss: 2.5144 - val_mae: 1.2376\n",
      "Epoch 90/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.8218 - mae: 1.7117 - val_loss: 2.4785 - val_mae: 1.2258\n",
      "Epoch 91/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.8700 - mae: 1.7426 - val_loss: 2.4098 - val_mae: 1.2041\n",
      "Epoch 92/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 4.7323 - mae: 1.7054 - val_loss: 2.3241 - val_mae: 1.1760\n",
      "Epoch 93/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 4.7099 - mae: 1.6890 - val_loss: 2.3254 - val_mae: 1.1717\n",
      "Epoch 94/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.4741 - mae: 1.6442 - val_loss: 2.3162 - val_mae: 1.1697\n",
      "Epoch 95/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.2689 - mae: 1.6148 - val_loss: 2.2976 - val_mae: 1.1640\n",
      "Epoch 96/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.4085 - mae: 1.6377 - val_loss: 2.2621 - val_mae: 1.1528\n",
      "Epoch 97/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 4.1463 - mae: 1.5850 - val_loss: 2.2155 - val_mae: 1.1374\n",
      "Epoch 98/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 3.8693 - mae: 1.5341 - val_loss: 2.2575 - val_mae: 1.1491\n",
      "Epoch 99/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 3.9535 - mae: 1.5566 - val_loss: 2.2009 - val_mae: 1.1300\n",
      "Epoch 100/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.8155 - mae: 1.5092 - val_loss: 2.2535 - val_mae: 1.1458\n",
      "\n",
      "\n",
      "actual\tpredicted\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-17fbd9808a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nactual'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predicted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#print('next', model.predict(q).squeeze(), sep='\\t')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=28, activation=\"relu\", filters=4),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=28, activation=\"relu\", filters=4),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),     # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "#test_size = int(0.2 * 256921)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "#X_train, X_test, y_train, y_test = test[:-test_size], test[-test_size:], impact[:-test_size], impact[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=2, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fd7855bebe0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd785572198>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_14 (Conv1D)           (None, 140, 4)            116       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 70, 4)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 70, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 43, 4)             452       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 43, 4)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 172)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 173       \n",
      "=================================================================\n",
      "Total params: 741\n",
      "Trainable params: 741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2003 samples, validate on 859 samples\n",
      "Epoch 1/100\n",
      "2003/2003 [==============================] - 14s 7ms/step - loss: 5222597.5419 - mae: 1131.8308 - val_loss: 1016.1759 - val_mae: 24.4133\n",
      "Epoch 2/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 445403.1579 - mae: 165.0162 - val_loss: 251.7175 - val_mae: 15.6234\n",
      "Epoch 3/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 440866.6647 - mae: 59.8751 - val_loss: 252.7013 - val_mae: 15.7739\n",
      "Epoch 4/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6420.6025 - mae: 34.3858 - val_loss: 251.5315 - val_mae: 15.7368\n",
      "Epoch 5/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9984.6226 - mae: 31.4600 - val_loss: 248.9738 - val_mae: 15.6553\n",
      "Epoch 6/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3484.3764 - mae: 23.1915 - val_loss: 246.3421 - val_mae: 15.5711\n",
      "Epoch 7/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 10432.2785 - mae: 22.6985 - val_loss: 240.9494 - val_mae: 15.3969\n",
      "Epoch 8/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 459.6852 - mae: 16.6245 - val_loss: 234.8785 - val_mae: 15.1985\n",
      "Epoch 9/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 2181.9494 - mae: 16.9147 - val_loss: 225.5693 - val_mae: 14.8891\n",
      "Epoch 10/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 254.1919 - mae: 14.3765 - val_loss: 215.0582 - val_mae: 14.5318\n",
      "Epoch 11/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 272.3430 - mae: 14.0247 - val_loss: 200.2752 - val_mae: 14.0140\n",
      "Epoch 12/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 203.9592 - mae: 12.9682 - val_loss: 182.9056 - val_mae: 13.3799\n",
      "Epoch 13/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 156.0190 - mae: 11.3011 - val_loss: 61.6976 - val_mae: 7.6084\n",
      "Epoch 14/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 40.7954 - mae: 4.4724 - val_loss: 5.8232 - val_mae: 1.9180\n",
      "Epoch 15/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 18.5168 - mae: 2.9956 - val_loss: 3.9030 - val_mae: 1.5550\n",
      "Epoch 16/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 13.7304 - mae: 2.7606 - val_loss: 3.9555 - val_mae: 1.5645\n",
      "Epoch 17/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 84.3213 - mae: 2.9817 - val_loss: 3.9506 - val_mae: 1.5624\n",
      "Epoch 18/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 14.9885 - mae: 2.6827 - val_loss: 3.8373 - val_mae: 1.5425\n",
      "Epoch 19/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 197.3083 - mae: 2.9054 - val_loss: 4.0705 - val_mae: 1.5895\n",
      "Epoch 20/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 10.4025 - mae: 2.5845 - val_loss: 3.9000 - val_mae: 1.5558\n",
      "Epoch 21/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 12.0185 - mae: 2.6259 - val_loss: 3.7900 - val_mae: 1.5341\n",
      "Epoch 22/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 9.6403 - mae: 2.4491 - val_loss: 3.7901 - val_mae: 1.5325\n",
      "Epoch 23/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 10.1266 - mae: 2.5471 - val_loss: 3.7291 - val_mae: 1.5237\n",
      "Epoch 24/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 9.3960 - mae: 2.4421 - val_loss: 3.6823 - val_mae: 1.5174\n",
      "Epoch 25/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 31.0398 - mae: 2.5281 - val_loss: 3.9319 - val_mae: 1.5593\n",
      "Epoch 26/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 9.2727 - mae: 2.4141 - val_loss: 3.7517 - val_mae: 1.5210\n",
      "Epoch 27/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 8.8178 - mae: 2.3570 - val_loss: 3.7722 - val_mae: 1.5239\n",
      "Epoch 28/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.6092 - mae: 2.3163 - val_loss: 3.6366 - val_mae: 1.4998\n",
      "Epoch 29/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7.9045 - mae: 2.2371 - val_loss: 3.5945 - val_mae: 1.4901\n",
      "Epoch 30/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.0125 - mae: 2.2507 - val_loss: 3.6505 - val_mae: 1.4982\n",
      "Epoch 31/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 11.2666 - mae: 2.3285 - val_loss: 3.4979 - val_mae: 1.4665\n",
      "Epoch 32/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.9679 - mae: 2.2217 - val_loss: 3.5448 - val_mae: 1.4764\n",
      "Epoch 33/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7.2962 - mae: 2.1499 - val_loss: 3.4696 - val_mae: 1.4584\n",
      "Epoch 34/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7.4881 - mae: 2.1398 - val_loss: 3.4055 - val_mae: 1.4430\n",
      "Epoch 35/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 6.9908 - mae: 2.1121 - val_loss: 3.3341 - val_mae: 1.4297\n",
      "Epoch 36/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 7.8354 - mae: 2.2045 - val_loss: 3.2883 - val_mae: 1.4205\n",
      "Epoch 37/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.7381 - mae: 2.0663 - val_loss: 3.1728 - val_mae: 1.3956\n",
      "Epoch 38/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.8502 - mae: 2.0569 - val_loss: 3.1492 - val_mae: 1.3791\n",
      "Epoch 39/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 6.1584 - mae: 1.9656 - val_loss: 2.9385 - val_mae: 1.3233\n",
      "Epoch 40/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.3896 - mae: 1.9771 - val_loss: 2.8456 - val_mae: 1.2990\n",
      "Epoch 41/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.8150 - mae: 1.9109 - val_loss: 2.5801 - val_mae: 1.2292\n",
      "Epoch 42/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 82185.9722 - mae: 8.4463 - val_loss: 2.7343 - val_mae: 1.2733\n",
      "Epoch 43/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 12.0685 - mae: 2.0038 - val_loss: 2.6875 - val_mae: 1.2645\n",
      "Epoch 44/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.2953 - mae: 1.9878 - val_loss: 2.6886 - val_mae: 1.2638\n",
      "Epoch 45/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 5.6475 - mae: 1.8744 - val_loss: 2.6162 - val_mae: 1.2439\n",
      "Epoch 46/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 6.7925 - mae: 1.9041 - val_loss: 2.6048 - val_mae: 1.2422\n",
      "Epoch 47/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.7341 - mae: 1.8743 - val_loss: 2.5739 - val_mae: 1.2312\n",
      "Epoch 48/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 5.9541 - mae: 1.9233 - val_loss: 2.5316 - val_mae: 1.2177\n",
      "Epoch 49/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 6.2236 - mae: 1.9497 - val_loss: 2.5734 - val_mae: 1.2297\n",
      "Epoch 50/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.6539 - mae: 1.8674 - val_loss: 2.5269 - val_mae: 1.2154\n",
      "Epoch 51/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.6401 - mae: 1.8902 - val_loss: 2.5239 - val_mae: 1.2139\n",
      "Epoch 52/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 5.5075 - mae: 1.8517 - val_loss: 2.5253 - val_mae: 1.2146\n",
      "Epoch 53/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 5.5627 - mae: 1.8575 - val_loss: 2.5402 - val_mae: 1.2201\n",
      "Epoch 54/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 5.2375 - mae: 1.8045 - val_loss: 2.5391 - val_mae: 1.2183\n",
      "Epoch 55/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 5.1134 - mae: 1.7872 - val_loss: 2.5472 - val_mae: 1.2229\n",
      "Epoch 56/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.1724 - mae: 1.8147 - val_loss: 2.4712 - val_mae: 1.1981\n",
      "Epoch 57/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.9915 - mae: 1.7617 - val_loss: 2.4723 - val_mae: 1.1994\n",
      "Epoch 58/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.7956 - mae: 1.7046 - val_loss: 2.6046 - val_mae: 1.2468\n",
      "Epoch 59/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.9861 - mae: 1.7355 - val_loss: 2.4233 - val_mae: 1.1901\n",
      "Epoch 60/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.6554 - mae: 1.6941 - val_loss: 2.4206 - val_mae: 1.1868\n",
      "Epoch 61/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 4.5508 - mae: 1.6808 - val_loss: 2.4180 - val_mae: 1.1926\n",
      "Epoch 62/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 4.5602 - mae: 1.6623 - val_loss: 2.4076 - val_mae: 1.1906\n",
      "Epoch 63/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 4.0183 - mae: 1.5534 - val_loss: 2.3648 - val_mae: 1.1783\n",
      "Epoch 64/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 4.1169 - mae: 1.5761 - val_loss: 2.3465 - val_mae: 1.1723\n",
      "Epoch 65/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.1910 - mae: 1.6118 - val_loss: 2.2821 - val_mae: 1.1545\n",
      "Epoch 66/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 51801.5576 - mae: 6.6923 - val_loss: 2.3978 - val_mae: 1.1824\n",
      "Epoch 67/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 31681.1232 - mae: 5.5687 - val_loss: 3.0007 - val_mae: 1.3698\n",
      "Epoch 68/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.6424 - mae: 1.6994 - val_loss: 2.6507 - val_mae: 1.2719\n",
      "Epoch 69/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.4030 - mae: 1.6459 - val_loss: 2.5488 - val_mae: 1.2419\n",
      "Epoch 70/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 4.6373 - mae: 1.6906 - val_loss: 2.5037 - val_mae: 1.2267\n",
      "Epoch 71/100\n",
      "2003/2003 [==============================] - 8s 4ms/step - loss: 199.2508 - mae: 1.9575 - val_loss: 2.5151 - val_mae: 1.2302\n",
      "Epoch 72/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 4.3387 - mae: 1.6212 - val_loss: 2.4719 - val_mae: 1.2143\n",
      "Epoch 73/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.4110 - mae: 1.6384 - val_loss: 2.4597 - val_mae: 1.2107\n",
      "Epoch 74/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.1383 - mae: 1.5919 - val_loss: 2.4257 - val_mae: 1.1960\n",
      "Epoch 75/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 4.2047 - mae: 1.6180 - val_loss: 2.4171 - val_mae: 1.1962\n",
      "Epoch 76/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 4.1631 - mae: 1.6047 - val_loss: 2.3833 - val_mae: 1.1838\n",
      "Epoch 77/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.9904 - mae: 1.5586 - val_loss: 2.4725 - val_mae: 1.2159\n",
      "Epoch 78/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.1265 - mae: 1.5911 - val_loss: 2.4960 - val_mae: 1.2252\n",
      "Epoch 79/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.7809 - mae: 1.5206 - val_loss: 2.3631 - val_mae: 1.1791\n",
      "Epoch 80/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.8232 - mae: 1.5277 - val_loss: 2.3741 - val_mae: 1.1822\n",
      "Epoch 81/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.8488 - mae: 1.5381 - val_loss: 2.3511 - val_mae: 1.1739\n",
      "Epoch 82/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.5800 - mae: 1.4690 - val_loss: 2.3251 - val_mae: 1.1640\n",
      "Epoch 83/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 3.5507 - mae: 1.4727 - val_loss: 2.3549 - val_mae: 1.1750\n",
      "Epoch 84/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 14.7484 - mae: 1.5757 - val_loss: 2.4622 - val_mae: 1.2041\n",
      "Epoch 85/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 3.7502 - mae: 1.5285 - val_loss: 2.4595 - val_mae: 1.2047\n",
      "Epoch 86/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 34657.3966 - mae: 5.7333 - val_loss: 2.7897 - val_mae: 1.2924\n",
      "Epoch 87/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 4.0235 - mae: 1.5891 - val_loss: 2.5665 - val_mae: 1.2271\n",
      "Epoch 88/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 3.9415 - mae: 1.5609 - val_loss: 2.5036 - val_mae: 1.2109\n",
      "Epoch 89/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 3.7009 - mae: 1.5218 - val_loss: 2.4768 - val_mae: 1.2029\n",
      "Epoch 90/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 3.6060 - mae: 1.4911 - val_loss: 2.4505 - val_mae: 1.1949\n",
      "Epoch 91/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.6786 - mae: 1.5115 - val_loss: 2.4419 - val_mae: 1.1922\n",
      "Epoch 92/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.6892 - mae: 1.5001 - val_loss: 2.4539 - val_mae: 1.1974\n",
      "Epoch 93/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.6127 - mae: 1.4909 - val_loss: 2.4046 - val_mae: 1.1810\n",
      "Epoch 94/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.6502 - mae: 1.4963 - val_loss: 2.4056 - val_mae: 1.1834\n",
      "Epoch 95/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 3.4661 - mae: 1.4552 - val_loss: 2.3687 - val_mae: 1.1705\n",
      "Epoch 96/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 3.5217 - mae: 1.4657 - val_loss: 2.3675 - val_mae: 1.1753\n",
      "Epoch 97/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 3.3705 - mae: 1.4403 - val_loss: 2.3360 - val_mae: 1.1586\n",
      "Epoch 98/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.4152 - mae: 1.4346 - val_loss: 2.3336 - val_mae: 1.1658\n",
      "Epoch 99/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 3.4157 - mae: 1.4290 - val_loss: 2.2895 - val_mae: 1.1495\n",
      "Epoch 100/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.1961 - mae: 1.4010 - val_loss: 2.2624 - val_mae: 1.1414\n",
      "Test Score: 1.50 RMSE\n"
     ]
    }
   ],
   "source": [
    "second = data[data['zip5'] == 37421]\n",
    "second = second.drop(['zip5', 'date_key'], axis =1)\n",
    "second_17_18 = second[second['year'] != 2019]\n",
    "y = second_17_18['impact_score']\n",
    "X = second_17_18.drop(['impact_score'], axis = 1)\n",
    "X = np.expand_dims(X, axis = 2)\n",
    "\n",
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=28, activation=\"relu\", filters=4),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=28, activation=\"relu\", filters=4),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),     # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "#test_size = int(0.2 * 256921)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "#X_train, X_test, y_train, y_test = test[:-test_size], test[-test_size:], impact[:-test_size], impact[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=2, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "#print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "#for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "#    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 140, 4)            116       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 70, 4)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 70, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 43, 4)             452       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 43, 4)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 172)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 173       \n",
      "=================================================================\n",
      "Total params: 741\n",
      "Trainable params: 741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2003 samples, validate on 859 samples\n",
      "Epoch 1/100\n",
      "2003/2003 [==============================] - 13s 7ms/step - loss: 13741349.2973 - mae: 1208.9812 - val_loss: 8943.1246 - val_mae: 91.8104\n",
      "Epoch 2/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 1223489.5252 - mae: 306.5124 - val_loss: 3012.8197 - val_mae: 49.7360\n",
      "Epoch 3/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 1374612.4105 - mae: 212.3478 - val_loss: 1131.5793 - val_mae: 29.4156\n",
      "Epoch 4/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 2130536.6132 - mae: 182.6138 - val_loss: 1910.1306 - val_mae: 31.9609\n",
      "Epoch 5/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 173051.2689 - mae: 87.6857 - val_loss: 240.7217 - val_mae: 14.2346\n",
      "Epoch 6/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 304870.6810 - mae: 57.7791 - val_loss: 190.7798 - val_mae: 13.4119\n",
      "Epoch 7/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 527268.7072 - mae: 48.9072 - val_loss: 119.0939 - val_mae: 10.2212\n",
      "Epoch 8/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 149872.0037 - mae: 32.8030 - val_loss: 249.3170 - val_mae: 15.6650\n",
      "Epoch 9/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 2475.3505 - mae: 18.6801 - val_loss: 248.3611 - val_mae: 15.6357\n",
      "Epoch 10/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 13926.2250 - mae: 19.2451 - val_loss: 245.5513 - val_mae: 15.5456\n",
      "Epoch 11/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 30500.7686 - mae: 20.7081 - val_loss: 241.8965 - val_mae: 15.4276\n",
      "Epoch 12/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 4676.5033 - mae: 17.6841 - val_loss: 237.1416 - val_mae: 15.2728\n",
      "Epoch 13/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 624.4442 - mae: 15.7772 - val_loss: 230.3663 - val_mae: 15.0493\n",
      "Epoch 14/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 20049.2953 - mae: 18.0286 - val_loss: 222.3749 - val_mae: 14.7814\n",
      "Epoch 15/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 1165.7523 - mae: 15.6858 - val_loss: 215.9327 - val_mae: 14.5619\n",
      "Epoch 16/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 926.7250 - mae: 15.1265 - val_loss: 207.0598 - val_mae: 14.2540\n",
      "Epoch 17/100\n",
      "2003/2003 [==============================] - 8s 4ms/step - loss: 497.3431 - mae: 14.5439 - val_loss: 195.4942 - val_mae: 13.8423\n",
      "Epoch 18/100\n",
      "2003/2003 [==============================] - 7s 3ms/step - loss: 526.6962 - mae: 13.8766 - val_loss: 180.9364 - val_mae: 13.3061\n",
      "Epoch 19/100\n",
      "2003/2003 [==============================] - 7s 4ms/step - loss: 130.9155 - mae: 10.4877 - val_loss: 32.6263 - val_mae: 5.3639\n",
      "Epoch 20/100\n",
      "2003/2003 [==============================] - 7s 4ms/step - loss: 2058.6943 - mae: 5.5329 - val_loss: 16.6007 - val_mae: 3.6089\n",
      "Epoch 21/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 92.5824 - mae: 3.4854 - val_loss: 8.7384 - val_mae: 2.4186\n",
      "Epoch 22/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 12.8459 - mae: 2.8086 - val_loss: 5.2627 - val_mae: 1.8067\n",
      "Epoch 23/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 330.5549 - mae: 3.0162 - val_loss: 5.2671 - val_mae: 1.8086\n",
      "Epoch 24/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 10.1874 - mae: 2.5019 - val_loss: 4.1602 - val_mae: 1.5968\n",
      "Epoch 25/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 12.6527 - mae: 2.6487 - val_loss: 3.9472 - val_mae: 1.5594\n",
      "Epoch 26/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 9.9548 - mae: 2.5055 - val_loss: 3.9307 - val_mae: 1.5582\n",
      "Epoch 27/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 12.0826 - mae: 2.5160 - val_loss: 3.7495 - val_mae: 1.5226\n",
      "Epoch 28/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 9.5511 - mae: 2.4505 - val_loss: 3.9249 - val_mae: 1.5564\n",
      "Epoch 29/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 9.8509 - mae: 2.4938 - val_loss: 3.8636 - val_mae: 1.5429\n",
      "Epoch 30/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.4143 - mae: 2.4197 - val_loss: 3.6743 - val_mae: 1.5010\n",
      "Epoch 31/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.5384 - mae: 2.4390 - val_loss: 3.5731 - val_mae: 1.4770\n",
      "Epoch 32/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.8969 - mae: 2.3603 - val_loss: 3.5910 - val_mae: 1.4759\n",
      "Epoch 33/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 158.4487 - mae: 2.6979 - val_loss: 3.5374 - val_mae: 1.4657\n",
      "Epoch 34/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 8.5974 - mae: 2.3325 - val_loss: 3.4277 - val_mae: 1.4365\n",
      "Epoch 35/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 19624.0039 - mae: 5.5779 - val_loss: 3.3410 - val_mae: 1.4105\n",
      "Epoch 36/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 929.1707 - mae: 3.0301 - val_loss: 3.2018 - val_mae: 1.3866\n",
      "Epoch 37/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.4431 - mae: 2.3257 - val_loss: 3.2393 - val_mae: 1.3864\n",
      "Epoch 38/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 28.6078 - mae: 2.4149 - val_loss: 3.3130 - val_mae: 1.4010\n",
      "Epoch 39/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.7208 - mae: 2.3095 - val_loss: 3.2946 - val_mae: 1.3976\n",
      "Epoch 40/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 8.3479 - mae: 2.2683 - val_loss: 3.2144 - val_mae: 1.3811\n",
      "Epoch 41/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.7617 - mae: 2.2305 - val_loss: 3.2671 - val_mae: 1.3894\n",
      "Epoch 42/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.8764 - mae: 2.2119 - val_loss: 3.0930 - val_mae: 1.3555\n",
      "Epoch 43/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.7218 - mae: 2.1832 - val_loss: 3.0962 - val_mae: 1.3519\n",
      "Epoch 44/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.2623 - mae: 2.1264 - val_loss: 3.0172 - val_mae: 1.3375\n",
      "Epoch 45/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 25.4899 - mae: 2.2943 - val_loss: 3.1159 - val_mae: 1.3521\n",
      "Epoch 46/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4151.9569 - mae: 3.5965 - val_loss: 3.0724 - val_mae: 1.3572\n",
      "Epoch 47/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.6894 - mae: 2.1926 - val_loss: 3.0366 - val_mae: 1.3468\n",
      "Epoch 48/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.6467 - mae: 2.1687 - val_loss: 3.0305 - val_mae: 1.3417\n",
      "Epoch 49/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.5262 - mae: 2.1898 - val_loss: 3.0298 - val_mae: 1.3437\n",
      "Epoch 50/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.5115 - mae: 2.1671 - val_loss: 3.1118 - val_mae: 1.3593\n",
      "Epoch 51/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.4422 - mae: 2.1503 - val_loss: 3.0327 - val_mae: 1.3388\n",
      "Epoch 52/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 7.3139 - mae: 2.1344 - val_loss: 3.1429 - val_mae: 1.3662\n",
      "Epoch 53/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 8.7798 - mae: 2.1712 - val_loss: 2.9362 - val_mae: 1.3179\n",
      "Epoch 54/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 7.1343 - mae: 2.1157 - val_loss: 2.9630 - val_mae: 1.3171\n",
      "Epoch 55/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 610.4252 - mae: 2.7038 - val_loss: 3.4184 - val_mae: 1.4369\n",
      "Epoch 56/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 7.1487 - mae: 2.0917 - val_loss: 3.0979 - val_mae: 1.3655\n",
      "Epoch 57/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 22410.8775 - mae: 5.5731 - val_loss: 5.8904 - val_mae: 1.9977\n",
      "Epoch 58/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 18963.1243 - mae: 5.3219 - val_loss: 3.6621 - val_mae: 1.5182\n",
      "Epoch 59/100\n",
      "2003/2003 [==============================] - 9s 4ms/step - loss: 7.6853 - mae: 2.1911 - val_loss: 3.5961 - val_mae: 1.5008\n",
      "Epoch 60/100\n",
      "2003/2003 [==============================] - 7s 4ms/step - loss: 7.3368 - mae: 2.1406 - val_loss: 3.5404 - val_mae: 1.4848\n",
      "Epoch 61/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.2982 - mae: 2.1414 - val_loss: 3.3756 - val_mae: 1.4425\n",
      "Epoch 62/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 7.7579 - mae: 2.2170 - val_loss: 3.3653 - val_mae: 1.4382\n",
      "Epoch 63/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 7.1595 - mae: 2.1100 - val_loss: 3.2262 - val_mae: 1.4000\n",
      "Epoch 64/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.3498 - mae: 2.1318 - val_loss: 3.1887 - val_mae: 1.3884\n",
      "Epoch 65/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.9786 - mae: 2.0878 - val_loss: 3.1985 - val_mae: 1.3894\n",
      "Epoch 66/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 6562.6041 - mae: 3.9559 - val_loss: 3.3060 - val_mae: 1.4274\n",
      "Epoch 67/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 7.9541 - mae: 2.1361 - val_loss: 3.1271 - val_mae: 1.3742\n",
      "Epoch 68/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 7.0648 - mae: 2.0906 - val_loss: 3.0646 - val_mae: 1.3677\n",
      "Epoch 69/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 12.3241 - mae: 2.1383 - val_loss: 2.8804 - val_mae: 1.3056\n",
      "Epoch 70/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 6.6436 - mae: 2.0287 - val_loss: 2.8406 - val_mae: 1.2922\n",
      "Epoch 71/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 6.6938 - mae: 2.0501 - val_loss: 2.8952 - val_mae: 1.3048\n",
      "Epoch 72/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 6.3911 - mae: 1.9708 - val_loss: 2.8262 - val_mae: 1.2876\n",
      "Epoch 73/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 6.7068 - mae: 2.0439 - val_loss: 2.9291 - val_mae: 1.3134\n",
      "Epoch 74/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 6.4797 - mae: 1.9956 - val_loss: 2.9277 - val_mae: 1.3099\n",
      "Epoch 75/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 6.1343 - mae: 1.9540 - val_loss: 2.7672 - val_mae: 1.2702\n",
      "Epoch 76/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 18.8507 - mae: 1.9661 - val_loss: 2.7720 - val_mae: 1.2776\n",
      "Epoch 77/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 5.9397 - mae: 1.8952 - val_loss: 2.6855 - val_mae: 1.2588\n",
      "Epoch 78/100\n",
      "2003/2003 [==============================] - 8s 4ms/step - loss: 5.4125 - mae: 1.8181 - val_loss: 2.6199 - val_mae: 1.2446\n",
      "Epoch 79/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 5.7481 - mae: 1.8728 - val_loss: 2.8791 - val_mae: 1.3232\n",
      "Epoch 80/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 5.3342 - mae: 1.7886 - val_loss: 2.5666 - val_mae: 1.2272\n",
      "Epoch 81/100\n",
      "2003/2003 [==============================] - 13s 6ms/step - loss: 12.0413 - mae: 1.8206 - val_loss: 2.7342 - val_mae: 1.2775\n",
      "Epoch 82/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 9.6867 - mae: 1.8322 - val_loss: 2.3779 - val_mae: 1.1724\n",
      "Epoch 83/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 4.8431 - mae: 1.7233 - val_loss: 2.3320 - val_mae: 1.1552\n",
      "Epoch 84/100\n",
      "2003/2003 [==============================] - 11s 5ms/step - loss: 203.2232 - mae: 2.0228 - val_loss: 2.3909 - val_mae: 1.1840\n",
      "Epoch 85/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 4.8858 - mae: 1.7336 - val_loss: 2.4273 - val_mae: 1.1939\n",
      "Epoch 86/100\n",
      "2003/2003 [==============================] - 11s 6ms/step - loss: 4.7646 - mae: 1.7062 - val_loss: 2.3826 - val_mae: 1.1841\n",
      "Epoch 87/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 4.6202 - mae: 1.6848 - val_loss: 2.2805 - val_mae: 1.1514\n",
      "Epoch 88/100\n",
      "2003/2003 [==============================] - 7s 4ms/step - loss: 4.3087 - mae: 1.6198 - val_loss: 2.2091 - val_mae: 1.1343\n",
      "Epoch 89/100\n",
      "2003/2003 [==============================] - 7s 3ms/step - loss: 4.1731 - mae: 1.5961 - val_loss: 2.1964 - val_mae: 1.1312\n",
      "Epoch 90/100\n",
      "2003/2003 [==============================] - 6s 3ms/step - loss: 4.1009 - mae: 1.5655 - val_loss: 2.1556 - val_mae: 1.1116\n",
      "Epoch 91/100\n",
      "2003/2003 [==============================] - 7s 3ms/step - loss: 4.0312 - mae: 1.5655 - val_loss: 2.0821 - val_mae: 1.0909\n",
      "Epoch 92/100\n",
      "2003/2003 [==============================] - 7s 3ms/step - loss: 4.3303 - mae: 1.5785 - val_loss: 2.1366 - val_mae: 1.1083\n",
      "Epoch 93/100\n",
      "2003/2003 [==============================] - 7s 4ms/step - loss: 3.8505 - mae: 1.5174 - val_loss: 2.0342 - val_mae: 1.0791\n",
      "Epoch 94/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.5065 - mae: 1.4589 - val_loss: 2.1655 - val_mae: 1.1147\n",
      "Epoch 95/100\n",
      "2003/2003 [==============================] - 12s 6ms/step - loss: 3.5648 - mae: 1.4715 - val_loss: 2.1114 - val_mae: 1.1026\n",
      "Epoch 96/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 3.6981 - mae: 1.4964 - val_loss: 2.1032 - val_mae: 1.1011\n",
      "Epoch 97/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 68046.0569 - mae: 7.4670 - val_loss: 2.1677 - val_mae: 1.1309\n",
      "Epoch 98/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 3.9542 - mae: 1.5482 - val_loss: 2.1364 - val_mae: 1.1247\n",
      "Epoch 99/100\n",
      "2003/2003 [==============================] - 10s 5ms/step - loss: 3.6879 - mae: 1.5049 - val_loss: 2.1169 - val_mae: 1.1175\n",
      "Epoch 100/100\n",
      "2003/2003 [==============================] - 9s 5ms/step - loss: 3.7394 - mae: 1.4983 - val_loss: 2.1021 - val_mae: 1.1089\n",
      "Test Score: 1.45 RMSE\n"
     ]
    }
   ],
   "source": [
    "third = data[data['zip5'] == 85043]\n",
    "third = third.drop(['zip5', 'date_key'], axis =1)\n",
    "third_17_18 = third[third['year'] != 2019]\n",
    "y = third_17_18['impact_score']\n",
    "X = third_17_18.drop(['impact_score'], axis = 1)\n",
    "X = np.expand_dims(X, axis = 2)\n",
    "\n",
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=28, activation=\"relu\", filters=4),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=28, activation=\"relu\", filters=4),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),     # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "#test_size = int(0.2 * 256921)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "#X_train, X_test, y_train, y_test = test[:-test_size], test[-test_size:], impact[:-test_size], impact[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=2, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "#print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "#for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "#    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 164, 16)           80        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 79, 16)            1040      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 79, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 1265      \n",
      "=================================================================\n",
      "Total params: 2,385\n",
      "Trainable params: 2,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2290 samples, validate on 572 samples\n",
      "Epoch 1/500\n",
      "2290/2290 [==============================] - 3s 1ms/step - loss: 10305089.9858 - mae: 2094.1226 - val_loss: 96799.8175 - val_mae: 271.7155\n",
      "Epoch 2/500\n",
      "2290/2290 [==============================] - 2s 894us/step - loss: 2206489.2334 - mae: 983.6501 - val_loss: 37316.5261 - val_mae: 188.7180\n",
      "Epoch 3/500\n",
      "2290/2290 [==============================] - 2s 776us/step - loss: 946132.7719 - mae: 677.6586 - val_loss: 74568.3343 - val_mae: 271.6514\n",
      "Epoch 4/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 865906.7926 - mae: 499.2551 - val_loss: 27602.5570 - val_mae: 164.8307\n",
      "Epoch 5/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 633398.0299 - mae: 410.3832 - val_loss: 35803.0162 - val_mae: 188.0561\n",
      "Epoch 6/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 211969.9488 - mae: 327.5138 - val_loss: 10516.4388 - val_mae: 100.7898\n",
      "Epoch 7/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 1584274.6214 - mae: 307.4168 - val_loss: 6905.5007 - val_mae: 79.9801\n",
      "Epoch 8/500\n",
      "2290/2290 [==============================] - 2s 850us/step - loss: 2249494.1505 - mae: 281.3843 - val_loss: 696.6807 - val_mae: 20.8387\n",
      "Epoch 9/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 120767.5026 - mae: 190.0164 - val_loss: 583.6508 - val_mae: 20.0105\n",
      "Epoch 10/500\n",
      "2290/2290 [==============================] - 2s 727us/step - loss: 158682.0836 - mae: 179.1518 - val_loss: 607.8950 - val_mae: 20.8021\n",
      "Epoch 11/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 231603.1212 - mae: 160.7708 - val_loss: 1378.7545 - val_mae: 33.8694\n",
      "Epoch 12/500\n",
      "2290/2290 [==============================] - 2s 855us/step - loss: 1628123.0311 - mae: 169.4976 - val_loss: 958.2307 - val_mae: 28.6364\n",
      "Epoch 13/500\n",
      "2290/2290 [==============================] - 2s 754us/step - loss: 1160549.7214 - mae: 151.2538 - val_loss: 2004.5591 - val_mae: 43.1655\n",
      "Epoch 14/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 403884.9058 - mae: 112.8598 - val_loss: 788.1702 - val_mae: 25.2127\n",
      "Epoch 15/500\n",
      "2290/2290 [==============================] - 2s 748us/step - loss: 275297.9881 - mae: 101.4005 - val_loss: 377.8978 - val_mae: 17.1943\n",
      "Epoch 16/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 55900.9966 - mae: 86.7251 - val_loss: 568.5518 - val_mae: 21.7667\n",
      "Epoch 17/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 264018.7134 - mae: 88.0681 - val_loss: 645.3871 - val_mae: 23.4885\n",
      "Epoch 18/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 93723.4647 - mae: 75.2502 - val_loss: 379.6252 - val_mae: 17.4813\n",
      "Epoch 19/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 193294.2814 - mae: 74.8899 - val_loss: 167.5456 - val_mae: 11.6167\n",
      "Epoch 20/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 186139.9397 - mae: 65.2951 - val_loss: 223.9030 - val_mae: 13.3790\n",
      "Epoch 21/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 174265.2398 - mae: 65.2759 - val_loss: 415.7587 - val_mae: 19.7494\n",
      "Epoch 22/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 56526.1037 - mae: 53.9258 - val_loss: 53.7960 - val_mae: 6.4011\n",
      "Epoch 23/500\n",
      "2290/2290 [==============================] - 2s 763us/step - loss: 43843.1626 - mae: 51.1684 - val_loss: 37.3680 - val_mae: 4.8423\n",
      "Epoch 24/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 406356.5145 - mae: 66.2553 - val_loss: 182.6318 - val_mae: 12.1895\n",
      "Epoch 25/500\n",
      "2290/2290 [==============================] - 2s 705us/step - loss: 68346.9090 - mae: 48.1606 - val_loss: 28.8837 - val_mae: 4.3446\n",
      "Epoch 26/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 122107.9214 - mae: 48.8585 - val_loss: 147.5488 - val_mae: 11.6992\n",
      "Epoch 27/500\n",
      "2290/2290 [==============================] - 2s 691us/step - loss: 7002.5363 - mae: 39.1570 - val_loss: 12.1267 - val_mae: 2.7038\n",
      "Epoch 28/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 31590.1884 - mae: 39.1829 - val_loss: 47.5417 - val_mae: 6.4469\n",
      "Epoch 29/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 6937.1691 - mae: 33.2014 - val_loss: 6.5791 - val_mae: 2.0754\n",
      "Epoch 30/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 118704.6846 - mae: 41.4626 - val_loss: 141.7780 - val_mae: 11.4962\n",
      "Epoch 31/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 77199.3303 - mae: 37.9447 - val_loss: 16.3224 - val_mae: 3.4491\n",
      "Epoch 32/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 3300.5308 - mae: 27.9481 - val_loss: 6.5722 - val_mae: 2.1084\n",
      "Epoch 33/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 32219.8167 - mae: 32.1429 - val_loss: 15.5112 - val_mae: 3.3814\n",
      "Epoch 34/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 27254.7157 - mae: 30.7262 - val_loss: 8.5569 - val_mae: 2.3519\n",
      "Epoch 35/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 34706.3443 - mae: 28.4146 - val_loss: 4.7448 - val_mae: 1.8458\n",
      "Epoch 36/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 55592.1613 - mae: 29.1382 - val_loss: 47.3393 - val_mae: 6.6004\n",
      "Epoch 37/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 42311.0835 - mae: 28.2674 - val_loss: 39.4065 - val_mae: 5.9625\n",
      "Epoch 38/500\n",
      "2290/2290 [==============================] - 2s 747us/step - loss: 12597.2325 - mae: 24.1687 - val_loss: 14.1958 - val_mae: 3.3091\n",
      "Epoch 39/500\n",
      "2290/2290 [==============================] - 2s 864us/step - loss: 21316.1989 - mae: 25.8520 - val_loss: 66.3495 - val_mae: 7.9308\n",
      "Epoch 40/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 43780.2005 - mae: 27.3321 - val_loss: 5.8223 - val_mae: 1.9337\n",
      "Epoch 41/500\n",
      "2290/2290 [==============================] - 2s 703us/step - loss: 9851.9910 - mae: 20.2938 - val_loss: 5.6565 - val_mae: 1.9119\n",
      "Epoch 42/500\n",
      "2290/2290 [==============================] - 2s 856us/step - loss: 71451.1348 - mae: 23.4884 - val_loss: 54.8597 - val_mae: 7.1671\n",
      "Epoch 43/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 10060.0086 - mae: 21.6465 - val_loss: 27.5229 - val_mae: 4.8207\n",
      "Epoch 44/500\n",
      "2290/2290 [==============================] - 2s 880us/step - loss: 10352.0611 - mae: 18.0553 - val_loss: 8.2547 - val_mae: 2.2603\n",
      "Epoch 45/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 3647.0952 - mae: 16.7354 - val_loss: 12.5096 - val_mae: 2.9788\n",
      "Epoch 46/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 30020.9142 - mae: 19.3858 - val_loss: 38.5464 - val_mae: 5.8918\n",
      "Epoch 47/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 10281.3261 - mae: 19.4684 - val_loss: 9.5266 - val_mae: 2.5750\n",
      "Epoch 48/500\n",
      "2290/2290 [==============================] - 2s 790us/step - loss: 15728.6343 - mae: 18.7849 - val_loss: 17.4766 - val_mae: 3.7146\n",
      "Epoch 49/500\n",
      "2290/2290 [==============================] - 2s 760us/step - loss: 90314.0118 - mae: 23.7022 - val_loss: 104.5178 - val_mae: 10.0443\n",
      "Epoch 50/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 28958.0488 - mae: 19.1657 - val_loss: 20.9080 - val_mae: 4.1768\n",
      "Epoch 51/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 3609.5073 - mae: 14.0121 - val_loss: 29.4017 - val_mae: 5.0734\n",
      "Epoch 52/500\n",
      "2290/2290 [==============================] - 2s 819us/step - loss: 10613.0657 - mae: 15.5055 - val_loss: 28.8666 - val_mae: 5.0778\n",
      "Epoch 53/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 4935.1319 - mae: 13.3082 - val_loss: 28.1029 - val_mae: 5.0078\n",
      "Epoch 54/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 2330.4663 - mae: 11.4672 - val_loss: 28.0492 - val_mae: 5.0030\n",
      "Epoch 55/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 13778.8471 - mae: 13.5457 - val_loss: 3.3893 - val_mae: 1.4417\n",
      "Epoch 56/500\n",
      "2290/2290 [==============================] - 2s 785us/step - loss: 2846.9019 - mae: 11.1403 - val_loss: 56.4432 - val_mae: 7.3548\n",
      "Epoch 57/500\n",
      "2290/2290 [==============================] - 2s 859us/step - loss: 1351.0302 - mae: 10.7485 - val_loss: 28.6380 - val_mae: 5.1252\n",
      "Epoch 58/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 3513.2111 - mae: 11.2718 - val_loss: 34.3502 - val_mae: 5.6569\n",
      "Epoch 59/500\n",
      "2290/2290 [==============================] - 2s 773us/step - loss: 6611.1225 - mae: 11.6322 - val_loss: 41.7211 - val_mae: 6.2791\n",
      "Epoch 60/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 27263.8260 - mae: 15.4461 - val_loss: 60.1937 - val_mae: 7.5976\n",
      "Epoch 61/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 9963.5706 - mae: 12.7260 - val_loss: 27.3339 - val_mae: 4.9637\n",
      "Epoch 62/500\n",
      "2290/2290 [==============================] - 2s 795us/step - loss: 922.0575 - mae: 9.2259 - val_loss: 28.3709 - val_mae: 5.0913\n",
      "Epoch 63/500\n",
      "2290/2290 [==============================] - 2s 775us/step - loss: 7849.0873 - mae: 11.5995 - val_loss: 26.8360 - val_mae: 4.9617\n",
      "Epoch 64/500\n",
      "2290/2290 [==============================] - 2s 834us/step - loss: 1888.5461 - mae: 9.5432 - val_loss: 30.0529 - val_mae: 5.2799\n",
      "Epoch 65/500\n",
      "2290/2290 [==============================] - 2s 812us/step - loss: 796.6935 - mae: 9.0421 - val_loss: 60.8223 - val_mae: 7.6644\n",
      "Epoch 66/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 3147.9754 - mae: 9.4538 - val_loss: 45.4599 - val_mae: 6.5388\n",
      "Epoch 67/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 1628.5444 - mae: 9.4077 - val_loss: 47.3481 - val_mae: 6.6465\n",
      "Epoch 68/500\n",
      "2290/2290 [==============================] - 2s 775us/step - loss: 773.2304 - mae: 8.1717 - val_loss: 46.4375 - val_mae: 6.6330\n",
      "Epoch 69/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 5181.1864 - mae: 9.9497 - val_loss: 30.7200 - val_mae: 5.3170\n",
      "Epoch 70/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 8940.7872 - mae: 10.3970 - val_loss: 4.4669 - val_mae: 1.6404\n",
      "Epoch 71/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 2907.8563 - mae: 9.0590 - val_loss: 57.4223 - val_mae: 7.4305\n",
      "Epoch 72/500\n",
      "2290/2290 [==============================] - 2s 899us/step - loss: 3983.7228 - mae: 9.2473 - val_loss: 52.1523 - val_mae: 7.0686\n",
      "Epoch 73/500\n",
      "2290/2290 [==============================] - 2s 790us/step - loss: 439.3727 - mae: 7.4410 - val_loss: 59.1412 - val_mae: 7.5416\n",
      "Epoch 74/500\n",
      "2290/2290 [==============================] - 2s 863us/step - loss: 5354.8568 - mae: 9.2189 - val_loss: 83.5036 - val_mae: 9.0083\n",
      "Epoch 75/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 2031.2244 - mae: 8.9808 - val_loss: 20.5806 - val_mae: 4.1989\n",
      "Epoch 76/500\n",
      "2290/2290 [==============================] - 2s 916us/step - loss: 1217.7207 - mae: 8.1544 - val_loss: 63.8482 - val_mae: 7.8139\n",
      "Epoch 77/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 7403.9231 - mae: 9.4116 - val_loss: 57.8518 - val_mae: 7.3599\n",
      "Epoch 78/500\n",
      "2290/2290 [==============================] - 2s 790us/step - loss: 3898.3447 - mae: 9.1514 - val_loss: 27.0625 - val_mae: 4.9438\n",
      "Epoch 79/500\n",
      "2290/2290 [==============================] - 2s 833us/step - loss: 3071.4336 - mae: 8.3689 - val_loss: 52.2443 - val_mae: 7.0351\n",
      "Epoch 80/500\n",
      "2290/2290 [==============================] - 2s 844us/step - loss: 858.5166 - mae: 6.8877 - val_loss: 42.0882 - val_mae: 6.2766\n",
      "Epoch 81/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 4479.0770 - mae: 7.8108 - val_loss: 72.6083 - val_mae: 8.3818\n",
      "Epoch 82/500\n",
      "2290/2290 [==============================] - 2s 868us/step - loss: 290.1708 - mae: 6.5905 - val_loss: 25.4893 - val_mae: 4.7621\n",
      "Epoch 83/500\n",
      "2290/2290 [==============================] - 2s 815us/step - loss: 398.5267 - mae: 6.7538 - val_loss: 28.6526 - val_mae: 5.0656\n",
      "Epoch 84/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 2103.3623 - mae: 8.6101 - val_loss: 25.9934 - val_mae: 4.7910\n",
      "Epoch 85/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 3102.7932 - mae: 8.3098 - val_loss: 4.9016 - val_mae: 1.7513\n",
      "Epoch 86/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 925.5901 - mae: 7.5805 - val_loss: 34.3321 - val_mae: 5.5963\n",
      "Epoch 87/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 1003.4757 - mae: 7.0074 - val_loss: 36.8810 - val_mae: 5.8160\n",
      "Epoch 88/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 1295.0253 - mae: 7.2231 - val_loss: 32.6344 - val_mae: 5.4156\n",
      "Epoch 89/500\n",
      "2290/2290 [==============================] - 2s 696us/step - loss: 611.6284 - mae: 6.7789 - val_loss: 31.2116 - val_mae: 5.2956\n",
      "Epoch 90/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 452.9116 - mae: 6.4457 - val_loss: 38.9404 - val_mae: 5.9730\n",
      "Epoch 91/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 266.1604 - mae: 6.1765 - val_loss: 39.2836 - val_mae: 6.0309\n",
      "Epoch 92/500\n",
      "2290/2290 [==============================] - 2s 902us/step - loss: 347.7139 - mae: 6.1021 - val_loss: 41.6384 - val_mae: 6.2251\n",
      "Epoch 93/500\n",
      "2290/2290 [==============================] - 2s 833us/step - loss: 99.6706 - mae: 5.7357 - val_loss: 33.7594 - val_mae: 5.5473\n",
      "Epoch 94/500\n",
      "2290/2290 [==============================] - 2s 858us/step - loss: 313.8641 - mae: 6.0551 - val_loss: 33.9344 - val_mae: 5.5675\n",
      "Epoch 95/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 1150.7799 - mae: 6.7402 - val_loss: 35.9686 - val_mae: 5.7266\n",
      "Epoch 96/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 437.3962 - mae: 6.3867 - val_loss: 24.8928 - val_mae: 4.6103\n",
      "Epoch 97/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 708.9625 - mae: 6.9760 - val_loss: 33.2957 - val_mae: 5.5037\n",
      "Epoch 98/500\n",
      "2290/2290 [==============================] - 2s 832us/step - loss: 347.0474 - mae: 6.0539 - val_loss: 33.4462 - val_mae: 5.5039\n",
      "Epoch 99/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 4046.9843 - mae: 8.1522 - val_loss: 40.2572 - val_mae: 6.0058\n",
      "Epoch 100/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 113.9587 - mae: 6.7074 - val_loss: 27.0918 - val_mae: 4.8944\n",
      "Epoch 101/500\n",
      "2290/2290 [==============================] - 2s 844us/step - loss: 580.0397 - mae: 6.3152 - val_loss: 62.1757 - val_mae: 7.6979\n",
      "Epoch 102/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 710.2140 - mae: 6.3298 - val_loss: 28.3173 - val_mae: 5.0321\n",
      "Epoch 103/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 1221.6702 - mae: 7.0266 - val_loss: 37.1864 - val_mae: 5.8162\n",
      "Epoch 104/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 1762.7350 - mae: 6.4895 - val_loss: 30.6629 - val_mae: 5.2140\n",
      "Epoch 105/500\n",
      "2290/2290 [==============================] - 2s 762us/step - loss: 3180.2789 - mae: 8.3267 - val_loss: 40.8188 - val_mae: 6.0820\n",
      "Epoch 106/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 1189.7121 - mae: 6.1667 - val_loss: 31.4376 - val_mae: 5.2346\n",
      "Epoch 107/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 451.7610 - mae: 6.1389 - val_loss: 48.2106 - val_mae: 6.6503\n",
      "Epoch 108/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 186.4692 - mae: 5.8019 - val_loss: 31.3774 - val_mae: 5.2646\n",
      "Epoch 109/500\n",
      "2290/2290 [==============================] - 2s 815us/step - loss: 442.7076 - mae: 5.9135 - val_loss: 80.2064 - val_mae: 8.7490\n",
      "Epoch 110/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 634.2114 - mae: 6.2186 - val_loss: 40.1782 - val_mae: 6.0253\n",
      "Epoch 111/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 161.6995 - mae: 5.5601 - val_loss: 25.0117 - val_mae: 4.5589\n",
      "Epoch 112/500\n",
      "2290/2290 [==============================] - 2s 866us/step - loss: 72.2798 - mae: 5.3503 - val_loss: 40.9691 - val_mae: 6.1263\n",
      "Epoch 113/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 119.1646 - mae: 5.2689 - val_loss: 36.4423 - val_mae: 5.7321\n",
      "Epoch 114/500\n",
      "2290/2290 [==============================] - 2s 846us/step - loss: 65.8824 - mae: 5.2463 - val_loss: 32.5855 - val_mae: 5.3863\n",
      "Epoch 115/500\n",
      "2290/2290 [==============================] - 2s 776us/step - loss: 194.8280 - mae: 5.6604 - val_loss: 22.1457 - val_mae: 4.2887\n",
      "Epoch 116/500\n",
      "2290/2290 [==============================] - 2s 759us/step - loss: 183.8813 - mae: 5.4703 - val_loss: 29.5773 - val_mae: 5.0815\n",
      "Epoch 117/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 155.2146 - mae: 5.1765 - val_loss: 27.4295 - val_mae: 4.8293\n",
      "Epoch 118/500\n",
      "2290/2290 [==============================] - 2s 692us/step - loss: 233.5643 - mae: 5.4793 - val_loss: 32.0479 - val_mae: 5.2374\n",
      "Epoch 119/500\n",
      "2290/2290 [==============================] - 2s 752us/step - loss: 88.4867 - mae: 5.3495 - val_loss: 29.3390 - val_mae: 5.0687\n",
      "Epoch 120/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 253.8434 - mae: 5.4630 - val_loss: 32.6735 - val_mae: 5.3854\n",
      "Epoch 121/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 62.3047 - mae: 5.1152 - val_loss: 42.6042 - val_mae: 6.2595\n",
      "Epoch 122/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 69.1870 - mae: 5.0560 - val_loss: 44.7541 - val_mae: 6.4254\n",
      "Epoch 123/500\n",
      "2290/2290 [==============================] - 2s 762us/step - loss: 54.0339 - mae: 4.9763 - val_loss: 29.8116 - val_mae: 5.1135\n",
      "Epoch 124/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 68.7049 - mae: 5.2168 - val_loss: 29.4584 - val_mae: 5.0789\n",
      "Epoch 125/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 1024.7717 - mae: 6.2251 - val_loss: 31.2779 - val_mae: 5.2537\n",
      "Epoch 126/500\n",
      "2290/2290 [==============================] - 2s 724us/step - loss: 95.7183 - mae: 5.8344 - val_loss: 24.4922 - val_mae: 4.5272\n",
      "Epoch 127/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 1022.2315 - mae: 5.7365 - val_loss: 21.5570 - val_mae: 4.3188\n",
      "Epoch 128/500\n",
      "2290/2290 [==============================] - 2s 745us/step - loss: 92.2464 - mae: 4.1286 - val_loss: 12.1645 - val_mae: 3.1606\n",
      "Epoch 129/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 218.0066 - mae: 4.1352 - val_loss: 6.7994 - val_mae: 2.2076\n",
      "Epoch 130/500\n",
      "2290/2290 [==============================] - 2s 730us/step - loss: 128.1908 - mae: 3.5501 - val_loss: 11.1505 - val_mae: 3.0343\n",
      "Epoch 131/500\n",
      "2290/2290 [==============================] - 2s 739us/step - loss: 14.8637 - mae: 2.8228 - val_loss: 2.3391 - val_mae: 1.1602\n",
      "Epoch 132/500\n",
      "2290/2290 [==============================] - 2s 763us/step - loss: 43.3705 - mae: 2.9746 - val_loss: 2.2533 - val_mae: 1.2778\n",
      "Epoch 133/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 1510.2953 - mae: 4.9728 - val_loss: 1.8785 - val_mae: 1.1298\n",
      "Epoch 134/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 13.6655 - mae: 2.5625 - val_loss: 2.2292 - val_mae: 1.2944\n",
      "Epoch 135/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 47.9997 - mae: 2.5661 - val_loss: 1.8288 - val_mae: 1.1338\n",
      "Epoch 136/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 407.7061 - mae: 3.2563 - val_loss: 2.6993 - val_mae: 1.4313\n",
      "Epoch 137/500\n",
      "2290/2290 [==============================] - 2s 784us/step - loss: 120.5236 - mae: 2.5310 - val_loss: 2.0331 - val_mae: 1.2305\n",
      "Epoch 138/500\n",
      "2290/2290 [==============================] - 2s 729us/step - loss: 178.3697 - mae: 2.7872 - val_loss: 5.6443 - val_mae: 1.9163\n",
      "Epoch 139/500\n",
      "2290/2290 [==============================] - 2s 733us/step - loss: 135.1386 - mae: 2.9362 - val_loss: 4.7339 - val_mae: 1.9168\n",
      "Epoch 140/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 11.5207 - mae: 2.2343 - val_loss: 2.1801 - val_mae: 1.2774\n",
      "Epoch 141/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 236.4023 - mae: 3.2559 - val_loss: 2.0864 - val_mae: 1.2447\n",
      "Epoch 142/500\n",
      "2290/2290 [==============================] - 2s 860us/step - loss: 238.4029 - mae: 2.9084 - val_loss: 2.5367 - val_mae: 1.3842\n",
      "Epoch 143/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 135.4256 - mae: 3.0554 - val_loss: 1.7374 - val_mae: 1.0676\n",
      "Epoch 144/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 130.2147 - mae: 2.5947 - val_loss: 2.1120 - val_mae: 1.2579\n",
      "Epoch 145/500\n",
      "2290/2290 [==============================] - 2s 736us/step - loss: 190.2657 - mae: 3.2586 - val_loss: 1.7597 - val_mae: 1.0441\n",
      "Epoch 146/500\n",
      "2290/2290 [==============================] - 2s 726us/step - loss: 11.7011 - mae: 2.1858 - val_loss: 3.5004 - val_mae: 1.6386\n",
      "Epoch 147/500\n",
      "2290/2290 [==============================] - 2s 854us/step - loss: 306.0421 - mae: 2.5785 - val_loss: 3.1680 - val_mae: 1.5500\n",
      "Epoch 148/500\n",
      "2290/2290 [==============================] - 2s 701us/step - loss: 20.4497 - mae: 2.1001 - val_loss: 3.0393 - val_mae: 1.5192\n",
      "Epoch 149/500\n",
      "2290/2290 [==============================] - 2s 709us/step - loss: 9.3193 - mae: 1.9721 - val_loss: 4.3106 - val_mae: 1.8216\n",
      "Epoch 150/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 268.4644 - mae: 2.4233 - val_loss: 14.1537 - val_mae: 3.5495\n",
      "Epoch 151/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 21.2382 - mae: 2.3798 - val_loss: 2.5464 - val_mae: 1.3867\n",
      "Epoch 152/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 984.0648 - mae: 3.5697 - val_loss: 4.4047 - val_mae: 1.8126\n",
      "Epoch 153/500\n",
      "2290/2290 [==============================] - 2s 732us/step - loss: 7.4750 - mae: 2.1114 - val_loss: 3.1367 - val_mae: 1.5429\n",
      "Epoch 154/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 29.6059 - mae: 1.9963 - val_loss: 3.8644 - val_mae: 1.7212\n",
      "Epoch 155/500\n",
      "2290/2290 [==============================] - 2s 911us/step - loss: 499.9565 - mae: 3.4595 - val_loss: 7.2132 - val_mae: 2.4153\n",
      "Epoch 156/500\n",
      "2290/2290 [==============================] - 2s 698us/step - loss: 171.2490 - mae: 2.4293 - val_loss: 3.1243 - val_mae: 1.5406\n",
      "Epoch 157/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 27.1129 - mae: 1.9549 - val_loss: 3.1356 - val_mae: 1.5415\n",
      "Epoch 158/500\n",
      "2290/2290 [==============================] - 2s 766us/step - loss: 94.9598 - mae: 2.2224 - val_loss: 4.2329 - val_mae: 1.8024\n",
      "Epoch 159/500\n",
      "2290/2290 [==============================] - 2s 703us/step - loss: 23.9042 - mae: 1.8915 - val_loss: 4.1003 - val_mae: 1.7711\n",
      "Epoch 160/500\n",
      "2290/2290 [==============================] - 2s 725us/step - loss: 5.5337 - mae: 1.8025 - val_loss: 3.4784 - val_mae: 1.6262\n",
      "Epoch 161/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 619.9498 - mae: 2.6290 - val_loss: 1.8167 - val_mae: 1.1476\n",
      "Epoch 162/500\n",
      "2290/2290 [==============================] - 2s 747us/step - loss: 333.4413 - mae: 2.2786 - val_loss: 2.1197 - val_mae: 1.2609\n",
      "Epoch 163/500\n",
      "2290/2290 [==============================] - 2s 770us/step - loss: 360.5267 - mae: 2.5376 - val_loss: 3.2311 - val_mae: 1.5659\n",
      "Epoch 164/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 17.8893 - mae: 1.8803 - val_loss: 3.3600 - val_mae: 1.5999\n",
      "Epoch 165/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 63.2178 - mae: 1.8983 - val_loss: 3.2219 - val_mae: 1.5601\n",
      "Epoch 166/500\n",
      "2290/2290 [==============================] - 2s 766us/step - loss: 6.8866 - mae: 1.8721 - val_loss: 3.7847 - val_mae: 1.6983\n",
      "Epoch 167/500\n",
      "2290/2290 [==============================] - 2s 795us/step - loss: 218.3941 - mae: 2.2001 - val_loss: 2.8603 - val_mae: 1.4687\n",
      "Epoch 168/500\n",
      "2290/2290 [==============================] - 2s 841us/step - loss: 11.4843 - mae: 1.7982 - val_loss: 3.3070 - val_mae: 1.5829\n",
      "Epoch 169/500\n",
      "2290/2290 [==============================] - 2s 808us/step - loss: 96.6537 - mae: 2.1191 - val_loss: 3.0123 - val_mae: 1.5086\n",
      "Epoch 170/500\n",
      "2290/2290 [==============================] - 2s 892us/step - loss: 14.7395 - mae: 1.7676 - val_loss: 3.6558 - val_mae: 1.6674\n",
      "Epoch 171/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 10.9681 - mae: 1.7815 - val_loss: 3.9281 - val_mae: 1.7311\n",
      "Epoch 172/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 5.1760 - mae: 1.6934 - val_loss: 3.8853 - val_mae: 1.7214\n",
      "Epoch 173/500\n",
      "2290/2290 [==============================] - 2s 709us/step - loss: 43.2153 - mae: 1.8854 - val_loss: 4.0081 - val_mae: 1.7463\n",
      "Epoch 174/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 17.4874 - mae: 1.7771 - val_loss: 4.6405 - val_mae: 1.8964\n",
      "Epoch 175/500\n",
      "2290/2290 [==============================] - 2s 685us/step - loss: 9.7521 - mae: 1.7851 - val_loss: 3.5618 - val_mae: 1.6462\n",
      "Epoch 176/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 7.4756 - mae: 1.7561 - val_loss: 3.6780 - val_mae: 1.6701\n",
      "Epoch 177/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 38.3798 - mae: 1.8887 - val_loss: 3.9155 - val_mae: 1.7261\n",
      "Epoch 178/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 4.7045 - mae: 1.6775 - val_loss: 3.9821 - val_mae: 1.7412\n",
      "Epoch 179/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 5.0055 - mae: 1.6673 - val_loss: 4.1022 - val_mae: 1.7665\n",
      "Epoch 180/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 4.3642 - mae: 1.6512 - val_loss: 4.4572 - val_mae: 1.8539\n",
      "Epoch 181/500\n",
      "2290/2290 [==============================] - 2s 785us/step - loss: 53.4686 - mae: 1.8537 - val_loss: 3.5751 - val_mae: 1.6414\n",
      "Epoch 182/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 12.5571 - mae: 1.7413 - val_loss: 4.9932 - val_mae: 1.9784\n",
      "Epoch 183/500\n",
      "2290/2290 [==============================] - 2s 849us/step - loss: 4.4006 - mae: 1.6388 - val_loss: 4.3245 - val_mae: 1.8218\n",
      "Epoch 184/500\n",
      "2290/2290 [==============================] - 2s 772us/step - loss: 25.9862 - mae: 1.8275 - val_loss: 5.6810 - val_mae: 2.1220\n",
      "Epoch 185/500\n",
      "2290/2290 [==============================] - 2s 819us/step - loss: 4.9691 - mae: 1.6787 - val_loss: 4.1744 - val_mae: 1.7848\n",
      "Epoch 186/500\n",
      "2290/2290 [==============================] - 2s 762us/step - loss: 4.7325 - mae: 1.6328 - val_loss: 4.0131 - val_mae: 1.7455\n",
      "Epoch 187/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 10.0033 - mae: 1.6871 - val_loss: 4.5557 - val_mae: 1.8773\n",
      "Epoch 188/500\n",
      "2290/2290 [==============================] - 2s 860us/step - loss: 718.8962 - mae: 2.9923 - val_loss: 4.5939 - val_mae: 1.8893\n",
      "Epoch 189/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 9.2593 - mae: 1.6703 - val_loss: 4.1298 - val_mae: 1.7758\n",
      "Epoch 190/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 7.4427 - mae: 1.6455 - val_loss: 3.7449 - val_mae: 1.6764\n",
      "Epoch 191/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 4.7664 - mae: 1.6461 - val_loss: 4.1300 - val_mae: 1.7738\n",
      "Epoch 192/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 17.0179 - mae: 1.7083 - val_loss: 3.8330 - val_mae: 1.6991\n",
      "Epoch 193/500\n",
      "2290/2290 [==============================] - 2s 835us/step - loss: 4.4378 - mae: 1.5992 - val_loss: 4.1612 - val_mae: 1.7811\n",
      "Epoch 194/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 81.6325 - mae: 1.7749 - val_loss: 5.2561 - val_mae: 2.0351\n",
      "Epoch 195/500\n",
      "2290/2290 [==============================] - 2s 714us/step - loss: 13.9432 - mae: 1.6631 - val_loss: 4.9691 - val_mae: 1.9722\n",
      "Epoch 196/500\n",
      "2290/2290 [==============================] - 2s 766us/step - loss: 4.2490 - mae: 1.5893 - val_loss: 4.6717 - val_mae: 1.9029\n",
      "Epoch 197/500\n",
      "2290/2290 [==============================] - 2s 745us/step - loss: 19.3878 - mae: 1.7158 - val_loss: 5.4384 - val_mae: 2.0757\n",
      "Epoch 198/500\n",
      "2290/2290 [==============================] - 1s 654us/step - loss: 23.8471 - mae: 1.6788 - val_loss: 6.7389 - val_mae: 2.3494\n",
      "Epoch 199/500\n",
      "2290/2290 [==============================] - 1s 654us/step - loss: 4.4984 - mae: 1.6004 - val_loss: 4.7801 - val_mae: 1.9280\n",
      "Epoch 200/500\n",
      "2290/2290 [==============================] - 2s 666us/step - loss: 4.7100 - mae: 1.6002 - val_loss: 4.9838 - val_mae: 1.9744\n",
      "Epoch 201/500\n",
      "2290/2290 [==============================] - 1s 651us/step - loss: 5.0203 - mae: 1.5833 - val_loss: 5.0319 - val_mae: 1.9854\n",
      "Epoch 202/500\n",
      "2290/2290 [==============================] - 2s 664us/step - loss: 4.0041 - mae: 1.5592 - val_loss: 4.9701 - val_mae: 1.9716\n",
      "Epoch 203/500\n",
      "2290/2290 [==============================] - 2s 671us/step - loss: 4.4194 - mae: 1.5956 - val_loss: 4.8448 - val_mae: 1.9434\n",
      "Epoch 204/500\n",
      "2290/2290 [==============================] - 2s 712us/step - loss: 31.8712 - mae: 1.7694 - val_loss: 5.6277 - val_mae: 2.1183\n",
      "Epoch 205/500\n",
      "2290/2290 [==============================] - 1s 612us/step - loss: 4.0742 - mae: 1.5941 - val_loss: 5.0544 - val_mae: 1.9911\n",
      "Epoch 206/500\n",
      "2290/2290 [==============================] - 1s 653us/step - loss: 3.8909 - mae: 1.5450 - val_loss: 4.6834 - val_mae: 1.9063\n",
      "Epoch 207/500\n",
      "2290/2290 [==============================] - 2s 738us/step - loss: 4.4613 - mae: 1.5708 - val_loss: 3.7195 - val_mae: 1.6668\n",
      "Epoch 208/500\n",
      "2290/2290 [==============================] - 1s 629us/step - loss: 3.9863 - mae: 1.5503 - val_loss: 5.1257 - val_mae: 2.0066\n",
      "Epoch 209/500\n",
      "2290/2290 [==============================] - 2s 744us/step - loss: 59.3051 - mae: 1.7277 - val_loss: 6.5638 - val_mae: 2.3172\n",
      "Epoch 210/500\n",
      "2290/2290 [==============================] - 1s 646us/step - loss: 4.1072 - mae: 1.5572 - val_loss: 4.9079 - val_mae: 1.9550\n",
      "Epoch 211/500\n",
      "2290/2290 [==============================] - 1s 650us/step - loss: 3.8307 - mae: 1.5146 - val_loss: 5.4000 - val_mae: 2.0622\n",
      "Epoch 212/500\n",
      "2290/2290 [==============================] - 1s 638us/step - loss: 3.8901 - mae: 1.5036 - val_loss: 5.2801 - val_mae: 2.0350\n",
      "Epoch 213/500\n",
      "2290/2290 [==============================] - 1s 644us/step - loss: 6.1674 - mae: 1.5372 - val_loss: 6.1063 - val_mae: 2.2165\n",
      "Epoch 214/500\n",
      "2290/2290 [==============================] - 1s 611us/step - loss: 3.7087 - mae: 1.4920 - val_loss: 5.4524 - val_mae: 2.0722\n",
      "Epoch 215/500\n",
      "2290/2290 [==============================] - 1s 602us/step - loss: 5.0605 - mae: 1.5755 - val_loss: 6.1803 - val_mae: 2.2337\n",
      "Epoch 216/500\n",
      "2290/2290 [==============================] - 2s 718us/step - loss: 4.4270 - mae: 1.5435 - val_loss: 6.3541 - val_mae: 2.2712\n",
      "Epoch 217/500\n",
      "2290/2290 [==============================] - 2s 848us/step - loss: 3.6667 - mae: 1.4702 - val_loss: 5.6149 - val_mae: 2.1095\n",
      "Epoch 218/500\n",
      "2290/2290 [==============================] - 2s 752us/step - loss: 3.4402 - mae: 1.4452 - val_loss: 5.5285 - val_mae: 2.0900\n",
      "Epoch 219/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 127.2375 - mae: 1.7343 - val_loss: 4.1268 - val_mae: 1.7647\n",
      "Epoch 220/500\n",
      "2290/2290 [==============================] - 2s 735us/step - loss: 95.0963 - mae: 1.7396 - val_loss: 5.6731 - val_mae: 2.1222\n",
      "Epoch 221/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 3.8498 - mae: 1.4680 - val_loss: 6.1826 - val_mae: 2.2339\n",
      "Epoch 222/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 3.4383 - mae: 1.4405 - val_loss: 7.2930 - val_mae: 2.4635\n",
      "Epoch 223/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 3.4548 - mae: 1.4512 - val_loss: 6.3797 - val_mae: 2.2766\n",
      "Epoch 224/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 4.5476 - mae: 1.4774 - val_loss: 5.0142 - val_mae: 1.9726\n",
      "Epoch 225/500\n",
      "2290/2290 [==============================] - 2s 756us/step - loss: 22.5207 - mae: 1.6255 - val_loss: 4.8503 - val_mae: 1.9325\n",
      "Epoch 226/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 3.7488 - mae: 1.4910 - val_loss: 5.1657 - val_mae: 2.0066\n",
      "Epoch 227/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 3.4464 - mae: 1.4529 - val_loss: 5.6308 - val_mae: 2.1120\n",
      "Epoch 228/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 3.3943 - mae: 1.4537 - val_loss: 5.9986 - val_mae: 2.1933\n",
      "Epoch 229/500\n",
      "2290/2290 [==============================] - 2s 845us/step - loss: 3.3847 - mae: 1.4530 - val_loss: 6.2970 - val_mae: 2.2581\n",
      "Epoch 230/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 5.3780 - mae: 1.4704 - val_loss: 5.7073 - val_mae: 2.1291\n",
      "Epoch 231/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 3.6380 - mae: 1.4504 - val_loss: 6.5081 - val_mae: 2.3025\n",
      "Epoch 232/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 53.7037 - mae: 1.6494 - val_loss: 7.6027 - val_mae: 2.5255\n",
      "Epoch 233/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 3.8857 - mae: 1.4444 - val_loss: 6.2995 - val_mae: 2.2609\n",
      "Epoch 234/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 3.2720 - mae: 1.4161 - val_loss: 7.0938 - val_mae: 2.4249\n",
      "Epoch 235/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 3.8334 - mae: 1.4311 - val_loss: 6.9566 - val_mae: 2.3964\n",
      "Epoch 236/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 386.0976 - mae: 1.8982 - val_loss: 1.8472 - val_mae: 1.1047\n",
      "Epoch 237/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 5.7805 - mae: 1.5406 - val_loss: 6.0525 - val_mae: 2.2092\n",
      "Epoch 238/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 3.4292 - mae: 1.4212 - val_loss: 6.2105 - val_mae: 2.2406\n",
      "Epoch 239/500\n",
      "2290/2290 [==============================] - 2s 804us/step - loss: 3.7185 - mae: 1.4306 - val_loss: 6.5029 - val_mae: 2.3062\n",
      "Epoch 240/500\n",
      "2290/2290 [==============================] - 2s 738us/step - loss: 3.6588 - mae: 1.4376 - val_loss: 6.7817 - val_mae: 2.3626\n",
      "Epoch 241/500\n",
      "2290/2290 [==============================] - 2s 852us/step - loss: 3.2058 - mae: 1.3888 - val_loss: 6.7401 - val_mae: 2.3529\n",
      "Epoch 242/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 3.5268 - mae: 1.4317 - val_loss: 6.3722 - val_mae: 2.2768\n",
      "Epoch 243/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 3.3149 - mae: 1.3954 - val_loss: 6.7351 - val_mae: 2.3530\n",
      "Epoch 244/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 3.1981 - mae: 1.3944 - val_loss: 6.6013 - val_mae: 2.3250\n",
      "Epoch 245/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 208.8061 - mae: 1.7990 - val_loss: 6.9417 - val_mae: 2.3946\n",
      "Epoch 246/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 123.9031 - mae: 1.7207 - val_loss: 7.6965 - val_mae: 2.5482\n",
      "Epoch 247/500\n",
      "2290/2290 [==============================] - 2s 895us/step - loss: 3.2721 - mae: 1.4230 - val_loss: 7.2051 - val_mae: 2.4517\n",
      "Epoch 248/500\n",
      "2290/2290 [==============================] - 2s 824us/step - loss: 3.4858 - mae: 1.4304 - val_loss: 6.5146 - val_mae: 2.3070\n",
      "Epoch 249/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 3.7438 - mae: 1.4301 - val_loss: 6.7912 - val_mae: 2.3622\n",
      "Epoch 250/500\n",
      "2290/2290 [==============================] - 2s 735us/step - loss: 3.2903 - mae: 1.4000 - val_loss: 6.7768 - val_mae: 2.3585\n",
      "Epoch 251/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 3.1846 - mae: 1.3955 - val_loss: 6.7903 - val_mae: 2.3600\n",
      "Epoch 252/500\n",
      "2290/2290 [==============================] - 2s 796us/step - loss: 14.1783 - mae: 1.4607 - val_loss: 7.4333 - val_mae: 2.4871\n",
      "Epoch 253/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 3.1537 - mae: 1.3980 - val_loss: 6.7722 - val_mae: 2.3540\n",
      "Epoch 254/500\n",
      "2290/2290 [==============================] - 2s 772us/step - loss: 3.0493 - mae: 1.3816 - val_loss: 6.4716 - val_mae: 2.2902\n",
      "Epoch 255/500\n",
      "2290/2290 [==============================] - 2s 724us/step - loss: 3.2168 - mae: 1.3939 - val_loss: 7.4056 - val_mae: 2.4795\n",
      "Epoch 256/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 3.1711 - mae: 1.3896 - val_loss: 7.5299 - val_mae: 2.5027\n",
      "Epoch 257/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 4.3383 - mae: 1.4087 - val_loss: 7.8812 - val_mae: 2.5711\n",
      "Epoch 258/500\n",
      "2290/2290 [==============================] - 2s 745us/step - loss: 3.0261 - mae: 1.3703 - val_loss: 7.6829 - val_mae: 2.5331\n",
      "Epoch 259/500\n",
      "2290/2290 [==============================] - 2s 759us/step - loss: 3.0006 - mae: 1.3463 - val_loss: 7.2108 - val_mae: 2.4389\n",
      "Epoch 260/500\n",
      "2290/2290 [==============================] - 2s 736us/step - loss: 3.2356 - mae: 1.4045 - val_loss: 7.4263 - val_mae: 2.4819\n",
      "Epoch 261/500\n",
      "2290/2290 [==============================] - 2s 718us/step - loss: 3.0351 - mae: 1.3561 - val_loss: 8.1026 - val_mae: 2.6109\n",
      "Epoch 262/500\n",
      "2290/2290 [==============================] - 2s 735us/step - loss: 3.1148 - mae: 1.3712 - val_loss: 7.2602 - val_mae: 2.4465\n",
      "Epoch 263/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 3.0477 - mae: 1.3674 - val_loss: 7.8168 - val_mae: 2.5545\n",
      "Epoch 264/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 3.0345 - mae: 1.3530 - val_loss: 7.4456 - val_mae: 2.4838\n",
      "Epoch 265/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 3.1318 - mae: 1.3709 - val_loss: 7.3438 - val_mae: 2.4642\n",
      "Epoch 266/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 2.9531 - mae: 1.3476 - val_loss: 7.5320 - val_mae: 2.5016\n",
      "Epoch 267/500\n",
      "2290/2290 [==============================] - 2s 870us/step - loss: 2.8743 - mae: 1.3199 - val_loss: 7.1067 - val_mae: 2.4168\n",
      "Epoch 268/500\n",
      "2290/2290 [==============================] - 2s 782us/step - loss: 2.9630 - mae: 1.3559 - val_loss: 7.6389 - val_mae: 2.5194\n",
      "Epoch 269/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 3.0345 - mae: 1.3668 - val_loss: 8.1790 - val_mae: 2.6228\n",
      "Epoch 270/500\n",
      "2290/2290 [==============================] - 2s 762us/step - loss: 6.3911 - mae: 1.4002 - val_loss: 9.9760 - val_mae: 2.9412\n",
      "Epoch 271/500\n",
      "2290/2290 [==============================] - 2s 774us/step - loss: 3.0804 - mae: 1.3778 - val_loss: 8.9041 - val_mae: 2.7561\n",
      "Epoch 272/500\n",
      "2290/2290 [==============================] - 2s 766us/step - loss: 2.9148 - mae: 1.3308 - val_loss: 8.0229 - val_mae: 2.5932\n",
      "Epoch 273/500\n",
      "2290/2290 [==============================] - 2s 729us/step - loss: 3.1406 - mae: 1.3831 - val_loss: 8.2072 - val_mae: 2.6291\n",
      "Epoch 274/500\n",
      "2290/2290 [==============================] - 2s 723us/step - loss: 2.9017 - mae: 1.3308 - val_loss: 8.9162 - val_mae: 2.7593\n",
      "Epoch 275/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 3.1372 - mae: 1.3447 - val_loss: 8.1557 - val_mae: 2.6212\n",
      "Epoch 276/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 2.9626 - mae: 1.3393 - val_loss: 8.1989 - val_mae: 2.6289\n",
      "Epoch 277/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 3.0996 - mae: 1.3616 - val_loss: 7.3846 - val_mae: 2.4737\n",
      "Epoch 278/500\n",
      "2290/2290 [==============================] - 2s 747us/step - loss: 2.9738 - mae: 1.3431 - val_loss: 7.9760 - val_mae: 2.5883\n",
      "Epoch 279/500\n",
      "2290/2290 [==============================] - 2s 724us/step - loss: 15.4988 - mae: 1.4199 - val_loss: 7.8489 - val_mae: 2.5657\n",
      "Epoch 280/500\n",
      "2290/2290 [==============================] - 2s 759us/step - loss: 2.9720 - mae: 1.3477 - val_loss: 8.8778 - val_mae: 2.7551\n",
      "Epoch 281/500\n",
      "2290/2290 [==============================] - 2s 714us/step - loss: 2.9898 - mae: 1.3560 - val_loss: 8.1086 - val_mae: 2.6114\n",
      "Epoch 282/500\n",
      "2290/2290 [==============================] - 2s 734us/step - loss: 2.9519 - mae: 1.3282 - val_loss: 9.2293 - val_mae: 2.8162\n",
      "Epoch 283/500\n",
      "2290/2290 [==============================] - 2s 752us/step - loss: 2.8147 - mae: 1.3082 - val_loss: 7.9362 - val_mae: 2.5795\n",
      "Epoch 284/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 2.8892 - mae: 1.3316 - val_loss: 7.9973 - val_mae: 2.5895\n",
      "Epoch 285/500\n",
      "2290/2290 [==============================] - 2s 760us/step - loss: 2.9283 - mae: 1.3295 - val_loss: 6.2826 - val_mae: 2.2440\n",
      "Epoch 286/500\n",
      "2290/2290 [==============================] - 2s 702us/step - loss: 2.9925 - mae: 1.3248 - val_loss: 8.3462 - val_mae: 2.6556\n",
      "Epoch 287/500\n",
      "2290/2290 [==============================] - 2s 797us/step - loss: 2.9554 - mae: 1.3417 - val_loss: 8.1583 - val_mae: 2.6180\n",
      "Epoch 288/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 2.8646 - mae: 1.2972 - val_loss: 9.6976 - val_mae: 2.8932\n",
      "Epoch 289/500\n",
      "2290/2290 [==============================] - 2s 752us/step - loss: 2.8953 - mae: 1.3378 - val_loss: 8.3957 - val_mae: 2.6628\n",
      "Epoch 290/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 2.8581 - mae: 1.3184 - val_loss: 8.8241 - val_mae: 2.7423\n",
      "Epoch 291/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 2.7196 - mae: 1.2845 - val_loss: 9.2618 - val_mae: 2.8206\n",
      "Epoch 292/500\n",
      "2290/2290 [==============================] - 2s 744us/step - loss: 2.8983 - mae: 1.3119 - val_loss: 7.5496 - val_mae: 2.5073\n",
      "Epoch 293/500\n",
      "2290/2290 [==============================] - 2s 725us/step - loss: 2.7917 - mae: 1.3072 - val_loss: 9.3688 - val_mae: 2.8399\n",
      "Epoch 294/500\n",
      "2290/2290 [==============================] - 2s 696us/step - loss: 2.9656 - mae: 1.3180 - val_loss: 7.4045 - val_mae: 2.4771\n",
      "Epoch 295/500\n",
      "2290/2290 [==============================] - 2s 808us/step - loss: 2.7864 - mae: 1.3106 - val_loss: 9.3469 - val_mae: 2.8341\n",
      "Epoch 296/500\n",
      "2290/2290 [==============================] - 2s 725us/step - loss: 2.7714 - mae: 1.2983 - val_loss: 9.2108 - val_mae: 2.8116\n",
      "Epoch 297/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 2.6929 - mae: 1.2895 - val_loss: 11.8086 - val_mae: 3.2378\n",
      "Epoch 298/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 2.8284 - mae: 1.3014 - val_loss: 8.5711 - val_mae: 2.7014\n",
      "Epoch 299/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 2.8353 - mae: 1.3055 - val_loss: 8.1200 - val_mae: 2.6166\n",
      "Epoch 300/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 2.7444 - mae: 1.2903 - val_loss: 9.1443 - val_mae: 2.8024\n",
      "Epoch 301/500\n",
      "2290/2290 [==============================] - 2s 719us/step - loss: 2.6666 - mae: 1.2777 - val_loss: 8.0049 - val_mae: 2.5954\n",
      "Epoch 302/500\n",
      "2290/2290 [==============================] - 2s 741us/step - loss: 3.4066 - mae: 1.2909 - val_loss: 9.0380 - val_mae: 2.7841\n",
      "Epoch 303/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 2.6668 - mae: 1.2763 - val_loss: 9.1308 - val_mae: 2.7999\n",
      "Epoch 304/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 2.6460 - mae: 1.2591 - val_loss: 8.2548 - val_mae: 2.6423\n",
      "Epoch 305/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 2.7960 - mae: 1.3101 - val_loss: 8.9326 - val_mae: 2.7688\n",
      "Epoch 306/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 2.8268 - mae: 1.3198 - val_loss: 8.9957 - val_mae: 2.7797\n",
      "Epoch 307/500\n",
      "2290/2290 [==============================] - 2s 840us/step - loss: 2.5662 - mae: 1.2467 - val_loss: 8.3174 - val_mae: 2.6552\n",
      "Epoch 308/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 2.9104 - mae: 1.2912 - val_loss: 9.1605 - val_mae: 2.8062\n",
      "Epoch 309/500\n",
      "2290/2290 [==============================] - 2s 892us/step - loss: 3.3112 - mae: 1.2992 - val_loss: 10.6366 - val_mae: 3.0559\n",
      "Epoch 310/500\n",
      "2290/2290 [==============================] - 2s 820us/step - loss: 2.6878 - mae: 1.2791 - val_loss: 8.0433 - val_mae: 2.6070\n",
      "Epoch 311/500\n",
      "2290/2290 [==============================] - 2s 838us/step - loss: 7.1291 - mae: 1.3154 - val_loss: 7.0771 - val_mae: 2.4179\n",
      "Epoch 312/500\n",
      "2290/2290 [==============================] - 2s 732us/step - loss: 2.6994 - mae: 1.2836 - val_loss: 9.1394 - val_mae: 2.8044\n",
      "Epoch 313/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 2.6577 - mae: 1.2786 - val_loss: 8.8611 - val_mae: 2.7551\n",
      "Epoch 314/500\n",
      "2290/2290 [==============================] - 2s 728us/step - loss: 2.8329 - mae: 1.3043 - val_loss: 8.3010 - val_mae: 2.6571\n",
      "Epoch 315/500\n",
      "2290/2290 [==============================] - 2s 725us/step - loss: 2.5631 - mae: 1.2390 - val_loss: 8.1166 - val_mae: 2.6227\n",
      "Epoch 316/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 12.6695 - mae: 1.3787 - val_loss: 8.3605 - val_mae: 2.6613\n",
      "Epoch 317/500\n",
      "2290/2290 [==============================] - 2s 813us/step - loss: 2.6429 - mae: 1.2806 - val_loss: 9.2506 - val_mae: 2.8217\n",
      "Epoch 318/500\n",
      "2290/2290 [==============================] - 2s 816us/step - loss: 2.5143 - mae: 1.2372 - val_loss: 9.0138 - val_mae: 2.7807\n",
      "Epoch 319/500\n",
      "2290/2290 [==============================] - 2s 728us/step - loss: 2.5826 - mae: 1.2569 - val_loss: 9.1990 - val_mae: 2.8127\n",
      "Epoch 320/500\n",
      "2290/2290 [==============================] - 2s 754us/step - loss: 2.6434 - mae: 1.2683 - val_loss: 8.5175 - val_mae: 2.6934\n",
      "Epoch 321/500\n",
      "2290/2290 [==============================] - 2s 705us/step - loss: 2.4126 - mae: 1.2205 - val_loss: 9.7931 - val_mae: 2.9183\n",
      "Epoch 322/500\n",
      "2290/2290 [==============================] - 2s 729us/step - loss: 2.5492 - mae: 1.2446 - val_loss: 9.1218 - val_mae: 2.8027\n",
      "Epoch 323/500\n",
      "2290/2290 [==============================] - 2s 730us/step - loss: 2.5595 - mae: 1.2497 - val_loss: 9.8371 - val_mae: 2.9253\n",
      "Epoch 324/500\n",
      "2290/2290 [==============================] - 2s 714us/step - loss: 2.5049 - mae: 1.2322 - val_loss: 9.2820 - val_mae: 2.8299\n",
      "Epoch 325/500\n",
      "2290/2290 [==============================] - 2s 679us/step - loss: 2.4894 - mae: 1.2367 - val_loss: 8.9822 - val_mae: 2.7810\n",
      "Epoch 326/500\n",
      "2290/2290 [==============================] - 1s 609us/step - loss: 2.5494 - mae: 1.2415 - val_loss: 9.4391 - val_mae: 2.8581\n",
      "Epoch 327/500\n",
      "2290/2290 [==============================] - 1s 597us/step - loss: 2.5775 - mae: 1.2432 - val_loss: 9.3312 - val_mae: 2.8409\n",
      "Epoch 328/500\n",
      "2290/2290 [==============================] - 2s 672us/step - loss: 2.5829 - mae: 1.2434 - val_loss: 9.4701 - val_mae: 2.8650\n",
      "Epoch 329/500\n",
      "2290/2290 [==============================] - 2s 680us/step - loss: 2.5011 - mae: 1.2192 - val_loss: 7.8417 - val_mae: 2.5724\n",
      "Epoch 330/500\n",
      "2290/2290 [==============================] - 1s 644us/step - loss: 2.4459 - mae: 1.2134 - val_loss: 8.1826 - val_mae: 2.6361\n",
      "Epoch 331/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 2.5218 - mae: 1.2427 - val_loss: 7.0057 - val_mae: 2.4090\n",
      "Epoch 332/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 2.4591 - mae: 1.2125 - val_loss: 9.1802 - val_mae: 2.8158\n",
      "Epoch 333/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 2.4780 - mae: 1.2263 - val_loss: 9.8264 - val_mae: 2.9249\n",
      "Epoch 334/500\n",
      "2290/2290 [==============================] - 2s 776us/step - loss: 2.3510 - mae: 1.1965 - val_loss: 9.9098 - val_mae: 2.9412\n",
      "Epoch 335/500\n",
      "2290/2290 [==============================] - 2s 752us/step - loss: 2.4358 - mae: 1.2218 - val_loss: 7.0508 - val_mae: 2.4191\n",
      "Epoch 336/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 2.5256 - mae: 1.2414 - val_loss: 8.3523 - val_mae: 2.6697\n",
      "Epoch 337/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 2.4319 - mae: 1.2180 - val_loss: 8.1832 - val_mae: 2.6387\n",
      "Epoch 338/500\n",
      "2290/2290 [==============================] - 2s 819us/step - loss: 2.4947 - mae: 1.2247 - val_loss: 9.6902 - val_mae: 2.9059\n",
      "Epoch 339/500\n",
      "2290/2290 [==============================] - 2s 776us/step - loss: 2.4260 - mae: 1.2111 - val_loss: 8.7065 - val_mae: 2.7332\n",
      "Epoch 340/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 2.4687 - mae: 1.2169 - val_loss: 7.8693 - val_mae: 2.5810\n",
      "Epoch 341/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 2.3673 - mae: 1.2024 - val_loss: 8.3654 - val_mae: 2.6753\n",
      "Epoch 342/500\n",
      "2290/2290 [==============================] - 2s 876us/step - loss: 2.4059 - mae: 1.1980 - val_loss: 8.6701 - val_mae: 2.7305\n",
      "Epoch 343/500\n",
      "2290/2290 [==============================] - 2s 813us/step - loss: 2.3939 - mae: 1.2177 - val_loss: 9.3096 - val_mae: 2.8468\n",
      "Epoch 344/500\n",
      "2290/2290 [==============================] - 2s 834us/step - loss: 2.3318 - mae: 1.1834 - val_loss: 8.7509 - val_mae: 2.7486\n",
      "Epoch 345/500\n",
      "2290/2290 [==============================] - 2s 760us/step - loss: 2.4272 - mae: 1.2032 - val_loss: 8.2986 - val_mae: 2.6673\n",
      "Epoch 346/500\n",
      "2290/2290 [==============================] - 2s 677us/step - loss: 2.3693 - mae: 1.1917 - val_loss: 10.8101 - val_mae: 3.0949\n",
      "Epoch 347/500\n",
      "2290/2290 [==============================] - 1s 620us/step - loss: 2.3864 - mae: 1.2113 - val_loss: 8.4034 - val_mae: 2.6877\n",
      "Epoch 348/500\n",
      "2290/2290 [==============================] - 2s 687us/step - loss: 2.3402 - mae: 1.1946 - val_loss: 7.4727 - val_mae: 2.5123\n",
      "Epoch 349/500\n",
      "2290/2290 [==============================] - 2s 660us/step - loss: 2.3343 - mae: 1.1775 - val_loss: 8.4422 - val_mae: 2.6957\n",
      "Epoch 350/500\n",
      "2290/2290 [==============================] - 1s 633us/step - loss: 2.3623 - mae: 1.1808 - val_loss: 10.5292 - val_mae: 3.0491\n",
      "Epoch 351/500\n",
      "2290/2290 [==============================] - 1s 624us/step - loss: 2.2915 - mae: 1.1723 - val_loss: 8.6877 - val_mae: 2.7400\n",
      "Epoch 352/500\n",
      "2290/2290 [==============================] - 2s 715us/step - loss: 2.7871 - mae: 1.1867 - val_loss: 8.0776 - val_mae: 2.6223\n",
      "Epoch 353/500\n",
      "2290/2290 [==============================] - 2s 674us/step - loss: 2.2769 - mae: 1.1702 - val_loss: 8.9792 - val_mae: 2.7915\n",
      "Epoch 354/500\n",
      "2290/2290 [==============================] - 1s 652us/step - loss: 2.3361 - mae: 1.1905 - val_loss: 9.3527 - val_mae: 2.8562\n",
      "Epoch 355/500\n",
      "2290/2290 [==============================] - 2s 756us/step - loss: 2.8262 - mae: 1.2147 - val_loss: 10.3539 - val_mae: 3.0244\n",
      "Epoch 356/500\n",
      "2290/2290 [==============================] - 1s 625us/step - loss: 2.2402 - mae: 1.1625 - val_loss: 8.6124 - val_mae: 2.7299\n",
      "Epoch 357/500\n",
      "2290/2290 [==============================] - 2s 684us/step - loss: 31.9899 - mae: 1.3303 - val_loss: 6.7560 - val_mae: 2.3410\n",
      "Epoch 358/500\n",
      "2290/2290 [==============================] - 1s 607us/step - loss: 2.8217 - mae: 1.1888 - val_loss: 9.5871 - val_mae: 2.8970\n",
      "Epoch 359/500\n",
      "2290/2290 [==============================] - 1s 620us/step - loss: 2.3033 - mae: 1.1878 - val_loss: 8.7159 - val_mae: 2.7452\n",
      "Epoch 360/500\n",
      "2290/2290 [==============================] - 2s 711us/step - loss: 2.3386 - mae: 1.1845 - val_loss: 9.6425 - val_mae: 2.9048\n",
      "Epoch 361/500\n",
      "2290/2290 [==============================] - 2s 737us/step - loss: 2.2581 - mae: 1.1652 - val_loss: 9.9629 - val_mae: 2.9593\n",
      "Epoch 362/500\n",
      "2290/2290 [==============================] - 2s 698us/step - loss: 2.2149 - mae: 1.1569 - val_loss: 10.2282 - val_mae: 3.0034\n",
      "Epoch 363/500\n",
      "2290/2290 [==============================] - 2s 723us/step - loss: 2.2245 - mae: 1.1521 - val_loss: 7.5500 - val_mae: 2.5258\n",
      "Epoch 364/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 2.2252 - mae: 1.1565 - val_loss: 8.5539 - val_mae: 2.7144\n",
      "Epoch 365/500\n",
      "2290/2290 [==============================] - 2s 728us/step - loss: 2.3422 - mae: 1.1854 - val_loss: 7.9275 - val_mae: 2.6015\n",
      "Epoch 366/500\n",
      "2290/2290 [==============================] - 2s 742us/step - loss: 2.3351 - mae: 1.1836 - val_loss: 9.5075 - val_mae: 2.8846\n",
      "Epoch 367/500\n",
      "2290/2290 [==============================] - 2s 736us/step - loss: 2.2546 - mae: 1.1526 - val_loss: 9.1473 - val_mae: 2.8211\n",
      "Epoch 368/500\n",
      "2290/2290 [==============================] - 2s 722us/step - loss: 2.2154 - mae: 1.1490 - val_loss: 9.0068 - val_mae: 2.7966\n",
      "Epoch 369/500\n",
      "2290/2290 [==============================] - 2s 704us/step - loss: 2.4936 - mae: 1.1775 - val_loss: 7.5458 - val_mae: 2.5205\n",
      "Epoch 370/500\n",
      "2290/2290 [==============================] - 2s 754us/step - loss: 2.3971 - mae: 1.1736 - val_loss: 9.6609 - val_mae: 2.9025\n",
      "Epoch 371/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 2.3324 - mae: 1.1854 - val_loss: 8.8555 - val_mae: 2.7647\n",
      "Epoch 372/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 2.1136 - mae: 1.1141 - val_loss: 9.0820 - val_mae: 2.8048\n",
      "Epoch 373/500\n",
      "2290/2290 [==============================] - 2s 747us/step - loss: 2.1490 - mae: 1.1306 - val_loss: 7.5470 - val_mae: 2.5223\n",
      "Epoch 374/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 2.2175 - mae: 1.1579 - val_loss: 8.8185 - val_mae: 2.7621\n",
      "Epoch 375/500\n",
      "2290/2290 [==============================] - 2s 714us/step - loss: 2.1808 - mae: 1.1391 - val_loss: 10.2605 - val_mae: 3.0056\n",
      "Epoch 376/500\n",
      "2290/2290 [==============================] - 2s 724us/step - loss: 2.2044 - mae: 1.1445 - val_loss: 8.7709 - val_mae: 2.7509\n",
      "Epoch 377/500\n",
      "2290/2290 [==============================] - 2s 725us/step - loss: 2.1091 - mae: 1.1173 - val_loss: 8.4574 - val_mae: 2.6964\n",
      "Epoch 378/500\n",
      "2290/2290 [==============================] - 2s 763us/step - loss: 2.1327 - mae: 1.1235 - val_loss: 9.0574 - val_mae: 2.8035\n",
      "Epoch 379/500\n",
      "2290/2290 [==============================] - 2s 745us/step - loss: 2.1510 - mae: 1.1189 - val_loss: 7.8503 - val_mae: 2.5831\n",
      "Epoch 380/500\n",
      "2290/2290 [==============================] - 2s 834us/step - loss: 2.1877 - mae: 1.1359 - val_loss: 10.7501 - val_mae: 3.0876\n",
      "Epoch 381/500\n",
      "2290/2290 [==============================] - 2s 857us/step - loss: 2.1016 - mae: 1.1158 - val_loss: 7.8721 - val_mae: 2.5906\n",
      "Epoch 382/500\n",
      "2290/2290 [==============================] - 2s 775us/step - loss: 2.1554 - mae: 1.1424 - val_loss: 7.3326 - val_mae: 2.4894\n",
      "Epoch 383/500\n",
      "2290/2290 [==============================] - 2s 857us/step - loss: 2.2233 - mae: 1.1502 - val_loss: 9.3463 - val_mae: 2.8590\n",
      "Epoch 384/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 2.3295 - mae: 1.1760 - val_loss: 9.8495 - val_mae: 2.9450\n",
      "Epoch 385/500\n",
      "2290/2290 [==============================] - 2s 716us/step - loss: 2.2199 - mae: 1.1477 - val_loss: 9.4095 - val_mae: 2.8707\n",
      "Epoch 386/500\n",
      "2290/2290 [==============================] - 2s 759us/step - loss: 2.2127 - mae: 1.1462 - val_loss: 7.7676 - val_mae: 2.5754\n",
      "Epoch 387/500\n",
      "2290/2290 [==============================] - 2s 709us/step - loss: 2.3792 - mae: 1.1365 - val_loss: 10.5072 - val_mae: 3.0460\n",
      "Epoch 388/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 2.2248 - mae: 1.1580 - val_loss: 8.2661 - val_mae: 2.6593\n",
      "Epoch 389/500\n",
      "2290/2290 [==============================] - 2s 872us/step - loss: 3.4278 - mae: 1.1557 - val_loss: 7.1342 - val_mae: 2.4383\n",
      "Epoch 390/500\n",
      "2290/2290 [==============================] - 2s 892us/step - loss: 2.1879 - mae: 1.1387 - val_loss: 9.3034 - val_mae: 2.8486\n",
      "Epoch 391/500\n",
      "2290/2290 [==============================] - 2s 782us/step - loss: 3.7603 - mae: 1.1814 - val_loss: 7.7560 - val_mae: 2.5631\n",
      "Epoch 392/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 2.3357 - mae: 1.1522 - val_loss: 7.1484 - val_mae: 2.4513\n",
      "Epoch 393/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 2.1307 - mae: 1.1351 - val_loss: 8.7007 - val_mae: 2.7460\n",
      "Epoch 394/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 2.0128 - mae: 1.0943 - val_loss: 8.1377 - val_mae: 2.6426\n",
      "Epoch 395/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 2.2502 - mae: 1.1348 - val_loss: 7.2553 - val_mae: 2.4746\n",
      "Epoch 396/500\n",
      "2290/2290 [==============================] - 2s 888us/step - loss: 2.1249 - mae: 1.1205 - val_loss: 8.4040 - val_mae: 2.6915\n",
      "Epoch 397/500\n",
      "2290/2290 [==============================] - 2s 858us/step - loss: 2.1429 - mae: 1.1384 - val_loss: 9.3306 - val_mae: 2.8582\n",
      "Epoch 398/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 2.1415 - mae: 1.1323 - val_loss: 6.6802 - val_mae: 2.3639\n",
      "Epoch 399/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 2.1373 - mae: 1.1199 - val_loss: 7.0081 - val_mae: 2.4297\n",
      "Epoch 400/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 2.7822 - mae: 1.1614 - val_loss: 9.4603 - val_mae: 2.8810\n",
      "Epoch 401/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 2.3576 - mae: 1.1585 - val_loss: 8.2747 - val_mae: 2.6715\n",
      "Epoch 402/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 2.1309 - mae: 1.1330 - val_loss: 7.8725 - val_mae: 2.5975\n",
      "Epoch 403/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 2.0494 - mae: 1.1082 - val_loss: 8.6458 - val_mae: 2.7443\n",
      "Epoch 404/500\n",
      "2290/2290 [==============================] - 2s 729us/step - loss: 1.9729 - mae: 1.0937 - val_loss: 8.9947 - val_mae: 2.8062\n",
      "Epoch 405/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 62.9614 - mae: 1.3356 - val_loss: 9.3391 - val_mae: 2.8744\n",
      "Epoch 406/500\n",
      "2290/2290 [==============================] - 2s 856us/step - loss: 2.2354 - mae: 1.1366 - val_loss: 7.9830 - val_mae: 2.6306\n",
      "Epoch 407/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 2.1181 - mae: 1.1255 - val_loss: 8.1464 - val_mae: 2.6575\n",
      "Epoch 408/500\n",
      "2290/2290 [==============================] - 2s 813us/step - loss: 2.1082 - mae: 1.1052 - val_loss: 7.8477 - val_mae: 2.6005\n",
      "Epoch 409/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 4.3008 - mae: 1.1307 - val_loss: 7.7559 - val_mae: 2.5862\n",
      "Epoch 410/500\n",
      "2290/2290 [==============================] - 2s 828us/step - loss: 2.0830 - mae: 1.1158 - val_loss: 7.7728 - val_mae: 2.5867\n",
      "Epoch 411/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 2.0641 - mae: 1.1039 - val_loss: 8.9087 - val_mae: 2.7948\n",
      "Epoch 412/500\n",
      "2290/2290 [==============================] - 2s 835us/step - loss: 2.1417 - mae: 1.1121 - val_loss: 6.9718 - val_mae: 2.4333\n",
      "Epoch 413/500\n",
      "2290/2290 [==============================] - 2s 743us/step - loss: 2.0202 - mae: 1.0904 - val_loss: 7.9965 - val_mae: 2.6306\n",
      "Epoch 414/500\n",
      "2290/2290 [==============================] - 2s 772us/step - loss: 2.0533 - mae: 1.0962 - val_loss: 8.2341 - val_mae: 2.6734\n",
      "Epoch 415/500\n",
      "2290/2290 [==============================] - 2s 867us/step - loss: 2.0210 - mae: 1.0900 - val_loss: 8.4373 - val_mae: 2.7138\n",
      "Epoch 416/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 39.0445 - mae: 1.2843 - val_loss: 7.1713 - val_mae: 2.4703\n",
      "Epoch 417/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 2.0882 - mae: 1.1221 - val_loss: 8.2576 - val_mae: 2.6772\n",
      "Epoch 418/500\n",
      "2290/2290 [==============================] - 2s 736us/step - loss: 2.0263 - mae: 1.0921 - val_loss: 7.9610 - val_mae: 2.6250\n",
      "Epoch 419/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 2.0144 - mae: 1.0881 - val_loss: 8.5293 - val_mae: 2.7293\n",
      "Epoch 420/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 2.0421 - mae: 1.0953 - val_loss: 8.6628 - val_mae: 2.7532\n",
      "Epoch 421/500\n",
      "2290/2290 [==============================] - 2s 770us/step - loss: 2.1069 - mae: 1.1049 - val_loss: 7.2627 - val_mae: 2.4884\n",
      "Epoch 422/500\n",
      "2290/2290 [==============================] - 2s 703us/step - loss: 2.1822 - mae: 1.1047 - val_loss: 7.1081 - val_mae: 2.4617\n",
      "Epoch 423/500\n",
      "2290/2290 [==============================] - 2s 882us/step - loss: 2.1005 - mae: 1.1165 - val_loss: 7.2551 - val_mae: 2.4880\n",
      "Epoch 424/500\n",
      "2290/2290 [==============================] - 2s 840us/step - loss: 6.7037 - mae: 1.1326 - val_loss: 8.9285 - val_mae: 2.7962\n",
      "Epoch 425/500\n",
      "2290/2290 [==============================] - 2s 785us/step - loss: 2.0405 - mae: 1.0940 - val_loss: 7.3715 - val_mae: 2.5123\n",
      "Epoch 426/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 1.9843 - mae: 1.0901 - val_loss: 7.6412 - val_mae: 2.5683\n",
      "Epoch 427/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 2.0412 - mae: 1.1035 - val_loss: 7.1067 - val_mae: 2.4644\n",
      "Epoch 428/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 1.9475 - mae: 1.0687 - val_loss: 8.9093 - val_mae: 2.8002\n",
      "Epoch 429/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 1.9817 - mae: 1.0892 - val_loss: 8.5092 - val_mae: 2.7294\n",
      "Epoch 430/500\n",
      "2290/2290 [==============================] - 2s 785us/step - loss: 2.0710 - mae: 1.1098 - val_loss: 8.4199 - val_mae: 2.7112\n",
      "Epoch 431/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 1.9649 - mae: 1.0630 - val_loss: 8.1861 - val_mae: 2.6690\n",
      "Epoch 432/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 2.0336 - mae: 1.0970 - val_loss: 8.5571 - val_mae: 2.7364\n",
      "Epoch 433/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 1.9512 - mae: 1.0762 - val_loss: 7.4852 - val_mae: 2.5368\n",
      "Epoch 434/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 2.0057 - mae: 1.0858 - val_loss: 7.9945 - val_mae: 2.6326\n",
      "Epoch 435/500\n",
      "2290/2290 [==============================] - 2s 837us/step - loss: 1.9655 - mae: 1.0834 - val_loss: 7.5467 - val_mae: 2.5479\n",
      "Epoch 436/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 1.9790 - mae: 1.0821 - val_loss: 6.8692 - val_mae: 2.4155\n",
      "Epoch 437/500\n",
      "2290/2290 [==============================] - 2s 733us/step - loss: 18.6058 - mae: 1.1848 - val_loss: 7.1332 - val_mae: 2.4755\n",
      "Epoch 438/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 1.9718 - mae: 1.0762 - val_loss: 7.4018 - val_mae: 2.5203\n",
      "Epoch 439/500\n",
      "2290/2290 [==============================] - 2s 723us/step - loss: 1.9029 - mae: 1.0663 - val_loss: 8.8742 - val_mae: 2.7937\n",
      "Epoch 440/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 2.0218 - mae: 1.0918 - val_loss: 7.6318 - val_mae: 2.5709\n",
      "Epoch 441/500\n",
      "2290/2290 [==============================] - 2s 721us/step - loss: 2.0368 - mae: 1.0872 - val_loss: 8.1289 - val_mae: 2.6591\n",
      "Epoch 442/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 1.9485 - mae: 1.0797 - val_loss: 7.1688 - val_mae: 2.4780\n",
      "Epoch 443/500\n",
      "2290/2290 [==============================] - 2s 727us/step - loss: 1.9830 - mae: 1.0809 - val_loss: 7.5248 - val_mae: 2.5470\n",
      "Epoch 444/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 1.9524 - mae: 1.0661 - val_loss: 8.1663 - val_mae: 2.6700\n",
      "Epoch 445/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 1.9876 - mae: 1.0806 - val_loss: 6.6024 - val_mae: 2.3629\n",
      "Epoch 446/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 1.9619 - mae: 1.0770 - val_loss: 7.4268 - val_mae: 2.5297\n",
      "Epoch 447/500\n",
      "2290/2290 [==============================] - 2s 841us/step - loss: 1.9575 - mae: 1.0796 - val_loss: 8.0378 - val_mae: 2.6484\n",
      "Epoch 448/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 1.9509 - mae: 1.0701 - val_loss: 7.4286 - val_mae: 2.5335\n",
      "Epoch 449/500\n",
      "2290/2290 [==============================] - 2s 772us/step - loss: 2.0040 - mae: 1.0839 - val_loss: 6.7216 - val_mae: 2.3908\n",
      "Epoch 450/500\n",
      "2290/2290 [==============================] - 2s 700us/step - loss: 1.9166 - mae: 1.0743 - val_loss: 8.9042 - val_mae: 2.8052\n",
      "Epoch 451/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 1.9498 - mae: 1.0737 - val_loss: 7.2494 - val_mae: 2.5022\n",
      "Epoch 452/500\n",
      "2290/2290 [==============================] - 2s 760us/step - loss: 7.1255 - mae: 1.1137 - val_loss: 6.9212 - val_mae: 2.4275\n",
      "Epoch 453/500\n",
      "2290/2290 [==============================] - 2s 686us/step - loss: 2.0045 - mae: 1.0765 - val_loss: 7.0893 - val_mae: 2.4651\n",
      "Epoch 454/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 1.9684 - mae: 1.0649 - val_loss: 7.4342 - val_mae: 2.5345\n",
      "Epoch 455/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 1.9614 - mae: 1.0523 - val_loss: 6.8653 - val_mae: 2.4203\n",
      "Epoch 456/500\n",
      "2290/2290 [==============================] - 2s 875us/step - loss: 1.9349 - mae: 1.0779 - val_loss: 6.7420 - val_mae: 2.3968\n",
      "Epoch 457/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 1.9755 - mae: 1.0776 - val_loss: 6.6709 - val_mae: 2.3818\n",
      "Epoch 458/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 45.4629 - mae: 1.3027 - val_loss: 7.0826 - val_mae: 2.4811\n",
      "Epoch 459/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 1.9481 - mae: 1.0771 - val_loss: 6.6154 - val_mae: 2.3874\n",
      "Epoch 460/500\n",
      "2290/2290 [==============================] - 2s 920us/step - loss: 5.9382 - mae: 1.1406 - val_loss: 6.7713 - val_mae: 2.4005\n",
      "Epoch 461/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 1.9041 - mae: 1.0672 - val_loss: 7.0398 - val_mae: 2.4534\n",
      "Epoch 462/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 1.9716 - mae: 1.0786 - val_loss: 7.8603 - val_mae: 2.6124\n",
      "Epoch 463/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 2.0079 - mae: 1.0967 - val_loss: 7.3706 - val_mae: 2.5194\n",
      "Epoch 464/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 1.9773 - mae: 1.0884 - val_loss: 7.9389 - val_mae: 2.6279\n",
      "Epoch 465/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 1.9801 - mae: 1.0842 - val_loss: 7.8341 - val_mae: 2.6079\n",
      "Epoch 466/500\n",
      "2290/2290 [==============================] - 2s 815us/step - loss: 1.9349 - mae: 1.0772 - val_loss: 7.4252 - val_mae: 2.5304\n",
      "Epoch 467/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 1.9479 - mae: 1.0676 - val_loss: 8.5917 - val_mae: 2.7484\n",
      "Epoch 468/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 1.9324 - mae: 1.0794 - val_loss: 7.2670 - val_mae: 2.4970\n",
      "Epoch 469/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 1.9381 - mae: 1.0698 - val_loss: 7.1277 - val_mae: 2.4710\n",
      "Epoch 470/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 1.9506 - mae: 1.0868 - val_loss: 6.3419 - val_mae: 2.3093\n",
      "Epoch 471/500\n",
      "2290/2290 [==============================] - 2s 736us/step - loss: 1.8858 - mae: 1.0502 - val_loss: 7.0841 - val_mae: 2.4630\n",
      "Epoch 472/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 1.9966 - mae: 1.0934 - val_loss: 7.9149 - val_mae: 2.6231\n",
      "Epoch 473/500\n",
      "2290/2290 [==============================] - 2s 727us/step - loss: 1.9165 - mae: 1.0682 - val_loss: 6.4595 - val_mae: 2.3323\n",
      "Epoch 474/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 1.9316 - mae: 1.0720 - val_loss: 6.8714 - val_mae: 2.4212\n",
      "Epoch 475/500\n",
      "2290/2290 [==============================] - 2s 853us/step - loss: 1.9727 - mae: 1.0856 - val_loss: 7.2996 - val_mae: 2.5076\n",
      "Epoch 476/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 1.8826 - mae: 1.0566 - val_loss: 6.9193 - val_mae: 2.4323\n",
      "Epoch 477/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 10.8508 - mae: 1.1123 - val_loss: 5.9529 - val_mae: 2.2479\n",
      "Epoch 478/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 1.9268 - mae: 1.0687 - val_loss: 6.9131 - val_mae: 2.4351\n",
      "Epoch 479/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 101.3953 - mae: 1.3715 - val_loss: 6.7581 - val_mae: 2.4018\n",
      "Epoch 480/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 48.5840 - mae: 1.2827 - val_loss: 7.1184 - val_mae: 2.4493\n",
      "Epoch 481/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 1.9330 - mae: 1.0765 - val_loss: 6.8655 - val_mae: 2.4094\n",
      "Epoch 482/500\n",
      "2290/2290 [==============================] - 2s 744us/step - loss: 1.9459 - mae: 1.0797 - val_loss: 8.1453 - val_mae: 2.6586\n",
      "Epoch 483/500\n",
      "2290/2290 [==============================] - 2s 785us/step - loss: 1.9746 - mae: 1.0759 - val_loss: 7.5292 - val_mae: 2.5455\n",
      "Epoch 484/500\n",
      "2290/2290 [==============================] - 2s 804us/step - loss: 1.9875 - mae: 1.0873 - val_loss: 6.8482 - val_mae: 2.4133\n",
      "Epoch 485/500\n",
      "2290/2290 [==============================] - 2s 759us/step - loss: 1.9144 - mae: 1.0707 - val_loss: 7.9911 - val_mae: 2.6340\n",
      "Epoch 486/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 1.9553 - mae: 1.0663 - val_loss: 7.5970 - val_mae: 2.5628\n",
      "Epoch 487/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 1.9764 - mae: 1.0765 - val_loss: 7.5553 - val_mae: 2.5557\n",
      "Epoch 488/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 1.9118 - mae: 1.0564 - val_loss: 7.6806 - val_mae: 2.5804\n",
      "Epoch 489/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 2280.6034 - mae: 2.4453 - val_loss: 6.2425 - val_mae: 2.3325\n",
      "Epoch 490/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 2.0852 - mae: 1.1111 - val_loss: 6.3631 - val_mae: 2.3531\n",
      "Epoch 491/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 2.0082 - mae: 1.0922 - val_loss: 6.6523 - val_mae: 2.4063\n",
      "Epoch 492/500\n",
      "2290/2290 [==============================] - 2s 837us/step - loss: 2.0391 - mae: 1.1128 - val_loss: 7.0282 - val_mae: 2.4759\n",
      "Epoch 493/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 1.9905 - mae: 1.0865 - val_loss: 6.6047 - val_mae: 2.3888\n",
      "Epoch 494/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 1.9874 - mae: 1.0944 - val_loss: 6.9488 - val_mae: 2.4547\n",
      "Epoch 495/500\n",
      "2290/2290 [==============================] - 2s 846us/step - loss: 1.9407 - mae: 1.0591 - val_loss: 7.1975 - val_mae: 2.4979\n",
      "Epoch 496/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 1.8584 - mae: 1.0383 - val_loss: 7.0997 - val_mae: 2.4767\n",
      "Epoch 497/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 1.9247 - mae: 1.0672 - val_loss: 7.6598 - val_mae: 2.5843\n",
      "Epoch 498/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 6.1744 - mae: 1.1202 - val_loss: 7.0556 - val_mae: 2.4605\n",
      "Epoch 499/500\n",
      "2290/2290 [==============================] - 2s 828us/step - loss: 1.9032 - mae: 1.0386 - val_loss: 7.8902 - val_mae: 2.6217\n",
      "Epoch 500/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 1.9875 - mae: 1.0651 - val_loss: 6.7345 - val_mae: 2.3963\n",
      "Test Score: 2.60 RMSE\n"
     ]
    }
   ],
   "source": [
    "third = data[data['zip5'] == 85043]\n",
    "third = third.drop(['zip5', 'date_key'], axis =1)\n",
    "third_17_18 = third[third['year'] != 2019]\n",
    "y = third_17_18['impact_score']\n",
    "X = third_17_18.drop(['impact_score'], axis = 1)\n",
    "X = np.expand_dims(X, axis = 2)\n",
    "\n",
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=4, activation=\"relu\", filters=16),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=4, activation=\"relu\", filters=16),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),     # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "test_size = int(0.2 * 2862)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=25, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "#print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "#for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "#    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_24 (Conv1D)           (None, 164, 16)           80        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 79, 16)            1040      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 79, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 1265      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2,387\n",
      "Trainable params: 2,387\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2034 samples, validate on 572 samples\n",
      "Epoch 1/500\n",
      "2034/2034 [==============================] - 3s 2ms/step - loss: 258934.6182 - mae: 372.4887 - val_loss: 4255.2163 - val_mae: 58.6888\n",
      "Epoch 2/500\n",
      "2034/2034 [==============================] - 2s 827us/step - loss: 42351.1112 - mae: 147.5633 - val_loss: 762.8303 - val_mae: 22.6736\n",
      "Epoch 3/500\n",
      "2034/2034 [==============================] - 2s 799us/step - loss: 14286.2944 - mae: 93.6715 - val_loss: 186.2942 - val_mae: 11.7859\n",
      "Epoch 4/500\n",
      "2034/2034 [==============================] - 2s 857us/step - loss: 10233.2336 - mae: 71.1377 - val_loss: 426.1301 - val_mae: 19.5240\n",
      "Epoch 5/500\n",
      "2034/2034 [==============================] - 2s 833us/step - loss: 6567.4069 - mae: 54.6373 - val_loss: 232.0597 - val_mae: 14.3287\n",
      "Epoch 6/500\n",
      "2034/2034 [==============================] - 2s 873us/step - loss: 10690.1541 - mae: 45.2195 - val_loss: 296.8922 - val_mae: 16.6583\n",
      "Epoch 7/500\n",
      "2034/2034 [==============================] - 2s 833us/step - loss: 11614.6533 - mae: 34.5860 - val_loss: 250.4969 - val_mae: 15.2850\n",
      "Epoch 8/500\n",
      "2034/2034 [==============================] - 2s 896us/step - loss: 1279.4078 - mae: 27.5828 - val_loss: 181.9544 - val_mae: 12.8654\n",
      "Epoch 9/500\n",
      "2034/2034 [==============================] - 2s 760us/step - loss: 3045.6650 - mae: 25.2229 - val_loss: 164.2718 - val_mae: 12.1673\n",
      "Epoch 10/500\n",
      "2034/2034 [==============================] - 2s 894us/step - loss: 3134.6993 - mae: 23.5536 - val_loss: 172.8938 - val_mae: 12.5291\n",
      "Epoch 11/500\n",
      "2034/2034 [==============================] - 2s 808us/step - loss: 1260.9220 - mae: 20.9431 - val_loss: 142.5102 - val_mae: 11.2583\n",
      "Epoch 12/500\n",
      "2034/2034 [==============================] - 2s 818us/step - loss: 693.5132 - mae: 19.2070 - val_loss: 132.0759 - val_mae: 10.7913\n",
      "Epoch 13/500\n",
      "2034/2034 [==============================] - 2s 832us/step - loss: 565.7991 - mae: 17.7628 - val_loss: 126.4956 - val_mae: 10.5364\n",
      "Epoch 14/500\n",
      "2034/2034 [==============================] - 2s 865us/step - loss: 486.6196 - mae: 16.7080 - val_loss: 124.2023 - val_mae: 10.4317\n",
      "Epoch 15/500\n",
      "2034/2034 [==============================] - 2s 828us/step - loss: 1488.1652 - mae: 16.7716 - val_loss: 121.2043 - val_mae: 10.2964\n",
      "Epoch 16/500\n",
      "2034/2034 [==============================] - 2s 840us/step - loss: 563.1256 - mae: 15.6626 - val_loss: 126.3606 - val_mae: 10.5574\n",
      "Epoch 17/500\n",
      "2034/2034 [==============================] - 2s 870us/step - loss: 851.5000 - mae: 15.3177 - val_loss: 122.3995 - val_mae: 10.3735\n",
      "Epoch 18/500\n",
      "2034/2034 [==============================] - 2s 852us/step - loss: 444.9283 - mae: 14.3702 - val_loss: 117.0067 - val_mae: 10.1123\n",
      "Epoch 19/500\n",
      "2034/2034 [==============================] - 2s 822us/step - loss: 444.3169 - mae: 13.9635 - val_loss: 113.7313 - val_mae: 9.9521\n",
      "Epoch 20/500\n",
      "2034/2034 [==============================] - 2s 868us/step - loss: 686.9300 - mae: 13.4515 - val_loss: 116.7691 - val_mae: 10.1098\n",
      "Epoch 21/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 651.8738 - mae: 13.0372 - val_loss: 126.6117 - val_mae: 10.5919\n",
      "Epoch 22/500\n",
      "2034/2034 [==============================] - 2s 805us/step - loss: 274.9176 - mae: 12.5216 - val_loss: 118.3309 - val_mae: 10.1944\n",
      "Epoch 23/500\n",
      "2034/2034 [==============================] - 2s 746us/step - loss: 225.0075 - mae: 11.7932 - val_loss: 121.5050 - val_mae: 10.3504\n",
      "Epoch 24/500\n",
      "2034/2034 [==============================] - 2s 744us/step - loss: 261.6071 - mae: 12.0744 - val_loss: 117.5916 - val_mae: 10.1608\n",
      "Epoch 25/500\n",
      "2034/2034 [==============================] - 2s 794us/step - loss: 344.3036 - mae: 12.1512 - val_loss: 117.1436 - val_mae: 10.1417\n",
      "Epoch 26/500\n",
      "2034/2034 [==============================] - 2s 782us/step - loss: 212.3040 - mae: 11.5200 - val_loss: 116.2165 - val_mae: 10.0976\n",
      "Epoch 27/500\n",
      "2034/2034 [==============================] - 2s 796us/step - loss: 305.5096 - mae: 11.8974 - val_loss: 114.9908 - val_mae: 10.0398\n",
      "Epoch 28/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 253.8520 - mae: 11.5632 - val_loss: 110.6880 - val_mae: 9.8248\n",
      "Epoch 29/500\n",
      "2034/2034 [==============================] - 2s 746us/step - loss: 198.7064 - mae: 11.2463 - val_loss: 110.7133 - val_mae: 9.8276\n",
      "Epoch 30/500\n",
      "2034/2034 [==============================] - 2s 774us/step - loss: 238.5874 - mae: 11.0504 - val_loss: 110.9660 - val_mae: 9.8423\n",
      "Epoch 31/500\n",
      "2034/2034 [==============================] - 2s 767us/step - loss: 208.3522 - mae: 10.9531 - val_loss: 108.2115 - val_mae: 9.7028\n",
      "Epoch 32/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 249.9553 - mae: 10.7859 - val_loss: 106.9874 - val_mae: 9.6416\n",
      "Epoch 33/500\n",
      "2034/2034 [==============================] - 2s 762us/step - loss: 252.6091 - mae: 10.9625 - val_loss: 110.3450 - val_mae: 9.8166\n",
      "Epoch 34/500\n",
      "2034/2034 [==============================] - 2s 777us/step - loss: 186.7184 - mae: 11.0079 - val_loss: 105.6173 - val_mae: 9.5729\n",
      "Epoch 35/500\n",
      "2034/2034 [==============================] - 1s 719us/step - loss: 162.3199 - mae: 10.4246 - val_loss: 102.1992 - val_mae: 9.3896\n",
      "Epoch 36/500\n",
      "2034/2034 [==============================] - 1s 708us/step - loss: 162.7026 - mae: 10.4920 - val_loss: 96.8287 - val_mae: 9.0931\n",
      "Epoch 37/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 190.5573 - mae: 10.4411 - val_loss: 97.9925 - val_mae: 9.1601\n",
      "Epoch 38/500\n",
      "2034/2034 [==============================] - 2s 868us/step - loss: 159.1256 - mae: 10.4108 - val_loss: 95.2874 - val_mae: 9.0123\n",
      "Epoch 39/500\n",
      "2034/2034 [==============================] - 2s 836us/step - loss: 167.2671 - mae: 10.1742 - val_loss: 92.7767 - val_mae: 8.8672\n",
      "Epoch 40/500\n",
      "2034/2034 [==============================] - 2s 782us/step - loss: 199.7004 - mae: 10.3155 - val_loss: 94.7247 - val_mae: 8.9755\n",
      "Epoch 41/500\n",
      "2034/2034 [==============================] - 2s 790us/step - loss: 151.2678 - mae: 10.1579 - val_loss: 91.9228 - val_mae: 8.8145\n",
      "Epoch 42/500\n",
      "2034/2034 [==============================] - 2s 754us/step - loss: 141.9399 - mae: 10.0240 - val_loss: 88.6089 - val_mae: 8.6263\n",
      "Epoch 43/500\n",
      "2034/2034 [==============================] - 2s 842us/step - loss: 168.4758 - mae: 10.2276 - val_loss: 90.8234 - val_mae: 8.7541\n",
      "Epoch 44/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 160.2465 - mae: 10.1074 - val_loss: 86.3192 - val_mae: 8.4950\n",
      "Epoch 45/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 136.7386 - mae: 10.1044 - val_loss: 85.9119 - val_mae: 8.4736\n",
      "Epoch 46/500\n",
      "2034/2034 [==============================] - 2s 845us/step - loss: 174.4567 - mae: 10.0631 - val_loss: 87.8874 - val_mae: 8.5855\n",
      "Epoch 47/500\n",
      "2034/2034 [==============================] - 2s 825us/step - loss: 133.4911 - mae: 9.9381 - val_loss: 81.6175 - val_mae: 8.2259\n",
      "Epoch 48/500\n",
      "2034/2034 [==============================] - 2s 883us/step - loss: 171.4453 - mae: 10.0344 - val_loss: 85.1768 - val_mae: 8.4349\n",
      "Epoch 49/500\n",
      "2034/2034 [==============================] - 2s 837us/step - loss: 152.9961 - mae: 9.8991 - val_loss: 82.3691 - val_mae: 8.2751\n",
      "Epoch 50/500\n",
      "2034/2034 [==============================] - 2s 947us/step - loss: 129.4600 - mae: 9.8445 - val_loss: 78.5338 - val_mae: 8.0497\n",
      "Epoch 51/500\n",
      "2034/2034 [==============================] - 2s 812us/step - loss: 131.6051 - mae: 9.6961 - val_loss: 77.3407 - val_mae: 7.9789\n",
      "Epoch 52/500\n",
      "2034/2034 [==============================] - 2s 807us/step - loss: 129.1585 - mae: 9.7852 - val_loss: 78.1468 - val_mae: 8.0285\n",
      "Epoch 53/500\n",
      "2034/2034 [==============================] - 2s 859us/step - loss: 123.4655 - mae: 9.6302 - val_loss: 79.3325 - val_mae: 8.1002\n",
      "Epoch 54/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 125.2700 - mae: 9.6563 - val_loss: 77.0713 - val_mae: 7.9650\n",
      "Epoch 55/500\n",
      "2034/2034 [==============================] - 2s 826us/step - loss: 142.3881 - mae: 9.8654 - val_loss: 79.8563 - val_mae: 8.1342\n",
      "Epoch 56/500\n",
      "2034/2034 [==============================] - 2s 796us/step - loss: 127.4372 - mae: 9.8037 - val_loss: 78.0757 - val_mae: 8.0285\n",
      "Epoch 57/500\n",
      "2034/2034 [==============================] - 2s 814us/step - loss: 131.4286 - mae: 9.8154 - val_loss: 80.4680 - val_mae: 8.1724\n",
      "Epoch 58/500\n",
      "2034/2034 [==============================] - 2s 863us/step - loss: 122.1109 - mae: 9.6075 - val_loss: 77.3054 - val_mae: 7.9854\n",
      "Epoch 59/500\n",
      "2034/2034 [==============================] - 2s 848us/step - loss: 120.4680 - mae: 9.6266 - val_loss: 75.0768 - val_mae: 7.8505\n",
      "Epoch 60/500\n",
      "2034/2034 [==============================] - 2s 841us/step - loss: 127.1252 - mae: 9.6940 - val_loss: 77.1768 - val_mae: 7.9797\n",
      "Epoch 61/500\n",
      "2034/2034 [==============================] - 2s 857us/step - loss: 125.4091 - mae: 9.7354 - val_loss: 73.6716 - val_mae: 7.7659\n",
      "Epoch 62/500\n",
      "2034/2034 [==============================] - 2s 824us/step - loss: 124.4903 - mae: 9.5718 - val_loss: 73.8590 - val_mae: 7.7776\n",
      "Epoch 63/500\n",
      "2034/2034 [==============================] - 2s 812us/step - loss: 127.6106 - mae: 9.7332 - val_loss: 71.9834 - val_mae: 7.6611\n",
      "Epoch 64/500\n",
      "2034/2034 [==============================] - 2s 877us/step - loss: 118.9471 - mae: 9.5676 - val_loss: 75.4103 - val_mae: 7.8746\n",
      "Epoch 65/500\n",
      "2034/2034 [==============================] - 2s 774us/step - loss: 128.7252 - mae: 9.7719 - val_loss: 75.5242 - val_mae: 7.8826\n",
      "Epoch 66/500\n",
      "2034/2034 [==============================] - 2s 894us/step - loss: 115.4675 - mae: 9.4845 - val_loss: 72.6991 - val_mae: 7.7078\n",
      "Epoch 67/500\n",
      "2034/2034 [==============================] - 2s 774us/step - loss: 121.5079 - mae: 9.6515 - val_loss: 73.0432 - val_mae: 7.7296\n",
      "Epoch 68/500\n",
      "2034/2034 [==============================] - 2s 799us/step - loss: 119.5738 - mae: 9.6794 - val_loss: 69.6527 - val_mae: 7.5145\n",
      "Epoch 69/500\n",
      "2034/2034 [==============================] - 2s 878us/step - loss: 117.9442 - mae: 9.5458 - val_loss: 74.1172 - val_mae: 7.7969\n",
      "Epoch 70/500\n",
      "2034/2034 [==============================] - 2s 799us/step - loss: 118.5760 - mae: 9.5479 - val_loss: 71.9138 - val_mae: 7.6595\n",
      "Epoch 71/500\n",
      "2034/2034 [==============================] - 2s 814us/step - loss: 117.6091 - mae: 9.5632 - val_loss: 72.9712 - val_mae: 7.7262\n",
      "Epoch 72/500\n",
      "2034/2034 [==============================] - 2s 867us/step - loss: 127.5588 - mae: 9.6443 - val_loss: 78.8263 - val_mae: 8.0850\n",
      "Epoch 73/500\n",
      "2034/2034 [==============================] - 2s 892us/step - loss: 123.5077 - mae: 9.5693 - val_loss: 86.1672 - val_mae: 8.5121\n",
      "Epoch 74/500\n",
      "2034/2034 [==============================] - 2s 822us/step - loss: 118.4185 - mae: 9.6022 - val_loss: 74.0274 - val_mae: 7.7933\n",
      "Epoch 75/500\n",
      "2034/2034 [==============================] - 2s 831us/step - loss: 114.0168 - mae: 9.4417 - val_loss: 70.7914 - val_mae: 7.5898\n",
      "Epoch 76/500\n",
      "2034/2034 [==============================] - 2s 797us/step - loss: 121.3775 - mae: 9.6755 - val_loss: 75.8829 - val_mae: 7.9083\n",
      "Epoch 77/500\n",
      "2034/2034 [==============================] - 2s 813us/step - loss: 122.0212 - mae: 9.6143 - val_loss: 69.6810 - val_mae: 7.5205\n",
      "Epoch 78/500\n",
      "2034/2034 [==============================] - 2s 816us/step - loss: 112.2909 - mae: 9.3544 - val_loss: 71.3265 - val_mae: 7.6258\n",
      "Epoch 79/500\n",
      "2034/2034 [==============================] - 2s 800us/step - loss: 113.9601 - mae: 9.4803 - val_loss: 67.1732 - val_mae: 7.3570\n",
      "Epoch 80/500\n",
      "2034/2034 [==============================] - 2s 841us/step - loss: 114.2211 - mae: 9.4149 - val_loss: 71.4575 - val_mae: 7.6329\n",
      "Epoch 81/500\n",
      "2034/2034 [==============================] - 2s 802us/step - loss: 113.6696 - mae: 9.4758 - val_loss: 67.9000 - val_mae: 7.4036\n",
      "Epoch 82/500\n",
      "2034/2034 [==============================] - 2s 884us/step - loss: 124.9019 - mae: 9.6025 - val_loss: 96.5454 - val_mae: 9.0997\n",
      "Epoch 83/500\n",
      "2034/2034 [==============================] - 2s 891us/step - loss: 118.8652 - mae: 9.6798 - val_loss: 65.2711 - val_mae: 7.2279\n",
      "Epoch 84/500\n",
      "2034/2034 [==============================] - 2s 834us/step - loss: 116.9258 - mae: 9.4778 - val_loss: 73.5995 - val_mae: 7.7676\n",
      "Epoch 85/500\n",
      "2034/2034 [==============================] - 2s 892us/step - loss: 114.0903 - mae: 9.4643 - val_loss: 72.0011 - val_mae: 7.6670\n",
      "Epoch 86/500\n",
      "2034/2034 [==============================] - 2s 893us/step - loss: 114.2220 - mae: 9.4172 - val_loss: 69.7792 - val_mae: 7.5220\n",
      "Epoch 87/500\n",
      "2034/2034 [==============================] - 2s 844us/step - loss: 115.0603 - mae: 9.4701 - val_loss: 64.4175 - val_mae: 7.1655\n",
      "Epoch 88/500\n",
      "2034/2034 [==============================] - 2s 808us/step - loss: 114.1054 - mae: 9.4403 - val_loss: 63.5218 - val_mae: 7.0994\n",
      "Epoch 89/500\n",
      "2034/2034 [==============================] - 2s 858us/step - loss: 117.1297 - mae: 9.4454 - val_loss: 75.8616 - val_mae: 7.8934\n",
      "Epoch 90/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 114.2793 - mae: 9.4870 - val_loss: 73.7783 - val_mae: 7.7576\n",
      "Epoch 91/500\n",
      "2034/2034 [==============================] - 2s 770us/step - loss: 113.6898 - mae: 9.3417 - val_loss: 76.4950 - val_mae: 7.9180\n",
      "Epoch 92/500\n",
      "2034/2034 [==============================] - 2s 889us/step - loss: 112.4432 - mae: 9.3757 - val_loss: 71.1578 - val_mae: 7.5731\n",
      "Epoch 93/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 113.0758 - mae: 9.3973 - val_loss: 62.9566 - val_mae: 7.0338\n",
      "Epoch 94/500\n",
      "2034/2034 [==============================] - 2s 892us/step - loss: 112.0695 - mae: 9.3812 - val_loss: 63.3075 - val_mae: 7.0603\n",
      "Epoch 95/500\n",
      "2034/2034 [==============================] - 2s 831us/step - loss: 113.0091 - mae: 9.3841 - val_loss: 63.6016 - val_mae: 7.0799\n",
      "Epoch 96/500\n",
      "2034/2034 [==============================] - 2s 827us/step - loss: 113.2841 - mae: 9.3885 - val_loss: 68.4010 - val_mae: 7.4015\n",
      "Epoch 97/500\n",
      "2034/2034 [==============================] - 2s 828us/step - loss: 114.1127 - mae: 9.4122 - val_loss: 54.6323 - val_mae: 6.4321\n",
      "Epoch 98/500\n",
      "2034/2034 [==============================] - 2s 784us/step - loss: 111.4289 - mae: 9.2986 - val_loss: 73.2059 - val_mae: 7.6943\n",
      "Epoch 99/500\n",
      "2034/2034 [==============================] - 2s 800us/step - loss: 111.7207 - mae: 9.3364 - val_loss: 65.6979 - val_mae: 7.2006\n",
      "Epoch 100/500\n",
      "2034/2034 [==============================] - 2s 881us/step - loss: 112.5332 - mae: 9.3376 - val_loss: 77.9423 - val_mae: 7.9832\n",
      "Epoch 101/500\n",
      "2034/2034 [==============================] - 2s 864us/step - loss: 110.8058 - mae: 9.2828 - val_loss: 73.4833 - val_mae: 7.7055\n",
      "Epoch 102/500\n",
      "2034/2034 [==============================] - 2s 859us/step - loss: 109.9843 - mae: 9.2803 - val_loss: 62.8929 - val_mae: 7.0144\n",
      "Epoch 103/500\n",
      "2034/2034 [==============================] - 2s 861us/step - loss: 111.0974 - mae: 9.3129 - val_loss: 67.6545 - val_mae: 7.3277\n",
      "Epoch 104/500\n",
      "2034/2034 [==============================] - 2s 770us/step - loss: 113.4385 - mae: 9.3427 - val_loss: 70.3129 - val_mae: 7.4906\n",
      "Epoch 105/500\n",
      "2034/2034 [==============================] - 2s 871us/step - loss: 111.1083 - mae: 9.3049 - val_loss: 80.0013 - val_mae: 8.1106\n",
      "Epoch 106/500\n",
      "2034/2034 [==============================] - 2s 817us/step - loss: 110.6674 - mae: 9.2840 - val_loss: 73.2643 - val_mae: 7.6879\n",
      "Epoch 107/500\n",
      "2034/2034 [==============================] - 2s 894us/step - loss: 110.0053 - mae: 9.2591 - val_loss: 57.3152 - val_mae: 6.5985\n",
      "Epoch 108/500\n",
      "2034/2034 [==============================] - 2s 872us/step - loss: 109.5024 - mae: 9.1790 - val_loss: 69.4716 - val_mae: 7.4309\n",
      "Epoch 109/500\n",
      "2034/2034 [==============================] - 2s 793us/step - loss: 109.2605 - mae: 9.2421 - val_loss: 55.5835 - val_mae: 6.4266\n",
      "Epoch 110/500\n",
      "2034/2034 [==============================] - 2s 830us/step - loss: 108.6674 - mae: 9.1407 - val_loss: 55.1460 - val_mae: 6.3897\n",
      "Epoch 111/500\n",
      "2034/2034 [==============================] - 2s 892us/step - loss: 109.2512 - mae: 9.2184 - val_loss: 70.1198 - val_mae: 7.4375\n",
      "Epoch 112/500\n",
      "2034/2034 [==============================] - 2s 901us/step - loss: 108.2513 - mae: 9.1957 - val_loss: 62.7446 - val_mae: 6.9035\n",
      "Epoch 113/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 110.4930 - mae: 9.2811 - val_loss: 62.9673 - val_mae: 6.9426\n",
      "Epoch 114/500\n",
      "2034/2034 [==============================] - 2s 883us/step - loss: 110.5384 - mae: 9.2141 - val_loss: 62.5783 - val_mae: 6.9173\n",
      "Epoch 115/500\n",
      "2034/2034 [==============================] - 2s 819us/step - loss: 109.3979 - mae: 9.2056 - val_loss: 59.7624 - val_mae: 6.7140\n",
      "Epoch 116/500\n",
      "2034/2034 [==============================] - 2s 850us/step - loss: 108.0110 - mae: 9.0841 - val_loss: 76.3216 - val_mae: 7.8395\n",
      "Epoch 117/500\n",
      "2034/2034 [==============================] - 2s 788us/step - loss: 108.8520 - mae: 9.1899 - val_loss: 65.0162 - val_mae: 7.0605\n",
      "Epoch 118/500\n",
      "2034/2034 [==============================] - 2s 910us/step - loss: 108.8869 - mae: 9.1735 - val_loss: 59.6867 - val_mae: 6.6579\n",
      "Epoch 119/500\n",
      "2034/2034 [==============================] - 2s 832us/step - loss: 110.3533 - mae: 9.1828 - val_loss: 66.8676 - val_mae: 7.1809\n",
      "Epoch 120/500\n",
      "2034/2034 [==============================] - 2s 857us/step - loss: 110.4628 - mae: 9.2506 - val_loss: 47.2743 - val_mae: 5.7773\n",
      "Epoch 121/500\n",
      "2034/2034 [==============================] - 2s 814us/step - loss: 108.9016 - mae: 9.1156 - val_loss: 59.4188 - val_mae: 6.6792\n",
      "Epoch 122/500\n",
      "2034/2034 [==============================] - 2s 867us/step - loss: 107.6531 - mae: 9.1183 - val_loss: 56.3550 - val_mae: 6.4352\n",
      "Epoch 123/500\n",
      "2034/2034 [==============================] - 2s 830us/step - loss: 107.1758 - mae: 9.1061 - val_loss: 60.6255 - val_mae: 6.7536\n",
      "Epoch 124/500\n",
      "2034/2034 [==============================] - 2s 849us/step - loss: 109.1053 - mae: 9.1388 - val_loss: 85.6610 - val_mae: 8.3936\n",
      "Epoch 125/500\n",
      "2034/2034 [==============================] - 2s 842us/step - loss: 108.6802 - mae: 9.1015 - val_loss: 61.1572 - val_mae: 6.7593\n",
      "Epoch 126/500\n",
      "2034/2034 [==============================] - 2s 809us/step - loss: 107.3639 - mae: 9.0884 - val_loss: 59.8717 - val_mae: 6.6687\n",
      "Epoch 127/500\n",
      "2034/2034 [==============================] - 2s 875us/step - loss: 110.9501 - mae: 9.2655 - val_loss: 60.9612 - val_mae: 6.7813\n",
      "Epoch 128/500\n",
      "2034/2034 [==============================] - 2s 823us/step - loss: 108.7853 - mae: 9.1258 - val_loss: 57.9303 - val_mae: 6.5155\n",
      "Epoch 129/500\n",
      "2034/2034 [==============================] - 2s 823us/step - loss: 109.0081 - mae: 9.1211 - val_loss: 44.0777 - val_mae: 5.5554\n",
      "Epoch 130/500\n",
      "2034/2034 [==============================] - 1s 729us/step - loss: 106.9892 - mae: 9.0135 - val_loss: 88.7613 - val_mae: 8.5269\n",
      "Epoch 131/500\n",
      "2034/2034 [==============================] - 2s 832us/step - loss: 105.8354 - mae: 8.9478 - val_loss: 78.2208 - val_mae: 7.8687\n",
      "Epoch 132/500\n",
      "2034/2034 [==============================] - 2s 811us/step - loss: 105.7310 - mae: 9.0052 - val_loss: 72.2435 - val_mae: 7.4706\n",
      "Epoch 133/500\n",
      "2034/2034 [==============================] - 2s 793us/step - loss: 107.8183 - mae: 9.0980 - val_loss: 58.6373 - val_mae: 6.6025\n",
      "Epoch 134/500\n",
      "2034/2034 [==============================] - 2s 848us/step - loss: 108.6257 - mae: 9.1700 - val_loss: 62.9330 - val_mae: 6.9185\n",
      "Epoch 135/500\n",
      "2034/2034 [==============================] - 2s 752us/step - loss: 108.3907 - mae: 9.0751 - val_loss: 64.3266 - val_mae: 6.9588\n",
      "Epoch 136/500\n",
      "2034/2034 [==============================] - 2s 742us/step - loss: 106.4435 - mae: 9.0116 - val_loss: 75.6190 - val_mae: 7.7218\n",
      "Epoch 137/500\n",
      "2034/2034 [==============================] - 2s 866us/step - loss: 107.7066 - mae: 9.0869 - val_loss: 82.9813 - val_mae: 8.1764\n",
      "Epoch 138/500\n",
      "2034/2034 [==============================] - 2s 907us/step - loss: 107.2770 - mae: 9.0077 - val_loss: 80.7132 - val_mae: 8.0244\n",
      "Epoch 139/500\n",
      "2034/2034 [==============================] - 2s 808us/step - loss: 109.1954 - mae: 9.1100 - val_loss: 73.8399 - val_mae: 7.5514\n",
      "Epoch 140/500\n",
      "2034/2034 [==============================] - 2s 953us/step - loss: 110.6351 - mae: 9.2132 - val_loss: 59.3056 - val_mae: 6.5818\n",
      "Epoch 141/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 108.5114 - mae: 9.1708 - val_loss: 58.8539 - val_mae: 6.5545\n",
      "Epoch 142/500\n",
      "2034/2034 [==============================] - 2s 860us/step - loss: 105.9537 - mae: 9.0161 - val_loss: 64.4135 - val_mae: 6.9486\n",
      "Epoch 143/500\n",
      "2034/2034 [==============================] - 2s 833us/step - loss: 107.6679 - mae: 9.0518 - val_loss: 67.4806 - val_mae: 7.1902\n",
      "Epoch 144/500\n",
      "2034/2034 [==============================] - 2s 823us/step - loss: 107.1420 - mae: 9.0857 - val_loss: 65.8860 - val_mae: 7.0642\n",
      "Epoch 145/500\n",
      "2034/2034 [==============================] - 2s 770us/step - loss: 105.7130 - mae: 8.9872 - val_loss: 78.1674 - val_mae: 7.8231\n",
      "Epoch 146/500\n",
      "2034/2034 [==============================] - 2s 890us/step - loss: 110.0309 - mae: 9.1607 - val_loss: 66.3871 - val_mae: 7.1244\n",
      "Epoch 147/500\n",
      "2034/2034 [==============================] - 2s 843us/step - loss: 105.5385 - mae: 8.9409 - val_loss: 78.9633 - val_mae: 7.8992\n",
      "Epoch 148/500\n",
      "2034/2034 [==============================] - 2s 850us/step - loss: 104.7609 - mae: 8.9603 - val_loss: 75.7629 - val_mae: 7.6652\n",
      "Epoch 149/500\n",
      "2034/2034 [==============================] - 2s 852us/step - loss: 108.0929 - mae: 9.0632 - val_loss: 74.4971 - val_mae: 7.5799\n",
      "Epoch 150/500\n",
      "2034/2034 [==============================] - 2s 854us/step - loss: 106.9676 - mae: 9.0386 - val_loss: 81.5513 - val_mae: 8.0777\n",
      "Epoch 151/500\n",
      "2034/2034 [==============================] - 2s 867us/step - loss: 107.0753 - mae: 9.0487 - val_loss: 98.0641 - val_mae: 9.0469\n",
      "Epoch 152/500\n",
      "2034/2034 [==============================] - 2s 821us/step - loss: 106.2810 - mae: 9.0321 - val_loss: 75.2672 - val_mae: 7.6483\n",
      "Epoch 153/500\n",
      "2034/2034 [==============================] - 2s 853us/step - loss: 106.5936 - mae: 9.0412 - val_loss: 69.3431 - val_mae: 7.2985\n",
      "Epoch 154/500\n",
      "2034/2034 [==============================] - 2s 865us/step - loss: 105.6214 - mae: 8.9654 - val_loss: 61.6818 - val_mae: 6.7553\n",
      "Epoch 155/500\n",
      "2034/2034 [==============================] - 2s 792us/step - loss: 105.7140 - mae: 8.9577 - val_loss: 86.8944 - val_mae: 8.3568\n",
      "Epoch 156/500\n",
      "2034/2034 [==============================] - 2s 829us/step - loss: 106.8974 - mae: 9.0804 - val_loss: 60.6696 - val_mae: 6.6678\n",
      "Epoch 157/500\n",
      "2034/2034 [==============================] - 2s 878us/step - loss: 108.8491 - mae: 9.1075 - val_loss: 76.1837 - val_mae: 7.7024\n",
      "Epoch 158/500\n",
      "2034/2034 [==============================] - 2s 785us/step - loss: 105.8139 - mae: 8.9506 - val_loss: 91.3191 - val_mae: 8.6246\n",
      "Epoch 159/500\n",
      "2034/2034 [==============================] - 2s 796us/step - loss: 105.5654 - mae: 8.9860 - val_loss: 66.0880 - val_mae: 7.0560\n",
      "Epoch 160/500\n",
      "2034/2034 [==============================] - 2s 769us/step - loss: 106.4576 - mae: 9.0350 - val_loss: 76.8330 - val_mae: 7.7876\n",
      "Epoch 161/500\n",
      "2034/2034 [==============================] - 1s 731us/step - loss: 106.3230 - mae: 9.0221 - val_loss: 58.4489 - val_mae: 6.5340\n",
      "Epoch 162/500\n",
      "2034/2034 [==============================] - 2s 757us/step - loss: 106.1306 - mae: 9.0211 - val_loss: 73.1417 - val_mae: 7.5579\n",
      "Epoch 163/500\n",
      "2034/2034 [==============================] - 2s 851us/step - loss: 106.7323 - mae: 9.0502 - val_loss: 50.4727 - val_mae: 6.0232\n",
      "Epoch 164/500\n",
      "2034/2034 [==============================] - 2s 833us/step - loss: 105.5462 - mae: 8.9918 - val_loss: 58.0311 - val_mae: 6.5180\n",
      "Epoch 165/500\n",
      "2034/2034 [==============================] - 2s 886us/step - loss: 105.1391 - mae: 8.9040 - val_loss: 107.6267 - val_mae: 9.5487\n",
      "Epoch 166/500\n",
      "2034/2034 [==============================] - 2s 814us/step - loss: 106.7989 - mae: 9.0421 - val_loss: 60.7669 - val_mae: 6.7106\n",
      "Epoch 167/500\n",
      "2034/2034 [==============================] - 2s 912us/step - loss: 104.2898 - mae: 8.8887 - val_loss: 87.6104 - val_mae: 8.4498\n",
      "Epoch 168/500\n",
      "2034/2034 [==============================] - 2s 816us/step - loss: 106.4499 - mae: 8.9813 - val_loss: 87.6333 - val_mae: 8.4464\n",
      "Epoch 169/500\n",
      "2034/2034 [==============================] - 2s 861us/step - loss: 105.1342 - mae: 8.9914 - val_loss: 75.3962 - val_mae: 7.6849\n",
      "Epoch 170/500\n",
      "2034/2034 [==============================] - 2s 798us/step - loss: 106.1113 - mae: 9.0192 - val_loss: 68.3091 - val_mae: 7.2118\n",
      "Epoch 171/500\n",
      "2034/2034 [==============================] - 2s 858us/step - loss: 107.0080 - mae: 9.0059 - val_loss: 68.3409 - val_mae: 7.2364\n",
      "Epoch 172/500\n",
      "2034/2034 [==============================] - 2s 830us/step - loss: 105.1001 - mae: 8.9340 - val_loss: 76.9098 - val_mae: 7.7637\n",
      "Epoch 173/500\n",
      "2034/2034 [==============================] - 2s 875us/step - loss: 105.6219 - mae: 8.9899 - val_loss: 75.4573 - val_mae: 7.6901\n",
      "Epoch 174/500\n",
      "2034/2034 [==============================] - 2s 819us/step - loss: 105.9811 - mae: 8.9766 - val_loss: 75.3331 - val_mae: 7.6989\n",
      "Epoch 175/500\n",
      "2034/2034 [==============================] - 2s 792us/step - loss: 104.9322 - mae: 8.9744 - val_loss: 48.2570 - val_mae: 5.8693\n",
      "Epoch 176/500\n",
      "2034/2034 [==============================] - 2s 860us/step - loss: 105.7232 - mae: 8.9548 - val_loss: 86.8911 - val_mae: 8.4038\n",
      "Epoch 177/500\n",
      "2034/2034 [==============================] - 2s 817us/step - loss: 104.7998 - mae: 8.9675 - val_loss: 56.2953 - val_mae: 6.3988\n",
      "Epoch 178/500\n",
      "2034/2034 [==============================] - 2s 862us/step - loss: 106.0429 - mae: 8.9145 - val_loss: 52.5968 - val_mae: 6.1309\n",
      "Epoch 179/500\n",
      "2034/2034 [==============================] - 2s 848us/step - loss: 105.8060 - mae: 8.9633 - val_loss: 75.2439 - val_mae: 7.6669\n",
      "Epoch 180/500\n",
      "2034/2034 [==============================] - 2s 824us/step - loss: 105.9133 - mae: 8.9458 - val_loss: 75.4686 - val_mae: 7.6595\n",
      "Epoch 181/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 104.1777 - mae: 8.9390 - val_loss: 60.0856 - val_mae: 6.6482\n",
      "Epoch 182/500\n",
      "2034/2034 [==============================] - 2s 791us/step - loss: 105.7280 - mae: 8.9569 - val_loss: 63.6794 - val_mae: 6.8717\n",
      "Epoch 183/500\n",
      "2034/2034 [==============================] - 2s 836us/step - loss: 104.4925 - mae: 8.8640 - val_loss: 63.9388 - val_mae: 6.9274\n",
      "Epoch 184/500\n",
      "2034/2034 [==============================] - 2s 847us/step - loss: 106.2675 - mae: 9.0487 - val_loss: 64.9288 - val_mae: 7.0104\n",
      "Epoch 185/500\n",
      "2034/2034 [==============================] - 2s 768us/step - loss: 104.5895 - mae: 8.8930 - val_loss: 54.1836 - val_mae: 6.2418\n",
      "Epoch 186/500\n",
      "2034/2034 [==============================] - 2s 796us/step - loss: 107.9629 - mae: 9.0604 - val_loss: 75.2763 - val_mae: 7.6943\n",
      "Epoch 187/500\n",
      "2034/2034 [==============================] - 2s 810us/step - loss: 105.3812 - mae: 8.9957 - val_loss: 62.0220 - val_mae: 6.7945\n",
      "Epoch 188/500\n",
      "2034/2034 [==============================] - 2s 800us/step - loss: 104.8627 - mae: 8.9509 - val_loss: 51.6300 - val_mae: 6.1005\n",
      "Epoch 189/500\n",
      "2034/2034 [==============================] - 2s 841us/step - loss: 104.4887 - mae: 8.8893 - val_loss: 59.7418 - val_mae: 6.6571\n",
      "Epoch 190/500\n",
      "2034/2034 [==============================] - 2s 834us/step - loss: 106.2256 - mae: 9.0143 - val_loss: 77.6371 - val_mae: 7.8416\n",
      "Epoch 191/500\n",
      "2034/2034 [==============================] - 2s 845us/step - loss: 105.0445 - mae: 8.9307 - val_loss: 55.6958 - val_mae: 6.3430\n",
      "Epoch 192/500\n",
      "2034/2034 [==============================] - 2s 793us/step - loss: 106.0700 - mae: 8.9240 - val_loss: 86.4845 - val_mae: 8.3616\n",
      "Epoch 193/500\n",
      "2034/2034 [==============================] - 2s 827us/step - loss: 103.9017 - mae: 8.9102 - val_loss: 54.4783 - val_mae: 6.2658\n",
      "Epoch 194/500\n",
      "2034/2034 [==============================] - 2s 785us/step - loss: 105.4700 - mae: 8.9391 - val_loss: 66.4139 - val_mae: 7.0907\n",
      "Epoch 195/500\n",
      "2034/2034 [==============================] - 2s 826us/step - loss: 106.3908 - mae: 8.9384 - val_loss: 81.7243 - val_mae: 8.0822\n",
      "Epoch 196/500\n",
      "2034/2034 [==============================] - 2s 876us/step - loss: 105.5275 - mae: 8.9623 - val_loss: 73.6284 - val_mae: 7.5831\n",
      "Epoch 197/500\n",
      "2034/2034 [==============================] - 2s 830us/step - loss: 104.1779 - mae: 8.8899 - val_loss: 76.7293 - val_mae: 7.7829\n",
      "Epoch 198/500\n",
      "2034/2034 [==============================] - 2s 868us/step - loss: 104.1261 - mae: 8.9199 - val_loss: 62.9893 - val_mae: 6.8153\n",
      "Epoch 199/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 104.4899 - mae: 8.8909 - val_loss: 69.1754 - val_mae: 7.2258\n",
      "Epoch 200/500\n",
      "2034/2034 [==============================] - 2s 819us/step - loss: 105.4373 - mae: 8.8823 - val_loss: 86.8143 - val_mae: 8.3813\n",
      "Epoch 201/500\n",
      "2034/2034 [==============================] - 2s 922us/step - loss: 105.5188 - mae: 8.9439 - val_loss: 60.5585 - val_mae: 6.6969\n",
      "Epoch 202/500\n",
      "2034/2034 [==============================] - 2s 792us/step - loss: 104.6115 - mae: 8.9463 - val_loss: 73.9241 - val_mae: 7.6051\n",
      "Epoch 203/500\n",
      "2034/2034 [==============================] - 2s 831us/step - loss: 105.7389 - mae: 8.9151 - val_loss: 76.9220 - val_mae: 7.8187\n",
      "Epoch 204/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 105.4682 - mae: 9.0220 - val_loss: 54.7479 - val_mae: 6.3046\n",
      "Epoch 205/500\n",
      "2034/2034 [==============================] - 2s 899us/step - loss: 103.1936 - mae: 8.8644 - val_loss: 91.5458 - val_mae: 8.6514\n",
      "Epoch 206/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 104.4355 - mae: 8.9399 - val_loss: 71.2088 - val_mae: 7.3999\n",
      "Epoch 207/500\n",
      "2034/2034 [==============================] - 2s 788us/step - loss: 104.0939 - mae: 8.8825 - val_loss: 59.9776 - val_mae: 6.6385\n",
      "Epoch 208/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 103.9927 - mae: 8.8488 - val_loss: 64.8811 - val_mae: 6.9773\n",
      "Epoch 209/500\n",
      "2034/2034 [==============================] - 2s 807us/step - loss: 103.5576 - mae: 8.8325 - val_loss: 67.1376 - val_mae: 7.1339\n",
      "Epoch 210/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 103.9724 - mae: 8.8859 - val_loss: 66.0749 - val_mae: 7.0405\n",
      "Epoch 211/500\n",
      "2034/2034 [==============================] - 2s 918us/step - loss: 105.2234 - mae: 8.9814 - val_loss: 52.8670 - val_mae: 6.1665\n",
      "Epoch 212/500\n",
      "2034/2034 [==============================] - 2s 833us/step - loss: 103.0766 - mae: 8.8253 - val_loss: 66.4842 - val_mae: 7.0990\n",
      "Epoch 213/500\n",
      "2034/2034 [==============================] - 2s 834us/step - loss: 103.1752 - mae: 8.8414 - val_loss: 58.8375 - val_mae: 6.5590\n",
      "Epoch 214/500\n",
      "2034/2034 [==============================] - 2s 850us/step - loss: 103.3931 - mae: 8.8306 - val_loss: 61.0158 - val_mae: 6.7068\n",
      "Epoch 215/500\n",
      "2034/2034 [==============================] - 2s 752us/step - loss: 108.8859 - mae: 9.0949 - val_loss: 70.5847 - val_mae: 7.3025\n",
      "Epoch 216/500\n",
      "2034/2034 [==============================] - 2s 819us/step - loss: 105.5556 - mae: 8.9041 - val_loss: 58.6696 - val_mae: 6.5618\n",
      "Epoch 217/500\n",
      "2034/2034 [==============================] - 2s 835us/step - loss: 103.0187 - mae: 8.8575 - val_loss: 58.5976 - val_mae: 6.5479\n",
      "Epoch 218/500\n",
      "2034/2034 [==============================] - 2s 753us/step - loss: 104.2143 - mae: 8.8963 - val_loss: 57.8812 - val_mae: 6.5044\n",
      "Epoch 219/500\n",
      "2034/2034 [==============================] - 2s 768us/step - loss: 104.5125 - mae: 8.8799 - val_loss: 79.2935 - val_mae: 7.9210\n",
      "Epoch 220/500\n",
      "2034/2034 [==============================] - 2s 808us/step - loss: 105.1751 - mae: 8.9709 - val_loss: 68.7104 - val_mae: 7.2422\n",
      "Epoch 221/500\n",
      "2034/2034 [==============================] - 2s 759us/step - loss: 104.7950 - mae: 8.9083 - val_loss: 67.2250 - val_mae: 7.1312\n",
      "Epoch 222/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 102.5990 - mae: 8.7918 - val_loss: 69.2448 - val_mae: 7.2486\n",
      "Epoch 223/500\n",
      "2034/2034 [==============================] - 2s 873us/step - loss: 103.3188 - mae: 8.8212 - val_loss: 75.0294 - val_mae: 7.6355\n",
      "Epoch 224/500\n",
      "2034/2034 [==============================] - 2s 851us/step - loss: 105.0274 - mae: 8.9394 - val_loss: 48.0350 - val_mae: 5.8689\n",
      "Epoch 225/500\n",
      "2034/2034 [==============================] - 2s 836us/step - loss: 103.3765 - mae: 8.8469 - val_loss: 62.1103 - val_mae: 6.8245\n",
      "Epoch 226/500\n",
      "2034/2034 [==============================] - 2s 817us/step - loss: 104.8898 - mae: 8.9160 - val_loss: 73.4379 - val_mae: 7.6214\n",
      "Epoch 227/500\n",
      "2034/2034 [==============================] - 2s 805us/step - loss: 554.9000 - mae: 12.0237 - val_loss: 139.2178 - val_mae: 11.2344\n",
      "Epoch 228/500\n",
      "2034/2034 [==============================] - 2s 870us/step - loss: 138.9013 - mae: 10.1943 - val_loss: 56.1850 - val_mae: 6.6443\n",
      "Epoch 229/500\n",
      "2034/2034 [==============================] - 2s 868us/step - loss: 124.6521 - mae: 9.8488 - val_loss: 38.8146 - val_mae: 5.2356\n",
      "Epoch 230/500\n",
      "2034/2034 [==============================] - 2s 1ms/step - loss: 116.3076 - mae: 9.4951 - val_loss: 55.8211 - val_mae: 6.5123\n",
      "Epoch 231/500\n",
      "2034/2034 [==============================] - 2s 773us/step - loss: 116.4693 - mae: 9.4776 - val_loss: 44.7963 - val_mae: 5.5751\n",
      "Epoch 232/500\n",
      "2034/2034 [==============================] - 2s 883us/step - loss: 111.3113 - mae: 9.2471 - val_loss: 66.1725 - val_mae: 7.0843\n",
      "Epoch 233/500\n",
      "2034/2034 [==============================] - 2s 845us/step - loss: 111.1334 - mae: 9.2835 - val_loss: 62.3547 - val_mae: 6.7439\n",
      "Epoch 234/500\n",
      "2034/2034 [==============================] - 2s 856us/step - loss: 110.5060 - mae: 9.1912 - val_loss: 59.0333 - val_mae: 6.5034\n",
      "Epoch 235/500\n",
      "2034/2034 [==============================] - 2s 847us/step - loss: 109.6319 - mae: 9.1511 - val_loss: 65.6601 - val_mae: 6.9005\n",
      "Epoch 236/500\n",
      "2034/2034 [==============================] - 2s 865us/step - loss: 108.2583 - mae: 9.0281 - val_loss: 66.8929 - val_mae: 6.9955\n",
      "Epoch 237/500\n",
      "2034/2034 [==============================] - 2s 831us/step - loss: 111.2787 - mae: 9.2069 - val_loss: 72.6653 - val_mae: 7.4267\n",
      "Epoch 238/500\n",
      "2034/2034 [==============================] - 2s 754us/step - loss: 107.9144 - mae: 9.0508 - val_loss: 75.1691 - val_mae: 7.5831\n",
      "Epoch 239/500\n",
      "2034/2034 [==============================] - 2s 791us/step - loss: 109.4346 - mae: 9.0803 - val_loss: 83.8228 - val_mae: 8.1152\n",
      "Epoch 240/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 108.0442 - mae: 9.0595 - val_loss: 57.6129 - val_mae: 6.4341\n",
      "Epoch 241/500\n",
      "2034/2034 [==============================] - 2s 785us/step - loss: 108.1629 - mae: 9.0605 - val_loss: 63.5397 - val_mae: 6.7937\n",
      "Epoch 242/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 108.2598 - mae: 9.0243 - val_loss: 68.1056 - val_mae: 7.0928\n",
      "Epoch 243/500\n",
      "2034/2034 [==============================] - 2s 809us/step - loss: 108.0646 - mae: 9.0294 - val_loss: 75.8246 - val_mae: 7.6079\n",
      "Epoch 244/500\n",
      "2034/2034 [==============================] - 2s 739us/step - loss: 108.6492 - mae: 9.0779 - val_loss: 64.7614 - val_mae: 6.8869\n",
      "Epoch 245/500\n",
      "2034/2034 [==============================] - 2s 936us/step - loss: 106.3895 - mae: 8.9663 - val_loss: 79.9498 - val_mae: 7.8252\n",
      "Epoch 246/500\n",
      "2034/2034 [==============================] - 2s 816us/step - loss: 107.2487 - mae: 9.0542 - val_loss: 55.2581 - val_mae: 6.2739\n",
      "Epoch 247/500\n",
      "2034/2034 [==============================] - 2s 854us/step - loss: 106.5791 - mae: 8.9773 - val_loss: 66.1679 - val_mae: 6.9496\n",
      "Epoch 248/500\n",
      "2034/2034 [==============================] - 2s 852us/step - loss: 107.0441 - mae: 8.9732 - val_loss: 106.0034 - val_mae: 9.3492\n",
      "Epoch 249/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 107.9854 - mae: 9.0722 - val_loss: 64.4617 - val_mae: 6.8774\n",
      "Epoch 250/500\n",
      "2034/2034 [==============================] - 2s 825us/step - loss: 106.7452 - mae: 9.0022 - val_loss: 69.9914 - val_mae: 7.2094\n",
      "Epoch 251/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 106.9188 - mae: 9.0395 - val_loss: 52.0785 - val_mae: 6.1037\n",
      "Epoch 252/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 107.8322 - mae: 9.0013 - val_loss: 74.9865 - val_mae: 7.5050\n",
      "Epoch 253/500\n",
      "2034/2034 [==============================] - 2s 815us/step - loss: 106.3650 - mae: 8.9766 - val_loss: 65.8236 - val_mae: 6.9122\n",
      "Epoch 254/500\n",
      "2034/2034 [==============================] - 2s 874us/step - loss: 107.5373 - mae: 8.9995 - val_loss: 56.4065 - val_mae: 6.3512\n",
      "Epoch 255/500\n",
      "2034/2034 [==============================] - 2s 858us/step - loss: 109.0409 - mae: 9.1479 - val_loss: 45.6489 - val_mae: 5.6990\n",
      "Epoch 256/500\n",
      "2034/2034 [==============================] - 2s 802us/step - loss: 106.3299 - mae: 8.9898 - val_loss: 66.7412 - val_mae: 6.9961\n",
      "Epoch 257/500\n",
      "2034/2034 [==============================] - 2s 993us/step - loss: 106.7648 - mae: 8.9761 - val_loss: 50.3124 - val_mae: 5.9993\n",
      "Epoch 258/500\n",
      "2034/2034 [==============================] - 2s 861us/step - loss: 106.5901 - mae: 8.9392 - val_loss: 64.0597 - val_mae: 6.8451\n",
      "Epoch 259/500\n",
      "2034/2034 [==============================] - 2s 858us/step - loss: 108.1879 - mae: 9.0872 - val_loss: 62.0315 - val_mae: 6.7234\n",
      "Epoch 260/500\n",
      "2034/2034 [==============================] - 2s 891us/step - loss: 107.0961 - mae: 9.0344 - val_loss: 61.0583 - val_mae: 6.6700\n",
      "Epoch 261/500\n",
      "2034/2034 [==============================] - 2s 799us/step - loss: 107.8637 - mae: 9.0146 - val_loss: 71.6524 - val_mae: 7.3183\n",
      "Epoch 262/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 105.7843 - mae: 8.9028 - val_loss: 78.2626 - val_mae: 7.7233\n",
      "Epoch 263/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 107.3435 - mae: 9.0046 - val_loss: 62.4463 - val_mae: 6.7230\n",
      "Epoch 264/500\n",
      "2034/2034 [==============================] - 2s 829us/step - loss: 107.1496 - mae: 8.9860 - val_loss: 52.8487 - val_mae: 6.1556\n",
      "Epoch 265/500\n",
      "2034/2034 [==============================] - 2s 842us/step - loss: 105.5180 - mae: 8.9157 - val_loss: 74.8556 - val_mae: 7.5724\n",
      "Epoch 266/500\n",
      "2034/2034 [==============================] - 2s 758us/step - loss: 107.2082 - mae: 9.0064 - val_loss: 66.6613 - val_mae: 7.0288\n",
      "Epoch 267/500\n",
      "2034/2034 [==============================] - 2s 782us/step - loss: 107.7277 - mae: 9.0544 - val_loss: 64.9965 - val_mae: 6.9128\n",
      "Epoch 268/500\n",
      "2034/2034 [==============================] - 2s 815us/step - loss: 105.1841 - mae: 8.8853 - val_loss: 75.2356 - val_mae: 7.5414\n",
      "Epoch 269/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 106.1623 - mae: 8.9593 - val_loss: 67.0995 - val_mae: 7.0250\n",
      "Epoch 270/500\n",
      "2034/2034 [==============================] - 2s 840us/step - loss: 106.2140 - mae: 8.9974 - val_loss: 46.0363 - val_mae: 5.7291\n",
      "Epoch 271/500\n",
      "2034/2034 [==============================] - 2s 816us/step - loss: 108.2618 - mae: 9.0480 - val_loss: 48.4060 - val_mae: 5.8946\n",
      "Epoch 272/500\n",
      "2034/2034 [==============================] - 1s 733us/step - loss: 105.9320 - mae: 8.9021 - val_loss: 64.3752 - val_mae: 6.8774\n",
      "Epoch 273/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 106.3503 - mae: 8.9661 - val_loss: 59.1985 - val_mae: 6.5563\n",
      "Epoch 274/500\n",
      "2034/2034 [==============================] - 2s 826us/step - loss: 105.3107 - mae: 8.9091 - val_loss: 73.3779 - val_mae: 7.4173\n",
      "Epoch 275/500\n",
      "2034/2034 [==============================] - 2s 807us/step - loss: 105.2952 - mae: 8.9131 - val_loss: 65.5202 - val_mae: 6.9506\n",
      "Epoch 276/500\n",
      "2034/2034 [==============================] - 2s 822us/step - loss: 105.4621 - mae: 8.9472 - val_loss: 61.9785 - val_mae: 6.7396\n",
      "Epoch 277/500\n",
      "2034/2034 [==============================] - 2s 881us/step - loss: 106.6507 - mae: 8.9793 - val_loss: 54.2030 - val_mae: 6.2615\n",
      "Epoch 278/500\n",
      "2034/2034 [==============================] - 2s 879us/step - loss: 105.9201 - mae: 8.9022 - val_loss: 73.0292 - val_mae: 7.3745\n",
      "Epoch 279/500\n",
      "2034/2034 [==============================] - 2s 847us/step - loss: 104.7530 - mae: 8.8713 - val_loss: 80.1851 - val_mae: 7.8503\n",
      "Epoch 280/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 105.6324 - mae: 8.9113 - val_loss: 67.9401 - val_mae: 7.0926\n",
      "Epoch 281/500\n",
      "2034/2034 [==============================] - 2s 828us/step - loss: 107.4929 - mae: 9.0450 - val_loss: 54.7709 - val_mae: 6.2884\n",
      "Epoch 282/500\n",
      "2034/2034 [==============================] - 2s 904us/step - loss: 105.2150 - mae: 8.9220 - val_loss: 55.1707 - val_mae: 6.2908\n",
      "Epoch 283/500\n",
      "2034/2034 [==============================] - 2s 839us/step - loss: 104.0669 - mae: 8.9268 - val_loss: 43.1009 - val_mae: 5.5784\n",
      "Epoch 284/500\n",
      "2034/2034 [==============================] - 2s 896us/step - loss: 107.2459 - mae: 8.9735 - val_loss: 63.4733 - val_mae: 6.7889\n",
      "Epoch 285/500\n",
      "2034/2034 [==============================] - 2s 818us/step - loss: 105.3304 - mae: 8.8731 - val_loss: 62.3566 - val_mae: 6.7379\n",
      "Epoch 286/500\n",
      "2034/2034 [==============================] - 2s 860us/step - loss: 107.0885 - mae: 9.0184 - val_loss: 62.4785 - val_mae: 6.7688\n",
      "Epoch 287/500\n",
      "2034/2034 [==============================] - 2s 856us/step - loss: 105.5708 - mae: 8.9391 - val_loss: 66.5213 - val_mae: 7.0310\n",
      "Epoch 288/500\n",
      "2034/2034 [==============================] - 2s 815us/step - loss: 104.1581 - mae: 8.8626 - val_loss: 62.8479 - val_mae: 6.7586\n",
      "Epoch 289/500\n",
      "2034/2034 [==============================] - 2s 839us/step - loss: 104.8817 - mae: 8.9161 - val_loss: 79.8901 - val_mae: 7.8321\n",
      "Epoch 290/500\n",
      "2034/2034 [==============================] - 2s 811us/step - loss: 106.8504 - mae: 8.9835 - val_loss: 47.5891 - val_mae: 5.8509\n",
      "Epoch 291/500\n",
      "2034/2034 [==============================] - 2s 870us/step - loss: 107.3137 - mae: 9.0248 - val_loss: 73.1056 - val_mae: 7.4371\n",
      "Epoch 292/500\n",
      "2034/2034 [==============================] - 2s 808us/step - loss: 104.5548 - mae: 8.8911 - val_loss: 62.5574 - val_mae: 6.7554\n",
      "Epoch 293/500\n",
      "2034/2034 [==============================] - 2s 744us/step - loss: 104.6785 - mae: 8.8485 - val_loss: 71.9190 - val_mae: 7.3986\n",
      "Epoch 294/500\n",
      "2034/2034 [==============================] - 2s 783us/step - loss: 105.2295 - mae: 8.9452 - val_loss: 77.2341 - val_mae: 7.6994\n",
      "Epoch 295/500\n",
      "2034/2034 [==============================] - 2s 835us/step - loss: 105.3590 - mae: 8.9155 - val_loss: 76.0451 - val_mae: 7.6093\n",
      "Epoch 296/500\n",
      "2034/2034 [==============================] - 2s 871us/step - loss: 105.8548 - mae: 8.9579 - val_loss: 86.4935 - val_mae: 8.2886\n",
      "Epoch 297/500\n",
      "2034/2034 [==============================] - 2s 756us/step - loss: 103.8603 - mae: 8.8253 - val_loss: 65.0078 - val_mae: 6.8858\n",
      "Epoch 298/500\n",
      "2034/2034 [==============================] - 2s 824us/step - loss: 106.9112 - mae: 8.9672 - val_loss: 63.6864 - val_mae: 6.8124\n",
      "Epoch 299/500\n",
      "2034/2034 [==============================] - 2s 888us/step - loss: 106.3222 - mae: 8.9951 - val_loss: 58.9964 - val_mae: 6.5412\n",
      "Epoch 300/500\n",
      "2034/2034 [==============================] - 2s 816us/step - loss: 103.5212 - mae: 8.8332 - val_loss: 67.2823 - val_mae: 7.0421\n",
      "Epoch 301/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 107.2205 - mae: 8.9953 - val_loss: 85.2947 - val_mae: 8.2016\n",
      "Epoch 302/500\n",
      "2034/2034 [==============================] - 2s 828us/step - loss: 104.7463 - mae: 8.9182 - val_loss: 72.0552 - val_mae: 7.3798\n",
      "Epoch 303/500\n",
      "2034/2034 [==============================] - 2s 807us/step - loss: 104.5744 - mae: 8.9230 - val_loss: 61.9347 - val_mae: 6.7270\n",
      "Epoch 304/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 105.2289 - mae: 8.9154 - val_loss: 57.0498 - val_mae: 6.4010\n",
      "Epoch 305/500\n",
      "2034/2034 [==============================] - 2s 856us/step - loss: 104.9202 - mae: 8.9192 - val_loss: 60.3786 - val_mae: 6.6325\n",
      "Epoch 306/500\n",
      "2034/2034 [==============================] - 2s 863us/step - loss: 105.3540 - mae: 8.8922 - val_loss: 71.5961 - val_mae: 7.3795\n",
      "Epoch 307/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 104.6435 - mae: 8.8980 - val_loss: 81.1541 - val_mae: 7.9763\n",
      "Epoch 308/500\n",
      "2034/2034 [==============================] - 2s 844us/step - loss: 104.7987 - mae: 8.8892 - val_loss: 68.1813 - val_mae: 7.1014\n",
      "Epoch 309/500\n",
      "2034/2034 [==============================] - 2s 839us/step - loss: 104.0810 - mae: 8.8710 - val_loss: 78.7809 - val_mae: 7.8492\n",
      "Epoch 310/500\n",
      "2034/2034 [==============================] - 1s 737us/step - loss: 105.5269 - mae: 8.9679 - val_loss: 78.8030 - val_mae: 7.8080\n",
      "Epoch 311/500\n",
      "2034/2034 [==============================] - 2s 760us/step - loss: 103.6612 - mae: 8.8019 - val_loss: 76.0834 - val_mae: 7.6334\n",
      "Epoch 312/500\n",
      "2034/2034 [==============================] - 2s 768us/step - loss: 107.0611 - mae: 8.9974 - val_loss: 55.3414 - val_mae: 6.3202\n",
      "Epoch 313/500\n",
      "2034/2034 [==============================] - 2s 757us/step - loss: 106.4466 - mae: 9.0129 - val_loss: 63.6811 - val_mae: 6.8493\n",
      "Epoch 314/500\n",
      "2034/2034 [==============================] - 2s 772us/step - loss: 105.2587 - mae: 8.9011 - val_loss: 69.4903 - val_mae: 7.2610\n",
      "Epoch 315/500\n",
      "2034/2034 [==============================] - 1s 729us/step - loss: 104.9000 - mae: 8.9358 - val_loss: 80.2356 - val_mae: 7.8984\n",
      "Epoch 316/500\n",
      "2034/2034 [==============================] - 2s 870us/step - loss: 104.0911 - mae: 8.8362 - val_loss: 68.8002 - val_mae: 7.1785\n",
      "Epoch 317/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 106.4107 - mae: 8.9641 - val_loss: 54.6607 - val_mae: 6.2935\n",
      "Epoch 318/500\n",
      "2034/2034 [==============================] - 2s 835us/step - loss: 104.1533 - mae: 8.8893 - val_loss: 64.5192 - val_mae: 6.9135\n",
      "Epoch 319/500\n",
      "2034/2034 [==============================] - 2s 841us/step - loss: 104.3184 - mae: 8.8633 - val_loss: 82.9902 - val_mae: 8.1062\n",
      "Epoch 320/500\n",
      "2034/2034 [==============================] - 2s 813us/step - loss: 104.8364 - mae: 8.9171 - val_loss: 70.8256 - val_mae: 7.3210\n",
      "Epoch 321/500\n",
      "2034/2034 [==============================] - 2s 795us/step - loss: 105.0955 - mae: 8.9063 - val_loss: 75.4455 - val_mae: 7.6097\n",
      "Epoch 322/500\n",
      "2034/2034 [==============================] - 2s 836us/step - loss: 103.3660 - mae: 8.8025 - val_loss: 96.7499 - val_mae: 8.9048\n",
      "Epoch 323/500\n",
      "2034/2034 [==============================] - 2s 851us/step - loss: 103.6402 - mae: 8.8647 - val_loss: 68.9573 - val_mae: 7.1803\n",
      "Epoch 324/500\n",
      "2034/2034 [==============================] - 2s 831us/step - loss: 115.3869 - mae: 9.0230 - val_loss: 155.9139 - val_mae: 11.8737\n",
      "Epoch 325/500\n",
      "2034/2034 [==============================] - 2s 849us/step - loss: 108.3706 - mae: 9.0835 - val_loss: 65.7624 - val_mae: 6.9927\n",
      "Epoch 326/500\n",
      "2034/2034 [==============================] - 2s 811us/step - loss: 104.4776 - mae: 8.8983 - val_loss: 70.4843 - val_mae: 7.2899\n",
      "Epoch 327/500\n",
      "2034/2034 [==============================] - 2s 919us/step - loss: 104.8190 - mae: 8.9204 - val_loss: 78.6652 - val_mae: 7.8340\n",
      "Epoch 328/500\n",
      "2034/2034 [==============================] - 2s 825us/step - loss: 104.6399 - mae: 8.8872 - val_loss: 59.6221 - val_mae: 6.6024\n",
      "Epoch 329/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 104.3406 - mae: 8.9223 - val_loss: 51.4628 - val_mae: 6.0847\n",
      "Epoch 330/500\n",
      "2034/2034 [==============================] - 2s 875us/step - loss: 105.1299 - mae: 8.9125 - val_loss: 63.2077 - val_mae: 6.8172\n",
      "Epoch 331/500\n",
      "2034/2034 [==============================] - 2s 845us/step - loss: 104.7317 - mae: 8.8767 - val_loss: 83.9986 - val_mae: 8.1897\n",
      "Epoch 332/500\n",
      "2034/2034 [==============================] - 2s 813us/step - loss: 103.5233 - mae: 8.8491 - val_loss: 56.8717 - val_mae: 6.4183\n",
      "Epoch 333/500\n",
      "2034/2034 [==============================] - 2s 862us/step - loss: 104.7769 - mae: 8.8497 - val_loss: 63.4474 - val_mae: 6.8193\n",
      "Epoch 334/500\n",
      "2034/2034 [==============================] - 1s 700us/step - loss: 103.7269 - mae: 8.8589 - val_loss: 74.1937 - val_mae: 7.5165\n",
      "Epoch 335/500\n",
      "2034/2034 [==============================] - 1s 639us/step - loss: 104.4669 - mae: 8.8313 - val_loss: 59.4484 - val_mae: 6.5801\n",
      "Epoch 336/500\n",
      "2034/2034 [==============================] - 1s 686us/step - loss: 104.9927 - mae: 8.8969 - val_loss: 68.2503 - val_mae: 7.1481\n",
      "Epoch 337/500\n",
      "2034/2034 [==============================] - 1s 600us/step - loss: 102.7561 - mae: 8.7690 - val_loss: 68.8770 - val_mae: 7.1800\n",
      "Epoch 338/500\n",
      "2034/2034 [==============================] - 1s 637us/step - loss: 104.2925 - mae: 8.9121 - val_loss: 91.8636 - val_mae: 8.6697\n",
      "Epoch 339/500\n",
      "2034/2034 [==============================] - 1s 703us/step - loss: 103.7813 - mae: 8.8451 - val_loss: 69.8862 - val_mae: 7.2393\n",
      "Epoch 340/500\n",
      "2034/2034 [==============================] - 1s 632us/step - loss: 103.2080 - mae: 8.8320 - val_loss: 76.7451 - val_mae: 7.7004\n",
      "Epoch 341/500\n",
      "2034/2034 [==============================] - 2s 740us/step - loss: 108.7601 - mae: 8.9695 - val_loss: 71.4388 - val_mae: 7.3832\n",
      "Epoch 342/500\n",
      "2034/2034 [==============================] - 1s 685us/step - loss: 103.8489 - mae: 8.8715 - val_loss: 65.7253 - val_mae: 7.0105\n",
      "Epoch 343/500\n",
      "2034/2034 [==============================] - 2s 747us/step - loss: 103.5951 - mae: 8.8476 - val_loss: 69.0453 - val_mae: 7.2165\n",
      "Epoch 344/500\n",
      "2034/2034 [==============================] - 2s 841us/step - loss: 104.0912 - mae: 8.8771 - val_loss: 73.0818 - val_mae: 7.4652\n",
      "Epoch 345/500\n",
      "2034/2034 [==============================] - 2s 811us/step - loss: 103.6689 - mae: 8.8347 - val_loss: 77.5333 - val_mae: 7.7866\n",
      "Epoch 346/500\n",
      "2034/2034 [==============================] - 2s 817us/step - loss: 104.0995 - mae: 8.8751 - val_loss: 81.1505 - val_mae: 7.9988\n",
      "Epoch 347/500\n",
      "2034/2034 [==============================] - 2s 831us/step - loss: 103.9662 - mae: 8.8705 - val_loss: 53.6505 - val_mae: 6.2093\n",
      "Epoch 348/500\n",
      "2034/2034 [==============================] - 2s 795us/step - loss: 104.7849 - mae: 8.9041 - val_loss: 54.8939 - val_mae: 6.3091\n",
      "Epoch 349/500\n",
      "2034/2034 [==============================] - 2s 810us/step - loss: 103.7064 - mae: 8.8153 - val_loss: 71.6669 - val_mae: 7.3900\n",
      "Epoch 350/500\n",
      "2034/2034 [==============================] - 2s 785us/step - loss: 104.2861 - mae: 8.9239 - val_loss: 63.1852 - val_mae: 6.8479\n",
      "Epoch 351/500\n",
      "2034/2034 [==============================] - 2s 829us/step - loss: 105.0041 - mae: 8.8644 - val_loss: 66.9144 - val_mae: 7.1044\n",
      "Epoch 352/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 103.2213 - mae: 8.8252 - val_loss: 68.3084 - val_mae: 7.1747\n",
      "Epoch 353/500\n",
      "2034/2034 [==============================] - 2s 915us/step - loss: 103.5005 - mae: 8.7739 - val_loss: 86.9074 - val_mae: 8.4001\n",
      "Epoch 354/500\n",
      "2034/2034 [==============================] - 2s 814us/step - loss: 104.2522 - mae: 8.9309 - val_loss: 60.3892 - val_mae: 6.6741\n",
      "Epoch 355/500\n",
      "2034/2034 [==============================] - 2s 858us/step - loss: 103.2996 - mae: 8.8177 - val_loss: 60.0050 - val_mae: 6.6364\n",
      "Epoch 356/500\n",
      "2034/2034 [==============================] - 2s 778us/step - loss: 103.1285 - mae: 8.8410 - val_loss: 76.1745 - val_mae: 7.7198\n",
      "Epoch 357/500\n",
      "2034/2034 [==============================] - 2s 785us/step - loss: 103.4561 - mae: 8.8804 - val_loss: 52.1734 - val_mae: 6.1618\n",
      "Epoch 358/500\n",
      "2034/2034 [==============================] - 2s 757us/step - loss: 104.9183 - mae: 8.9160 - val_loss: 68.1200 - val_mae: 7.1823\n",
      "Epoch 359/500\n",
      "2034/2034 [==============================] - 1s 651us/step - loss: 102.8887 - mae: 8.8413 - val_loss: 49.7714 - val_mae: 5.9208\n",
      "Epoch 360/500\n",
      "2034/2034 [==============================] - 1s 687us/step - loss: 103.9356 - mae: 8.8686 - val_loss: 66.8556 - val_mae: 7.1437\n",
      "Epoch 361/500\n",
      "2034/2034 [==============================] - 1s 673us/step - loss: 103.8165 - mae: 8.8542 - val_loss: 66.9516 - val_mae: 7.1001\n",
      "Epoch 362/500\n",
      "2034/2034 [==============================] - 1s 630us/step - loss: 105.5356 - mae: 8.8901 - val_loss: 85.2714 - val_mae: 8.2914\n",
      "Epoch 363/500\n",
      "2034/2034 [==============================] - 1s 665us/step - loss: 103.4915 - mae: 8.8367 - val_loss: 92.8405 - val_mae: 8.7392\n",
      "Epoch 364/500\n",
      "2034/2034 [==============================] - 1s 691us/step - loss: 104.3415 - mae: 8.9199 - val_loss: 58.1320 - val_mae: 6.4919\n",
      "Epoch 365/500\n",
      "2034/2034 [==============================] - 1s 686us/step - loss: 103.1943 - mae: 8.8072 - val_loss: 53.4235 - val_mae: 6.2037\n",
      "Epoch 366/500\n",
      "2034/2034 [==============================] - 2s 741us/step - loss: 102.6891 - mae: 8.7909 - val_loss: 71.8962 - val_mae: 7.4067\n",
      "Epoch 367/500\n",
      "2034/2034 [==============================] - 2s 770us/step - loss: 103.1113 - mae: 8.8478 - val_loss: 74.9919 - val_mae: 7.5982\n",
      "Epoch 368/500\n",
      "2034/2034 [==============================] - 1s 686us/step - loss: 104.3726 - mae: 8.8794 - val_loss: 76.8132 - val_mae: 7.7594\n",
      "Epoch 369/500\n",
      "2034/2034 [==============================] - 1s 682us/step - loss: 102.7930 - mae: 8.7666 - val_loss: 69.4008 - val_mae: 7.2851\n",
      "Epoch 370/500\n",
      "2034/2034 [==============================] - 1s 718us/step - loss: 102.9930 - mae: 8.8237 - val_loss: 60.0400 - val_mae: 6.6455\n",
      "Epoch 371/500\n",
      "2034/2034 [==============================] - 1s 614us/step - loss: 103.2134 - mae: 8.7931 - val_loss: 76.0283 - val_mae: 7.7184\n",
      "Epoch 372/500\n",
      "2034/2034 [==============================] - 1s 620us/step - loss: 103.3162 - mae: 8.8318 - val_loss: 59.4160 - val_mae: 6.5905\n",
      "Epoch 373/500\n",
      "2034/2034 [==============================] - 1s 602us/step - loss: 159.2143 - mae: 9.4402 - val_loss: 68.1377 - val_mae: 7.1396\n",
      "Epoch 374/500\n",
      "2034/2034 [==============================] - 1s 640us/step - loss: 105.2625 - mae: 8.9359 - val_loss: 85.8311 - val_mae: 8.3091\n",
      "Epoch 375/500\n",
      "2034/2034 [==============================] - 1s 666us/step - loss: 103.5095 - mae: 8.8828 - val_loss: 58.1595 - val_mae: 6.5074\n",
      "Epoch 376/500\n",
      "2034/2034 [==============================] - 1s 644us/step - loss: 102.9967 - mae: 8.7833 - val_loss: 66.5627 - val_mae: 7.0380\n",
      "Epoch 377/500\n",
      "2034/2034 [==============================] - 1s 729us/step - loss: 103.0577 - mae: 8.8421 - val_loss: 66.2972 - val_mae: 7.0239\n",
      "Epoch 378/500\n",
      "2034/2034 [==============================] - 1s 690us/step - loss: 103.4276 - mae: 8.7952 - val_loss: 87.5765 - val_mae: 8.4125\n",
      "Epoch 379/500\n",
      "2034/2034 [==============================] - 1s 719us/step - loss: 102.7884 - mae: 8.7924 - val_loss: 71.3899 - val_mae: 7.3756\n",
      "Epoch 380/500\n",
      "2034/2034 [==============================] - 1s 690us/step - loss: 143.5432 - mae: 9.5103 - val_loss: 79.9047 - val_mae: 7.9382\n",
      "Epoch 381/500\n",
      "2034/2034 [==============================] - 2s 838us/step - loss: 104.1776 - mae: 8.8835 - val_loss: 84.1763 - val_mae: 8.2334\n",
      "Epoch 382/500\n",
      "2034/2034 [==============================] - 2s 836us/step - loss: 102.5216 - mae: 8.8082 - val_loss: 69.8914 - val_mae: 7.2840\n",
      "Epoch 383/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 103.6346 - mae: 8.8817 - val_loss: 81.9940 - val_mae: 8.0693\n",
      "Epoch 384/500\n",
      "2034/2034 [==============================] - 2s 815us/step - loss: 103.0928 - mae: 8.8312 - val_loss: 65.4341 - val_mae: 6.9835\n",
      "Epoch 385/500\n",
      "2034/2034 [==============================] - 2s 762us/step - loss: 102.8694 - mae: 8.8225 - val_loss: 74.2582 - val_mae: 7.5393\n",
      "Epoch 386/500\n",
      "2034/2034 [==============================] - 2s 768us/step - loss: 101.5425 - mae: 8.7652 - val_loss: 83.9809 - val_mae: 8.1390\n",
      "Epoch 387/500\n",
      "2034/2034 [==============================] - 2s 781us/step - loss: 102.4882 - mae: 8.7729 - val_loss: 83.5042 - val_mae: 8.0974\n",
      "Epoch 388/500\n",
      "2034/2034 [==============================] - 2s 845us/step - loss: 103.5119 - mae: 8.8411 - val_loss: 76.2922 - val_mae: 7.6876\n",
      "Epoch 389/500\n",
      "2034/2034 [==============================] - 2s 783us/step - loss: 102.9720 - mae: 8.7837 - val_loss: 96.2728 - val_mae: 8.9139\n",
      "Epoch 390/500\n",
      "2034/2034 [==============================] - 2s 853us/step - loss: 103.1613 - mae: 8.8434 - val_loss: 72.4149 - val_mae: 7.4604\n",
      "Epoch 391/500\n",
      "2034/2034 [==============================] - 2s 839us/step - loss: 103.1979 - mae: 8.7833 - val_loss: 77.7556 - val_mae: 7.8222\n",
      "Epoch 392/500\n",
      "2034/2034 [==============================] - 2s 961us/step - loss: 102.5569 - mae: 8.8493 - val_loss: 52.3320 - val_mae: 6.1432\n",
      "Epoch 393/500\n",
      "2034/2034 [==============================] - 2s 867us/step - loss: 103.5700 - mae: 8.8635 - val_loss: 40.9045 - val_mae: 5.3743\n",
      "Epoch 394/500\n",
      "2034/2034 [==============================] - 2s 826us/step - loss: 102.1697 - mae: 8.7652 - val_loss: 57.7483 - val_mae: 6.4777\n",
      "Epoch 395/500\n",
      "2034/2034 [==============================] - 2s 836us/step - loss: 103.5453 - mae: 8.8147 - val_loss: 75.7669 - val_mae: 7.7015\n",
      "Epoch 396/500\n",
      "2034/2034 [==============================] - 2s 857us/step - loss: 103.0045 - mae: 8.7746 - val_loss: 69.2124 - val_mae: 7.2192\n",
      "Epoch 397/500\n",
      "2034/2034 [==============================] - 2s 806us/step - loss: 102.7842 - mae: 8.8157 - val_loss: 63.5676 - val_mae: 6.8678\n",
      "Epoch 398/500\n",
      "2034/2034 [==============================] - 2s 903us/step - loss: 102.2171 - mae: 8.7697 - val_loss: 67.7515 - val_mae: 7.1538\n",
      "Epoch 399/500\n",
      "2034/2034 [==============================] - 2s 800us/step - loss: 101.2941 - mae: 8.7057 - val_loss: 80.0679 - val_mae: 7.9424\n",
      "Epoch 400/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 102.2646 - mae: 8.8009 - val_loss: 66.4874 - val_mae: 7.0341\n",
      "Epoch 401/500\n",
      "2034/2034 [==============================] - 2s 834us/step - loss: 102.0789 - mae: 8.7401 - val_loss: 74.2136 - val_mae: 7.5737\n",
      "Epoch 402/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 102.0311 - mae: 8.7252 - val_loss: 77.6072 - val_mae: 7.8045\n",
      "Epoch 403/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 103.0145 - mae: 8.8603 - val_loss: 62.9711 - val_mae: 6.8414\n",
      "Epoch 404/500\n",
      "2034/2034 [==============================] - 2s 842us/step - loss: 102.9265 - mae: 8.7687 - val_loss: 65.0557 - val_mae: 6.9773\n",
      "Epoch 405/500\n",
      "2034/2034 [==============================] - 2s 767us/step - loss: 101.3688 - mae: 8.7356 - val_loss: 54.2559 - val_mae: 6.2477\n",
      "Epoch 406/500\n",
      "2034/2034 [==============================] - 2s 786us/step - loss: 101.1071 - mae: 8.6887 - val_loss: 67.6721 - val_mae: 7.1999\n",
      "Epoch 407/500\n",
      "2034/2034 [==============================] - 2s 763us/step - loss: 101.2393 - mae: 8.7412 - val_loss: 65.8993 - val_mae: 7.0444\n",
      "Epoch 408/500\n",
      "2034/2034 [==============================] - 2s 777us/step - loss: 101.8299 - mae: 8.7458 - val_loss: 66.1626 - val_mae: 7.0551\n",
      "Epoch 409/500\n",
      "2034/2034 [==============================] - 2s 780us/step - loss: 101.3217 - mae: 8.7252 - val_loss: 69.2369 - val_mae: 7.2476\n",
      "Epoch 410/500\n",
      "2034/2034 [==============================] - 2s 763us/step - loss: 101.2959 - mae: 8.7071 - val_loss: 57.4485 - val_mae: 6.4573\n",
      "Epoch 411/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 102.9270 - mae: 8.7513 - val_loss: 64.7760 - val_mae: 6.9467\n",
      "Epoch 412/500\n",
      "2034/2034 [==============================] - 2s 749us/step - loss: 101.4098 - mae: 8.7336 - val_loss: 68.2351 - val_mae: 7.1896\n",
      "Epoch 413/500\n",
      "2034/2034 [==============================] - 2s 751us/step - loss: 101.8286 - mae: 8.7423 - val_loss: 71.4127 - val_mae: 7.3784\n",
      "Epoch 414/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 101.5091 - mae: 8.6988 - val_loss: 78.4675 - val_mae: 7.8508\n",
      "Epoch 415/500\n",
      "2034/2034 [==============================] - 2s 810us/step - loss: 101.1029 - mae: 8.7476 - val_loss: 62.9911 - val_mae: 6.8266\n",
      "Epoch 416/500\n",
      "2034/2034 [==============================] - 2s 746us/step - loss: 100.3374 - mae: 8.6730 - val_loss: 63.7203 - val_mae: 6.8720\n",
      "Epoch 417/500\n",
      "2034/2034 [==============================] - 2s 802us/step - loss: 101.8754 - mae: 8.6932 - val_loss: 71.5349 - val_mae: 7.4110\n",
      "Epoch 418/500\n",
      "2034/2034 [==============================] - 2s 815us/step - loss: 99.6311 - mae: 8.6453 - val_loss: 66.8941 - val_mae: 7.0922\n",
      "Epoch 419/500\n",
      "2034/2034 [==============================] - 2s 740us/step - loss: 100.3201 - mae: 8.6676 - val_loss: 63.1572 - val_mae: 6.8284\n",
      "Epoch 420/500\n",
      "2034/2034 [==============================] - 2s 791us/step - loss: 101.6642 - mae: 8.7555 - val_loss: 79.8437 - val_mae: 7.9450\n",
      "Epoch 421/500\n",
      "2034/2034 [==============================] - 2s 799us/step - loss: 99.3067 - mae: 8.6032 - val_loss: 54.2671 - val_mae: 6.2128\n",
      "Epoch 422/500\n",
      "2034/2034 [==============================] - 2s 773us/step - loss: 100.1896 - mae: 8.6200 - val_loss: 61.2948 - val_mae: 6.7078\n",
      "Epoch 423/500\n",
      "2034/2034 [==============================] - 2s 800us/step - loss: 101.0706 - mae: 8.6217 - val_loss: 68.8460 - val_mae: 7.1720\n",
      "Epoch 424/500\n",
      "2034/2034 [==============================] - 2s 777us/step - loss: 101.0678 - mae: 8.7060 - val_loss: 49.0501 - val_mae: 5.9101\n",
      "Epoch 425/500\n",
      "2034/2034 [==============================] - 2s 853us/step - loss: 98.4082 - mae: 8.5099 - val_loss: 64.2249 - val_mae: 6.8744\n",
      "Epoch 426/500\n",
      "2034/2034 [==============================] - 2s 747us/step - loss: 98.7659 - mae: 8.5419 - val_loss: 66.2155 - val_mae: 6.9970\n",
      "Epoch 427/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 99.6730 - mae: 8.6211 - val_loss: 77.3920 - val_mae: 7.7466\n",
      "Epoch 428/500\n",
      "2034/2034 [==============================] - 2s 790us/step - loss: 99.4892 - mae: 8.6119 - val_loss: 57.9477 - val_mae: 6.4634\n",
      "Epoch 429/500\n",
      "2034/2034 [==============================] - 2s 769us/step - loss: 103.2782 - mae: 8.7092 - val_loss: 78.4403 - val_mae: 7.7687\n",
      "Epoch 430/500\n",
      "2034/2034 [==============================] - 2s 767us/step - loss: 100.0856 - mae: 8.6217 - val_loss: 71.3509 - val_mae: 7.3323\n",
      "Epoch 431/500\n",
      "2034/2034 [==============================] - 2s 801us/step - loss: 100.0285 - mae: 8.5822 - val_loss: 71.3104 - val_mae: 7.3283\n",
      "Epoch 432/500\n",
      "2034/2034 [==============================] - 2s 856us/step - loss: 97.8979 - mae: 8.5301 - val_loss: 67.8658 - val_mae: 7.0732\n",
      "Epoch 433/500\n",
      "2034/2034 [==============================] - 2s 760us/step - loss: 99.3358 - mae: 8.5671 - val_loss: 56.0761 - val_mae: 6.3557\n",
      "Epoch 434/500\n",
      "2034/2034 [==============================] - 2s 787us/step - loss: 100.2851 - mae: 8.5708 - val_loss: 64.3100 - val_mae: 6.8694\n",
      "Epoch 435/500\n",
      "2034/2034 [==============================] - 2s 864us/step - loss: 100.1928 - mae: 8.5866 - val_loss: 60.8970 - val_mae: 6.6724\n",
      "Epoch 436/500\n",
      "2034/2034 [==============================] - 2s 828us/step - loss: 98.4159 - mae: 8.5286 - val_loss: 61.1918 - val_mae: 6.6868\n",
      "Epoch 437/500\n",
      "2034/2034 [==============================] - 2s 828us/step - loss: 99.3329 - mae: 8.5409 - val_loss: 65.8162 - val_mae: 6.9496\n",
      "Epoch 438/500\n",
      "2034/2034 [==============================] - 2s 835us/step - loss: 97.6882 - mae: 8.4504 - val_loss: 67.9699 - val_mae: 7.0783\n",
      "Epoch 439/500\n",
      "2034/2034 [==============================] - 2s 773us/step - loss: 97.9257 - mae: 8.4626 - val_loss: 60.3905 - val_mae: 6.5598\n",
      "Epoch 440/500\n",
      "2034/2034 [==============================] - 2s 820us/step - loss: 97.5845 - mae: 8.4861 - val_loss: 47.3448 - val_mae: 5.7554\n",
      "Epoch 441/500\n",
      "2034/2034 [==============================] - 1s 723us/step - loss: 97.7136 - mae: 8.4671 - val_loss: 60.8040 - val_mae: 6.6373\n",
      "Epoch 442/500\n",
      "2034/2034 [==============================] - 1s 719us/step - loss: 96.1165 - mae: 8.3949 - val_loss: 70.7429 - val_mae: 7.2273\n",
      "Epoch 443/500\n",
      "2034/2034 [==============================] - 1s 637us/step - loss: 95.9682 - mae: 8.3821 - val_loss: 67.3183 - val_mae: 7.0319\n",
      "Epoch 444/500\n",
      "2034/2034 [==============================] - 1s 637us/step - loss: 98.7936 - mae: 8.5141 - val_loss: 59.8184 - val_mae: 6.5937\n",
      "Epoch 445/500\n",
      "2034/2034 [==============================] - 1s 660us/step - loss: 97.7692 - mae: 8.4651 - val_loss: 64.8854 - val_mae: 6.8870\n",
      "Epoch 446/500\n",
      "2034/2034 [==============================] - 1s 644us/step - loss: 98.2779 - mae: 8.4773 - val_loss: 51.1180 - val_mae: 6.0379\n",
      "Epoch 447/500\n",
      "2034/2034 [==============================] - 1s 707us/step - loss: 98.7007 - mae: 8.4997 - val_loss: 68.3023 - val_mae: 7.1135\n",
      "Epoch 448/500\n",
      "2034/2034 [==============================] - 1s 731us/step - loss: 97.5169 - mae: 8.5268 - val_loss: 56.7744 - val_mae: 6.3823\n",
      "Epoch 449/500\n",
      "2034/2034 [==============================] - 1s 658us/step - loss: 96.9386 - mae: 8.4577 - val_loss: 57.3809 - val_mae: 6.4033\n",
      "Epoch 450/500\n",
      "2034/2034 [==============================] - 1s 696us/step - loss: 99.4147 - mae: 8.5243 - val_loss: 65.4956 - val_mae: 6.9120\n",
      "Epoch 451/500\n",
      "2034/2034 [==============================] - 1s 642us/step - loss: 97.1097 - mae: 8.4103 - val_loss: 57.3517 - val_mae: 6.4148\n",
      "Epoch 452/500\n",
      "2034/2034 [==============================] - 1s 623us/step - loss: 96.6758 - mae: 8.3661 - val_loss: 60.9147 - val_mae: 6.6332\n",
      "Epoch 453/500\n",
      "2034/2034 [==============================] - 1s 605us/step - loss: 108.6234 - mae: 8.6375 - val_loss: 69.7501 - val_mae: 7.2145\n",
      "Epoch 454/500\n",
      "2034/2034 [==============================] - 1s 642us/step - loss: 96.9825 - mae: 8.4247 - val_loss: 57.1119 - val_mae: 6.4373\n",
      "Epoch 455/500\n",
      "2034/2034 [==============================] - 2s 749us/step - loss: 96.7405 - mae: 8.4090 - val_loss: 58.8360 - val_mae: 6.5456\n",
      "Epoch 456/500\n",
      "2034/2034 [==============================] - 2s 822us/step - loss: 96.3878 - mae: 8.3903 - val_loss: 65.1684 - val_mae: 6.9291\n",
      "Epoch 457/500\n",
      "2034/2034 [==============================] - 2s 810us/step - loss: 93.9250 - mae: 8.2938 - val_loss: 55.1120 - val_mae: 6.3304\n",
      "Epoch 458/500\n",
      "2034/2034 [==============================] - 2s 826us/step - loss: 95.1060 - mae: 8.3188 - val_loss: 61.2844 - val_mae: 6.6876\n",
      "Epoch 459/500\n",
      "2034/2034 [==============================] - 2s 842us/step - loss: 97.2401 - mae: 8.3674 - val_loss: 64.5153 - val_mae: 6.8206\n",
      "Epoch 460/500\n",
      "2034/2034 [==============================] - 2s 830us/step - loss: 96.6216 - mae: 8.3936 - val_loss: 53.1785 - val_mae: 6.2010\n",
      "Epoch 461/500\n",
      "2034/2034 [==============================] - 2s 833us/step - loss: 97.0500 - mae: 8.3117 - val_loss: 68.1185 - val_mae: 7.0927\n",
      "Epoch 462/500\n",
      "2034/2034 [==============================] - 2s 866us/step - loss: 94.1817 - mae: 8.2864 - val_loss: 62.6015 - val_mae: 6.7613\n",
      "Epoch 463/500\n",
      "2034/2034 [==============================] - 2s 863us/step - loss: 94.9413 - mae: 8.3246 - val_loss: 58.8077 - val_mae: 6.5455\n",
      "Epoch 464/500\n",
      "2034/2034 [==============================] - 2s 751us/step - loss: 93.5953 - mae: 8.2453 - val_loss: 58.7700 - val_mae: 6.5464\n",
      "Epoch 465/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 96.6070 - mae: 8.3374 - val_loss: 59.8540 - val_mae: 6.5910\n",
      "Epoch 466/500\n",
      "2034/2034 [==============================] - 2s 844us/step - loss: 103.7514 - mae: 8.3502 - val_loss: 67.8283 - val_mae: 7.0812\n",
      "Epoch 467/500\n",
      "2034/2034 [==============================] - 2s 821us/step - loss: 96.3277 - mae: 8.4741 - val_loss: 58.4943 - val_mae: 6.5280\n",
      "Epoch 468/500\n",
      "2034/2034 [==============================] - 2s 797us/step - loss: 93.7525 - mae: 8.2484 - val_loss: 58.1875 - val_mae: 6.5203\n",
      "Epoch 469/500\n",
      "2034/2034 [==============================] - 2s 866us/step - loss: 95.4223 - mae: 8.3222 - val_loss: 59.8172 - val_mae: 6.6295\n",
      "Epoch 470/500\n",
      "2034/2034 [==============================] - 2s 804us/step - loss: 93.4149 - mae: 8.2153 - val_loss: 61.7454 - val_mae: 6.7115\n",
      "Epoch 471/500\n",
      "2034/2034 [==============================] - 2s 846us/step - loss: 93.2130 - mae: 8.2367 - val_loss: 48.6420 - val_mae: 5.9608\n",
      "Epoch 472/500\n",
      "2034/2034 [==============================] - 2s 871us/step - loss: 95.8278 - mae: 8.3557 - val_loss: 51.1596 - val_mae: 6.0930\n",
      "Epoch 473/500\n",
      "2034/2034 [==============================] - 2s 799us/step - loss: 93.8279 - mae: 8.2174 - val_loss: 56.2398 - val_mae: 6.4050\n",
      "Epoch 474/500\n",
      "2034/2034 [==============================] - 2s 829us/step - loss: 108.3104 - mae: 8.4717 - val_loss: 69.3114 - val_mae: 7.1602\n",
      "Epoch 475/500\n",
      "2034/2034 [==============================] - 2s 861us/step - loss: 94.2350 - mae: 8.2655 - val_loss: 56.4012 - val_mae: 6.3623\n",
      "Epoch 476/500\n",
      "2034/2034 [==============================] - 2s 863us/step - loss: 100.0000 - mae: 8.3540 - val_loss: 63.7840 - val_mae: 6.8198\n",
      "Epoch 477/500\n",
      "2034/2034 [==============================] - 2s 835us/step - loss: 101.1912 - mae: 8.3372 - val_loss: 52.8601 - val_mae: 6.1672\n",
      "Epoch 478/500\n",
      "2034/2034 [==============================] - 2s 798us/step - loss: 95.7572 - mae: 8.2882 - val_loss: 55.1003 - val_mae: 6.3046\n",
      "Epoch 479/500\n",
      "2034/2034 [==============================] - 2s 859us/step - loss: 93.1296 - mae: 8.1683 - val_loss: 59.9244 - val_mae: 6.5944\n",
      "Epoch 480/500\n",
      "2034/2034 [==============================] - 2s 781us/step - loss: 93.9678 - mae: 8.2564 - val_loss: 54.7074 - val_mae: 6.2784\n",
      "Epoch 481/500\n",
      "2034/2034 [==============================] - 2s 802us/step - loss: 92.2044 - mae: 8.1803 - val_loss: 53.0530 - val_mae: 6.1997\n",
      "Epoch 482/500\n",
      "2034/2034 [==============================] - 2s 803us/step - loss: 92.2304 - mae: 8.1915 - val_loss: 49.4504 - val_mae: 5.9984\n",
      "Epoch 483/500\n",
      "2034/2034 [==============================] - 1s 670us/step - loss: 91.3238 - mae: 8.1408 - val_loss: 50.6721 - val_mae: 6.0659\n",
      "Epoch 484/500\n",
      "2034/2034 [==============================] - 1s 658us/step - loss: 92.2953 - mae: 8.0849 - val_loss: 48.7954 - val_mae: 5.9300\n",
      "Epoch 485/500\n",
      "2034/2034 [==============================] - 1s 685us/step - loss: 91.9919 - mae: 8.0852 - val_loss: 54.4334 - val_mae: 6.2874\n",
      "Epoch 486/500\n",
      "2034/2034 [==============================] - 1s 644us/step - loss: 93.7533 - mae: 8.0929 - val_loss: 53.3943 - val_mae: 6.1849\n",
      "Epoch 487/500\n",
      "2034/2034 [==============================] - 1s 656us/step - loss: 90.1405 - mae: 8.0112 - val_loss: 46.6207 - val_mae: 5.8145\n",
      "Epoch 488/500\n",
      "2034/2034 [==============================] - 1s 677us/step - loss: 90.6625 - mae: 8.0254 - val_loss: 52.2801 - val_mae: 6.1211\n",
      "Epoch 489/500\n",
      "2034/2034 [==============================] - 1s 694us/step - loss: 186.3205 - mae: 8.6622 - val_loss: 60.1387 - val_mae: 6.5622\n",
      "Epoch 490/500\n",
      "2034/2034 [==============================] - 1s 709us/step - loss: 91.2318 - mae: 8.0790 - val_loss: 53.5420 - val_mae: 6.1830\n",
      "Epoch 491/500\n",
      "2034/2034 [==============================] - 2s 805us/step - loss: 91.4538 - mae: 8.1094 - val_loss: 53.6671 - val_mae: 6.1740\n",
      "Epoch 492/500\n",
      "2034/2034 [==============================] - 1s 638us/step - loss: 92.3076 - mae: 8.1324 - val_loss: 54.1111 - val_mae: 6.2261\n",
      "Epoch 493/500\n",
      "2034/2034 [==============================] - 1s 648us/step - loss: 90.5896 - mae: 8.1100 - val_loss: 50.5000 - val_mae: 6.0234\n",
      "Epoch 494/500\n",
      "2034/2034 [==============================] - 1s 632us/step - loss: 90.1864 - mae: 8.0499 - val_loss: 54.9979 - val_mae: 6.2815\n",
      "Epoch 495/500\n",
      "2034/2034 [==============================] - 2s 775us/step - loss: 89.4626 - mae: 8.0146 - val_loss: 51.3723 - val_mae: 6.0925\n",
      "Epoch 496/500\n",
      "2034/2034 [==============================] - 1s 671us/step - loss: 89.6412 - mae: 8.0189 - val_loss: 49.2308 - val_mae: 5.9340\n",
      "Epoch 497/500\n",
      "2034/2034 [==============================] - 2s 739us/step - loss: 88.1941 - mae: 7.9118 - val_loss: 48.5008 - val_mae: 5.9158\n",
      "Epoch 498/500\n",
      "2034/2034 [==============================] - 1s 609us/step - loss: 9694.0510 - mae: 10.0856 - val_loss: 53.4750 - val_mae: 6.2038\n",
      "Epoch 499/500\n",
      "2034/2034 [==============================] - 1s 601us/step - loss: 98.6418 - mae: 8.6791 - val_loss: 63.1840 - val_mae: 6.8378\n",
      "Epoch 500/500\n",
      "2034/2034 [==============================] - 2s 754us/step - loss: 99.4723 - mae: 8.6741 - val_loss: 63.9509 - val_mae: 6.8778\n",
      "Test Score: 8.00 RMSE\n"
     ]
    }
   ],
   "source": [
    "fourth = data[data['zip5'] == 33182]\n",
    "fourth = fourth.drop(['zip5', 'date_key'], axis =1)\n",
    "fourth_17_18 = fourth[fourth['year'] != 2019]\n",
    "y = fourth_17_18['impact_score']\n",
    "X = fourth_17_18.drop(['impact_score'], axis = 1)\n",
    "X = np.expand_dims(X, axis = 2)\n",
    "\n",
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=4, activation=\"relu\", filters=16),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=4, activation=\"relu\", filters=16),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),\n",
    "        Dense(1, activation='linear'),# For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "test_size = int(0.2 * 2862)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=25, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "#print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "#for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "#    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f7e36f69f60>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e36e6e4a8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e36deceb8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 164, 16)           80        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 79, 16)            1040      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 79, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 1265      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2,387\n",
      "Trainable params: 2,387\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2290 samples, validate on 572 samples\n",
      "Epoch 1/500\n",
      "2290/2290 [==============================] - 3s 1ms/step - loss: 4630154.0961 - mae: 1585.7118 - val_loss: 19551.5619 - val_mae: 125.7547\n",
      "Epoch 2/500\n",
      "2290/2290 [==============================] - 2s 832us/step - loss: 886235.2443 - mae: 667.3837 - val_loss: 5507.3944 - val_mae: 52.9523\n",
      "Epoch 3/500\n",
      "2290/2290 [==============================] - 2s 737us/step - loss: 432958.4492 - mae: 453.6788 - val_loss: 3362.9269 - val_mae: 49.8171\n",
      "Epoch 4/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 405995.6275 - mae: 330.2762 - val_loss: 3080.0490 - val_mae: 45.5166\n",
      "Epoch 5/500\n",
      "2290/2290 [==============================] - 2s 896us/step - loss: 915779.7291 - mae: 274.5931 - val_loss: 2715.4887 - val_mae: 41.7341\n",
      "Epoch 6/500\n",
      "2290/2290 [==============================] - 2s 840us/step - loss: 199511.1791 - mae: 211.1556 - val_loss: 2875.5064 - val_mae: 47.4286\n",
      "Epoch 7/500\n",
      "2290/2290 [==============================] - 2s 872us/step - loss: 207614.6950 - mae: 182.3171 - val_loss: 1775.7880 - val_mae: 36.5196\n",
      "Epoch 8/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 89402.5682 - mae: 155.9039 - val_loss: 1567.7072 - val_mae: 35.4831\n",
      "Epoch 9/500\n",
      "2290/2290 [==============================] - 2s 831us/step - loss: 152277.3774 - mae: 136.3656 - val_loss: 1417.9998 - val_mae: 34.6198\n",
      "Epoch 10/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 204883.3019 - mae: 118.6124 - val_loss: 1284.7209 - val_mae: 33.3878\n",
      "Epoch 11/500\n",
      "2290/2290 [==============================] - 2s 719us/step - loss: 152182.5652 - mae: 105.7040 - val_loss: 1900.9623 - val_mae: 41.9918\n",
      "Epoch 12/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 145061.0963 - mae: 97.1980 - val_loss: 373.7226 - val_mae: 16.5026\n",
      "Epoch 13/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 27055.4762 - mae: 78.1518 - val_loss: 391.9513 - val_mae: 17.4237\n",
      "Epoch 14/500\n",
      "2290/2290 [==============================] - 2s 824us/step - loss: 18351.2287 - mae: 70.5349 - val_loss: 521.1161 - val_mae: 21.0746\n",
      "Epoch 15/500\n",
      "2290/2290 [==============================] - 2s 712us/step - loss: 21523.6599 - mae: 68.6362 - val_loss: 731.0319 - val_mae: 25.7322\n",
      "Epoch 16/500\n",
      "2290/2290 [==============================] - 1s 649us/step - loss: 17536.6241 - mae: 60.7464 - val_loss: 469.3846 - val_mae: 20.3716\n",
      "Epoch 17/500\n",
      "2290/2290 [==============================] - 2s 677us/step - loss: 16233.4495 - mae: 59.2292 - val_loss: 510.6566 - val_mae: 21.6042\n",
      "Epoch 18/500\n",
      "2290/2290 [==============================] - 1s 651us/step - loss: 62401.8139 - mae: 59.0379 - val_loss: 484.4071 - val_mae: 21.2349\n",
      "Epoch 19/500\n",
      "2290/2290 [==============================] - 2s 690us/step - loss: 38493.2460 - mae: 52.0468 - val_loss: 213.8171 - val_mae: 13.6325\n",
      "Epoch 20/500\n",
      "2290/2290 [==============================] - 2s 723us/step - loss: 34520.9957 - mae: 48.9823 - val_loss: 224.6954 - val_mae: 14.0730\n",
      "Epoch 21/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 21013.9484 - mae: 43.8058 - val_loss: 294.4117 - val_mae: 16.5213\n",
      "Epoch 22/500\n",
      "2290/2290 [==============================] - 2s 841us/step - loss: 18104.1504 - mae: 40.7681 - val_loss: 207.8869 - val_mae: 13.7369\n",
      "Epoch 23/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 8383.8828 - mae: 37.0224 - val_loss: 78.0897 - val_mae: 7.7633\n",
      "Epoch 24/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 15890.2717 - mae: 35.3589 - val_loss: 137.8630 - val_mae: 11.1919\n",
      "Epoch 25/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 8723.6956 - mae: 32.3890 - val_loss: 90.7394 - val_mae: 8.9117\n",
      "Epoch 26/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 2891.9843 - mae: 29.5598 - val_loss: 81.6527 - val_mae: 8.4364\n",
      "Epoch 27/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 6168.7668 - mae: 28.4306 - val_loss: 54.2540 - val_mae: 6.5934\n",
      "Epoch 28/500\n",
      "2290/2290 [==============================] - 2s 865us/step - loss: 7513.6445 - mae: 28.0071 - val_loss: 48.3261 - val_mae: 6.0958\n",
      "Epoch 29/500\n",
      "2290/2290 [==============================] - 2s 834us/step - loss: 3002.3866 - mae: 26.2518 - val_loss: 43.2744 - val_mae: 5.6842\n",
      "Epoch 30/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 10899.5539 - mae: 26.8871 - val_loss: 164.5490 - val_mae: 12.4728\n",
      "Epoch 31/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 17315.0821 - mae: 26.5274 - val_loss: 15.2771 - val_mae: 2.3771\n",
      "Epoch 32/500\n",
      "2290/2290 [==============================] - 2s 797us/step - loss: 6898.1609 - mae: 24.1401 - val_loss: 57.4246 - val_mae: 6.9487\n",
      "Epoch 33/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 6361.8262 - mae: 23.2316 - val_loss: 46.3147 - val_mae: 6.0269\n",
      "Epoch 34/500\n",
      "2290/2290 [==============================] - 2s 867us/step - loss: 31555.3382 - mae: 26.2744 - val_loss: 62.1689 - val_mae: 7.1268\n",
      "Epoch 35/500\n",
      "2290/2290 [==============================] - 2s 984us/step - loss: 4576.1266 - mae: 20.3879 - val_loss: 30.8135 - val_mae: 4.5267\n",
      "Epoch 36/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 5393.9965 - mae: 20.8085 - val_loss: 65.2135 - val_mae: 7.5229\n",
      "Epoch 37/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 1159.4259 - mae: 18.4599 - val_loss: 20.1603 - val_mae: 3.1203\n",
      "Epoch 38/500\n",
      "2290/2290 [==============================] - 2s 741us/step - loss: 5734.2540 - mae: 20.1635 - val_loss: 37.0937 - val_mae: 5.2124\n",
      "Epoch 39/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 12501.9598 - mae: 19.6142 - val_loss: 38.4770 - val_mae: 5.4804\n",
      "Epoch 40/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 772.9409 - mae: 15.9250 - val_loss: 24.8254 - val_mae: 4.0560\n",
      "Epoch 41/500\n",
      "2290/2290 [==============================] - 2s 797us/step - loss: 10513.0600 - mae: 17.9505 - val_loss: 41.0775 - val_mae: 5.7003\n",
      "Epoch 42/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 2793.4767 - mae: 16.4933 - val_loss: 14.3151 - val_mae: 1.8344\n",
      "Epoch 43/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 7681.8502 - mae: 16.9601 - val_loss: 31.9671 - val_mae: 4.5476\n",
      "Epoch 44/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 2173.9739 - mae: 15.4796 - val_loss: 15.2204 - val_mae: 1.9096\n",
      "Epoch 45/500\n",
      "2290/2290 [==============================] - 2s 804us/step - loss: 2693.1949 - mae: 15.0198 - val_loss: 30.8240 - val_mae: 4.5916\n",
      "Epoch 46/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 1276.1956 - mae: 12.9547 - val_loss: 14.9746 - val_mae: 1.8980\n",
      "Epoch 47/500\n",
      "2290/2290 [==============================] - 2s 803us/step - loss: 808.0263 - mae: 12.4338 - val_loss: 21.6396 - val_mae: 3.3595\n",
      "Epoch 48/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 1010.9749 - mae: 11.9137 - val_loss: 14.6012 - val_mae: 1.6864\n",
      "Epoch 49/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 1721.5026 - mae: 11.8602 - val_loss: 24.4392 - val_mae: 3.7158\n",
      "Epoch 50/500\n",
      "2290/2290 [==============================] - 2s 728us/step - loss: 7452.3497 - mae: 12.9028 - val_loss: 14.5742 - val_mae: 1.9902\n",
      "Epoch 51/500\n",
      "2290/2290 [==============================] - 2s 774us/step - loss: 3124.8485 - mae: 11.8789 - val_loss: 13.0986 - val_mae: 1.7038\n",
      "Epoch 52/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 1527.8755 - mae: 10.6310 - val_loss: 12.6498 - val_mae: 1.7892\n",
      "Epoch 53/500\n",
      "2290/2290 [==============================] - 2s 773us/step - loss: 1511.7956 - mae: 10.5084 - val_loss: 10.8848 - val_mae: 1.3213\n",
      "Epoch 54/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 1949.7446 - mae: 10.0860 - val_loss: 9.8471 - val_mae: 0.9534\n",
      "Epoch 55/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 823.3437 - mae: 9.2578 - val_loss: 18.0197 - val_mae: 3.3417\n",
      "Epoch 56/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 784.1273 - mae: 8.7714 - val_loss: 16.9813 - val_mae: 3.1821\n",
      "Epoch 57/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 259.2996 - mae: 8.3807 - val_loss: 9.8221 - val_mae: 0.9566\n",
      "Epoch 58/500\n",
      "2290/2290 [==============================] - 2s 824us/step - loss: 3428.6819 - mae: 9.9647 - val_loss: 9.8459 - val_mae: 0.7492\n",
      "Epoch 59/500\n",
      "2290/2290 [==============================] - 2s 816us/step - loss: 1238.2866 - mae: 8.4398 - val_loss: 9.8667 - val_mae: 0.7794\n",
      "Epoch 60/500\n",
      "2290/2290 [==============================] - 2s 877us/step - loss: 1857.9006 - mae: 8.9321 - val_loss: 11.0056 - val_mae: 1.2299\n",
      "Epoch 61/500\n",
      "2290/2290 [==============================] - 2s 836us/step - loss: 3229.9058 - mae: 8.7760 - val_loss: 10.1844 - val_mae: 1.3471\n",
      "Epoch 62/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 185.2904 - mae: 6.7535 - val_loss: 9.8117 - val_mae: 0.8822\n",
      "Epoch 63/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 3014.7172 - mae: 8.6778 - val_loss: 10.3504 - val_mae: 1.1741\n",
      "Epoch 64/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 1382.0161 - mae: 7.8588 - val_loss: 10.0253 - val_mae: 0.8432\n",
      "Epoch 65/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 2557.9628 - mae: 8.6197 - val_loss: 9.7970 - val_mae: 0.7244\n",
      "Epoch 66/500\n",
      "2290/2290 [==============================] - 2s 860us/step - loss: 221.4784 - mae: 6.4633 - val_loss: 9.2756 - val_mae: 0.5075\n",
      "Epoch 67/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 4587.0154 - mae: 7.9761 - val_loss: 142.0037 - val_mae: 11.8659\n",
      "Epoch 68/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 4655.0827 - mae: 9.6241 - val_loss: 15.8651 - val_mae: 3.0985\n",
      "Epoch 69/500\n",
      "2290/2290 [==============================] - 2s 795us/step - loss: 423.5765 - mae: 6.2859 - val_loss: 10.1830 - val_mae: 1.5300\n",
      "Epoch 70/500\n",
      "2290/2290 [==============================] - 2s 811us/step - loss: 134.4503 - mae: 5.1230 - val_loss: 9.4759 - val_mae: 0.9764\n",
      "Epoch 71/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 222.3522 - mae: 5.0647 - val_loss: 9.2412 - val_mae: 0.4627\n",
      "Epoch 72/500\n",
      "2290/2290 [==============================] - 2s 837us/step - loss: 899.0682 - mae: 6.0810 - val_loss: 13.3134 - val_mae: 2.4781\n",
      "Epoch 73/500\n",
      "2290/2290 [==============================] - 2s 890us/step - loss: 146.1611 - mae: 4.9845 - val_loss: 9.8384 - val_mae: 1.3140\n",
      "Epoch 74/500\n",
      "2290/2290 [==============================] - 2s 796us/step - loss: 148.7390 - mae: 4.5089 - val_loss: 11.6181 - val_mae: 2.0547\n",
      "Epoch 75/500\n",
      "2290/2290 [==============================] - 2s 846us/step - loss: 498.0893 - mae: 4.9576 - val_loss: 9.1447 - val_mae: 0.8625\n",
      "Epoch 76/500\n",
      "2290/2290 [==============================] - 2s 796us/step - loss: 227.3614 - mae: 4.3061 - val_loss: 13.3316 - val_mae: 2.5396\n",
      "Epoch 77/500\n",
      "2290/2290 [==============================] - 2s 840us/step - loss: 89.1522 - mae: 4.2086 - val_loss: 11.4756 - val_mae: 1.5362\n",
      "Epoch 78/500\n",
      "2290/2290 [==============================] - 2s 831us/step - loss: 773.2064 - mae: 5.1385 - val_loss: 10.5172 - val_mae: 1.1875\n",
      "Epoch 79/500\n",
      "2290/2290 [==============================] - 2s 833us/step - loss: 167.8377 - mae: 4.6602 - val_loss: 9.3243 - val_mae: 0.9934\n",
      "Epoch 80/500\n",
      "2290/2290 [==============================] - 2s 816us/step - loss: 149.6839 - mae: 3.9637 - val_loss: 9.1643 - val_mae: 0.3782\n",
      "Epoch 81/500\n",
      "2290/2290 [==============================] - 2s 849us/step - loss: 1404.6486 - mae: 4.3035 - val_loss: 10.3530 - val_mae: 1.6469\n",
      "Epoch 82/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 5523.1397 - mae: 8.3725 - val_loss: 9.4180 - val_mae: 0.6560\n",
      "Epoch 83/500\n",
      "2290/2290 [==============================] - 2s 870us/step - loss: 1865.1274 - mae: 5.5810 - val_loss: 8.9874 - val_mae: 0.7861\n",
      "Epoch 84/500\n",
      "2290/2290 [==============================] - 2s 773us/step - loss: 197.6292 - mae: 3.6249 - val_loss: 9.4507 - val_mae: 0.5847\n",
      "Epoch 85/500\n",
      "2290/2290 [==============================] - 2s 740us/step - loss: 68.9708 - mae: 3.3365 - val_loss: 9.3140 - val_mae: 0.5514\n",
      "Epoch 86/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 1665.2400 - mae: 4.5705 - val_loss: 9.3179 - val_mae: 0.7937\n",
      "Epoch 87/500\n",
      "2290/2290 [==============================] - 2s 731us/step - loss: 620.6287 - mae: 4.4425 - val_loss: 20.7662 - val_mae: 3.4099\n",
      "Epoch 88/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 223.6507 - mae: 3.8576 - val_loss: 9.5241 - val_mae: 1.1873\n",
      "Epoch 89/500\n",
      "2290/2290 [==============================] - 2s 760us/step - loss: 185.4204 - mae: 3.2360 - val_loss: 9.7905 - val_mae: 1.3864\n",
      "Epoch 90/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 142.0763 - mae: 3.0771 - val_loss: 9.4160 - val_mae: 1.0459\n",
      "Epoch 91/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 3427.0565 - mae: 5.3135 - val_loss: 9.0694 - val_mae: 0.5632\n",
      "Epoch 92/500\n",
      "2290/2290 [==============================] - 2s 790us/step - loss: 42.8898 - mae: 3.0208 - val_loss: 9.2250 - val_mae: 0.4015\n",
      "Epoch 93/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 167.7962 - mae: 3.1515 - val_loss: 9.6138 - val_mae: 0.7882\n",
      "Epoch 94/500\n",
      "2290/2290 [==============================] - 2s 705us/step - loss: 293.1636 - mae: 3.4792 - val_loss: 9.8972 - val_mae: 1.4208\n",
      "Epoch 95/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 91.3041 - mae: 2.5393 - val_loss: 9.1507 - val_mae: 0.5078\n",
      "Epoch 96/500\n",
      "2290/2290 [==============================] - 2s 853us/step - loss: 45.8373 - mae: 2.4629 - val_loss: 9.3074 - val_mae: 0.4751\n",
      "Epoch 97/500\n",
      "2290/2290 [==============================] - 2s 748us/step - loss: 1075.9412 - mae: 3.8799 - val_loss: 9.2573 - val_mae: 0.9798\n",
      "Epoch 98/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 225.8311 - mae: 2.8727 - val_loss: 9.0922 - val_mae: 0.3970\n",
      "Epoch 99/500\n",
      "2290/2290 [==============================] - 2s 808us/step - loss: 510.4074 - mae: 2.5988 - val_loss: 9.1987 - val_mae: 0.6483\n",
      "Epoch 100/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 804.1338 - mae: 4.2021 - val_loss: 9.1479 - val_mae: 0.4793\n",
      "Epoch 101/500\n",
      "2290/2290 [==============================] - 2s 762us/step - loss: 70.4568 - mae: 2.1775 - val_loss: 9.3572 - val_mae: 1.0196\n",
      "Epoch 102/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 292.3362 - mae: 2.5102 - val_loss: 11.3591 - val_mae: 1.5140\n",
      "Epoch 103/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 37.6800 - mae: 2.5534 - val_loss: 9.4369 - val_mae: 1.0824\n",
      "Epoch 104/500\n",
      "2290/2290 [==============================] - 2s 757us/step - loss: 31.5789 - mae: 2.0024 - val_loss: 9.3609 - val_mae: 1.0624\n",
      "Epoch 105/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 170.1962 - mae: 2.1876 - val_loss: 9.1380 - val_mae: 0.6543\n",
      "Epoch 106/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 219.7685 - mae: 2.2696 - val_loss: 9.2310 - val_mae: 0.3957\n",
      "Epoch 107/500\n",
      "2290/2290 [==============================] - 2s 771us/step - loss: 90.2911 - mae: 2.2942 - val_loss: 9.8786 - val_mae: 1.4173\n",
      "Epoch 108/500\n",
      "2290/2290 [==============================] - 2s 656us/step - loss: 87.6973 - mae: 1.9421 - val_loss: 9.1500 - val_mae: 0.8141\n",
      "Epoch 109/500\n",
      "2290/2290 [==============================] - 1s 618us/step - loss: 67.6209 - mae: 1.7915 - val_loss: 9.1607 - val_mae: 0.4918\n",
      "Epoch 110/500\n",
      "2290/2290 [==============================] - 2s 687us/step - loss: 52.2353 - mae: 1.7870 - val_loss: 9.2518 - val_mae: 0.7148\n",
      "Epoch 111/500\n",
      "2290/2290 [==============================] - 2s 674us/step - loss: 23.1297 - mae: 1.7448 - val_loss: 9.0870 - val_mae: 0.7206\n",
      "Epoch 112/500\n",
      "2290/2290 [==============================] - 2s 722us/step - loss: 914.7843 - mae: 2.8354 - val_loss: 9.2659 - val_mae: 0.5732\n",
      "Epoch 113/500\n",
      "2290/2290 [==============================] - 2s 701us/step - loss: 40.9391 - mae: 2.1860 - val_loss: 9.5211 - val_mae: 1.1403\n",
      "Epoch 114/500\n",
      "2290/2290 [==============================] - 2s 721us/step - loss: 27.6018 - mae: 1.6279 - val_loss: 9.1414 - val_mae: 0.3927\n",
      "Epoch 115/500\n",
      "2290/2290 [==============================] - 2s 743us/step - loss: 133.2568 - mae: 1.9898 - val_loss: 9.3617 - val_mae: 1.0184\n",
      "Epoch 116/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 167.2955 - mae: 2.3000 - val_loss: 9.0829 - val_mae: 0.4368\n",
      "Epoch 117/500\n",
      "2290/2290 [==============================] - 2s 838us/step - loss: 384.4974 - mae: 2.4097 - val_loss: 26.7835 - val_mae: 4.6230\n",
      "Epoch 118/500\n",
      "2290/2290 [==============================] - 2s 874us/step - loss: 42.8759 - mae: 3.0452 - val_loss: 9.0419 - val_mae: 0.4872\n",
      "Epoch 119/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 282.8039 - mae: 2.1533 - val_loss: 9.1377 - val_mae: 0.7768\n",
      "Epoch 120/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 20.9236 - mae: 1.4910 - val_loss: 9.1854 - val_mae: 0.8797\n",
      "Epoch 121/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 16.7819 - mae: 1.1944 - val_loss: 9.1846 - val_mae: 0.9146\n",
      "Epoch 122/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 28.6289 - mae: 1.4590 - val_loss: 9.1011 - val_mae: 0.7762\n",
      "Epoch 123/500\n",
      "2290/2290 [==============================] - 2s 832us/step - loss: 38.2553 - mae: 1.4861 - val_loss: 9.0825 - val_mae: 0.8144\n",
      "Epoch 124/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 21.2955 - mae: 1.3270 - val_loss: 9.0703 - val_mae: 0.7573\n",
      "Epoch 125/500\n",
      "2290/2290 [==============================] - 2s 856us/step - loss: 133.7137 - mae: 1.7502 - val_loss: 9.1992 - val_mae: 0.9294\n",
      "Epoch 126/500\n",
      "2290/2290 [==============================] - 2s 759us/step - loss: 18.8786 - mae: 1.3884 - val_loss: 9.2170 - val_mae: 0.9330\n",
      "Epoch 127/500\n",
      "2290/2290 [==============================] - 2s 772us/step - loss: 17.5029 - mae: 1.3355 - val_loss: 8.9878 - val_mae: 0.4829\n",
      "Epoch 128/500\n",
      "2290/2290 [==============================] - 2s 853us/step - loss: 27.4408 - mae: 1.3797 - val_loss: 9.7055 - val_mae: 1.2967\n",
      "Epoch 129/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 20.1133 - mae: 1.3170 - val_loss: 9.2723 - val_mae: 0.9549\n",
      "Epoch 130/500\n",
      "2290/2290 [==============================] - 2s 848us/step - loss: 28.9575 - mae: 1.3898 - val_loss: 9.0264 - val_mae: 0.4485\n",
      "Epoch 131/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 16.0571 - mae: 1.2496 - val_loss: 9.1442 - val_mae: 0.8722\n",
      "Epoch 132/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 84.8741 - mae: 1.7863 - val_loss: 9.3852 - val_mae: 1.1039\n",
      "Epoch 133/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 17.6146 - mae: 1.3083 - val_loss: 9.2088 - val_mae: 0.9385\n",
      "Epoch 134/500\n",
      "2290/2290 [==============================] - 2s 856us/step - loss: 24.2942 - mae: 1.2633 - val_loss: 9.0205 - val_mae: 0.6537\n",
      "Epoch 135/500\n",
      "2290/2290 [==============================] - 2s 881us/step - loss: 19.9745 - mae: 1.3271 - val_loss: 9.3129 - val_mae: 1.0588\n",
      "Epoch 136/500\n",
      "2290/2290 [==============================] - 2s 795us/step - loss: 30.3608 - mae: 1.3766 - val_loss: 9.3593 - val_mae: 1.0671\n",
      "Epoch 137/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 16.0159 - mae: 1.2184 - val_loss: 9.5647 - val_mae: 1.1886\n",
      "Epoch 138/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 19.8357 - mae: 1.1954 - val_loss: 9.7862 - val_mae: 1.3661\n",
      "Epoch 139/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 19.1179 - mae: 1.4288 - val_loss: 9.1305 - val_mae: 0.8504\n",
      "Epoch 140/500\n",
      "2290/2290 [==============================] - 2s 768us/step - loss: 19.5435 - mae: 1.2807 - val_loss: 9.0532 - val_mae: 0.7355\n",
      "Epoch 141/500\n",
      "2290/2290 [==============================] - 2s 845us/step - loss: 22.1809 - mae: 1.4844 - val_loss: 9.0160 - val_mae: 0.6829\n",
      "Epoch 142/500\n",
      "2290/2290 [==============================] - 2s 796us/step - loss: 97.9053 - mae: 1.6474 - val_loss: 9.7561 - val_mae: 1.3608\n",
      "Epoch 143/500\n",
      "2290/2290 [==============================] - 2s 841us/step - loss: 17.9444 - mae: 1.2785 - val_loss: 9.0701 - val_mae: 0.7256\n",
      "Epoch 144/500\n",
      "2290/2290 [==============================] - 2s 774us/step - loss: 15.2760 - mae: 1.1464 - val_loss: 9.0835 - val_mae: 0.8193\n",
      "Epoch 145/500\n",
      "2290/2290 [==============================] - 2s 843us/step - loss: 18.7331 - mae: 1.2949 - val_loss: 9.1550 - val_mae: 0.8952\n",
      "Epoch 146/500\n",
      "2290/2290 [==============================] - 2s 860us/step - loss: 28.3047 - mae: 1.2987 - val_loss: 9.4789 - val_mae: 1.1969\n",
      "Epoch 147/500\n",
      "2290/2290 [==============================] - 2s 875us/step - loss: 19.9174 - mae: 1.2953 - val_loss: 9.6857 - val_mae: 1.2284\n",
      "Epoch 148/500\n",
      "2290/2290 [==============================] - 2s 834us/step - loss: 16.7623 - mae: 1.1354 - val_loss: 9.1564 - val_mae: 0.8655\n",
      "Epoch 149/500\n",
      "2290/2290 [==============================] - 2s 770us/step - loss: 15.4612 - mae: 1.1439 - val_loss: 9.2205 - val_mae: 0.9493\n",
      "Epoch 150/500\n",
      "2290/2290 [==============================] - 2s 837us/step - loss: 201.1004 - mae: 2.0795 - val_loss: 9.7212 - val_mae: 1.2866\n",
      "Epoch 151/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 15.2090 - mae: 1.2822 - val_loss: 9.3646 - val_mae: 1.0817\n",
      "Epoch 152/500\n",
      "2290/2290 [==============================] - 2s 746us/step - loss: 16.1094 - mae: 1.1200 - val_loss: 8.9952 - val_mae: 0.6192\n",
      "Epoch 153/500\n",
      "2290/2290 [==============================] - 2s 705us/step - loss: 16.3823 - mae: 1.2316 - val_loss: 9.1831 - val_mae: 0.8998\n",
      "Epoch 154/500\n",
      "2290/2290 [==============================] - 2s 764us/step - loss: 204.5937 - mae: 1.8958 - val_loss: 9.1369 - val_mae: 0.8410\n",
      "Epoch 155/500\n",
      "2290/2290 [==============================] - 2s 803us/step - loss: 15.7235 - mae: 1.1429 - val_loss: 9.3287 - val_mae: 1.0529\n",
      "Epoch 156/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 19.5115 - mae: 1.2204 - val_loss: 9.3894 - val_mae: 1.0789\n",
      "Epoch 157/500\n",
      "2290/2290 [==============================] - 2s 776us/step - loss: 15.4476 - mae: 1.1662 - val_loss: 9.2618 - val_mae: 0.9890\n",
      "Epoch 158/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 13.9138 - mae: 1.0148 - val_loss: 9.3159 - val_mae: 1.0412\n",
      "Epoch 159/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 25.3198 - mae: 1.2095 - val_loss: 8.9277 - val_mae: 0.6030\n",
      "Epoch 160/500\n",
      "2290/2290 [==============================] - 2s 841us/step - loss: 14.5590 - mae: 1.2212 - val_loss: 9.2345 - val_mae: 0.9600\n",
      "Epoch 161/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 14.7657 - mae: 1.2257 - val_loss: 9.2305 - val_mae: 0.9369\n",
      "Epoch 162/500\n",
      "2290/2290 [==============================] - 2s 844us/step - loss: 14.4481 - mae: 1.0836 - val_loss: 9.3831 - val_mae: 1.0731\n",
      "Epoch 163/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 18.1309 - mae: 1.2193 - val_loss: 9.2978 - val_mae: 0.9748\n",
      "Epoch 164/500\n",
      "2290/2290 [==============================] - 2s 810us/step - loss: 298.2911 - mae: 2.0916 - val_loss: 9.2122 - val_mae: 0.9502\n",
      "Epoch 165/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 21.1793 - mae: 1.2781 - val_loss: 9.2021 - val_mae: 0.9413\n",
      "Epoch 166/500\n",
      "2290/2290 [==============================] - 2s 804us/step - loss: 102.0724 - mae: 1.5622 - val_loss: 9.1725 - val_mae: 0.9090\n",
      "Epoch 167/500\n",
      "2290/2290 [==============================] - 2s 826us/step - loss: 38.0837 - mae: 1.2888 - val_loss: 9.1659 - val_mae: 0.9008\n",
      "Epoch 168/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 14.7296 - mae: 1.0323 - val_loss: 9.1586 - val_mae: 0.8918\n",
      "Epoch 169/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 15.4086 - mae: 1.1125 - val_loss: 9.1583 - val_mae: 0.8915\n",
      "Epoch 170/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 15.6023 - mae: 0.9861 - val_loss: 9.1522 - val_mae: 0.8838\n",
      "Epoch 171/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 14.7174 - mae: 1.0860 - val_loss: 9.1427 - val_mae: 0.8713\n",
      "Epoch 172/500\n",
      "2290/2290 [==============================] - 2s 882us/step - loss: 18.4151 - mae: 1.0524 - val_loss: 9.1351 - val_mae: 0.8611\n",
      "Epoch 173/500\n",
      "2290/2290 [==============================] - 2s 840us/step - loss: 15.9316 - mae: 1.0841 - val_loss: 9.1290 - val_mae: 0.8523\n",
      "Epoch 174/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 13.6936 - mae: 0.9109 - val_loss: 9.1254 - val_mae: 0.8466\n",
      "Epoch 175/500\n",
      "2290/2290 [==============================] - 2s 830us/step - loss: 15.3625 - mae: 0.9534 - val_loss: 9.1229 - val_mae: 0.8429\n",
      "Epoch 176/500\n",
      "2290/2290 [==============================] - 2s 843us/step - loss: 18.2068 - mae: 1.0938 - val_loss: 9.1133 - val_mae: 0.8295\n",
      "Epoch 177/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 80.3050 - mae: 1.2440 - val_loss: 9.1153 - val_mae: 0.8324\n",
      "Epoch 178/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 131.0899 - mae: 1.4114 - val_loss: 9.1191 - val_mae: 0.8382\n",
      "Epoch 179/500\n",
      "2290/2290 [==============================] - 2s 840us/step - loss: 15.6606 - mae: 1.2529 - val_loss: 9.1123 - val_mae: 0.8280\n",
      "Epoch 180/500\n",
      "2290/2290 [==============================] - 2s 720us/step - loss: 14.1617 - mae: 0.9778 - val_loss: 9.1066 - val_mae: 0.8192\n",
      "Epoch 181/500\n",
      "2290/2290 [==============================] - 2s 692us/step - loss: 13.7716 - mae: 0.9457 - val_loss: 9.1015 - val_mae: 0.8111\n",
      "Epoch 182/500\n",
      "2290/2290 [==============================] - 2s 716us/step - loss: 14.0612 - mae: 0.9643 - val_loss: 9.0965 - val_mae: 0.8030\n",
      "Epoch 183/500\n",
      "2290/2290 [==============================] - 2s 852us/step - loss: 46.8738 - mae: 1.1635 - val_loss: 9.0799 - val_mae: 0.7740\n",
      "Epoch 184/500\n",
      "2290/2290 [==============================] - 2s 770us/step - loss: 14.1755 - mae: 0.9485 - val_loss: 9.0762 - val_mae: 0.7671\n",
      "Epoch 185/500\n",
      "2290/2290 [==============================] - 2s 721us/step - loss: 48.3723 - mae: 1.0947 - val_loss: 9.0720 - val_mae: 0.7590\n",
      "Epoch 186/500\n",
      "2290/2290 [==============================] - 2s 747us/step - loss: 18.3283 - mae: 1.3814 - val_loss: 9.0718 - val_mae: 0.7586\n",
      "Epoch 187/500\n",
      "2290/2290 [==============================] - 2s 770us/step - loss: 15.0545 - mae: 1.0909 - val_loss: 9.0672 - val_mae: 0.7495\n",
      "Epoch 188/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 23.1906 - mae: 1.0354 - val_loss: 9.0635 - val_mae: 0.7417\n",
      "Epoch 189/500\n",
      "2290/2290 [==============================] - 2s 730us/step - loss: 13.8525 - mae: 0.9453 - val_loss: 9.0599 - val_mae: 0.7342\n",
      "Epoch 190/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 14.1465 - mae: 0.9171 - val_loss: 9.0551 - val_mae: 0.7233\n",
      "Epoch 191/500\n",
      "2290/2290 [==============================] - 2s 732us/step - loss: 13.7386 - mae: 0.9245 - val_loss: 9.0507 - val_mae: 0.7130\n",
      "Epoch 192/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 13.7027 - mae: 0.8336 - val_loss: 9.0492 - val_mae: 0.7093\n",
      "Epoch 193/500\n",
      "2290/2290 [==============================] - 2s 819us/step - loss: 13.9323 - mae: 0.8622 - val_loss: 9.0476 - val_mae: 0.7052\n",
      "Epoch 194/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 13.7070 - mae: 0.8245 - val_loss: 9.0464 - val_mae: 0.7023\n",
      "Epoch 195/500\n",
      "2290/2290 [==============================] - 2s 760us/step - loss: 19.1115 - mae: 0.8763 - val_loss: 9.0479 - val_mae: 0.7061\n",
      "Epoch 196/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 13.6488 - mae: 0.8273 - val_loss: 9.0480 - val_mae: 0.7063\n",
      "Epoch 197/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 13.6091 - mae: 0.8068 - val_loss: 9.0468 - val_mae: 0.7033\n",
      "Epoch 198/500\n",
      "2290/2290 [==============================] - 2s 738us/step - loss: 13.6305 - mae: 0.8567 - val_loss: 9.0447 - val_mae: 0.6980\n",
      "Epoch 199/500\n",
      "2290/2290 [==============================] - 2s 742us/step - loss: 13.6972 - mae: 0.8729 - val_loss: 9.0418 - val_mae: 0.6903\n",
      "Epoch 200/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 13.9849 - mae: 0.8287 - val_loss: 9.0408 - val_mae: 0.6873\n",
      "Epoch 201/500\n",
      "2290/2290 [==============================] - 2s 830us/step - loss: 13.7681 - mae: 0.8558 - val_loss: 9.0384 - val_mae: 0.6806\n",
      "Epoch 202/500\n",
      "2290/2290 [==============================] - 2s 838us/step - loss: 13.5982 - mae: 0.7959 - val_loss: 9.0382 - val_mae: 0.6801\n",
      "Epoch 203/500\n",
      "2290/2290 [==============================] - 2s 755us/step - loss: 14.1606 - mae: 0.8726 - val_loss: 9.0346 - val_mae: 0.6693\n",
      "Epoch 204/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 14.2164 - mae: 0.8201 - val_loss: 9.0354 - val_mae: 0.6718\n",
      "Epoch 205/500\n",
      "2290/2290 [==============================] - 2s 858us/step - loss: 13.5288 - mae: 0.7815 - val_loss: 9.0354 - val_mae: 0.6715\n",
      "Epoch 206/500\n",
      "2290/2290 [==============================] - 2s 892us/step - loss: 13.6439 - mae: 0.7779 - val_loss: 9.0366 - val_mae: 0.6752\n",
      "Epoch 207/500\n",
      "2290/2290 [==============================] - 2s 883us/step - loss: 29.5496 - mae: 0.8850 - val_loss: 9.0326 - val_mae: 0.6627\n",
      "Epoch 208/500\n",
      "2290/2290 [==============================] - 2s 879us/step - loss: 14.2204 - mae: 0.8053 - val_loss: 9.0326 - val_mae: 0.6629\n",
      "Epoch 209/500\n",
      "2290/2290 [==============================] - 2s 929us/step - loss: 13.8070 - mae: 0.7808 - val_loss: 9.0336 - val_mae: 0.6660\n",
      "Epoch 210/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 13.6399 - mae: 0.8198 - val_loss: 9.0339 - val_mae: 0.6671\n",
      "Epoch 211/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 13.5037 - mae: 0.7787 - val_loss: 9.0328 - val_mae: 0.6633\n",
      "Epoch 212/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 17.8741 - mae: 0.8147 - val_loss: 9.0367 - val_mae: 0.6755\n",
      "Epoch 213/500\n",
      "2290/2290 [==============================] - 2s 816us/step - loss: 13.6849 - mae: 0.8244 - val_loss: 9.0345 - val_mae: 0.6690\n",
      "Epoch 214/500\n",
      "2290/2290 [==============================] - 2s 836us/step - loss: 13.8058 - mae: 0.8188 - val_loss: 9.0345 - val_mae: 0.6689\n",
      "Epoch 215/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 18.4385 - mae: 0.8360 - val_loss: 9.0300 - val_mae: 0.6542\n",
      "Epoch 216/500\n",
      "2290/2290 [==============================] - 2s 748us/step - loss: 14.3699 - mae: 0.8355 - val_loss: 9.0328 - val_mae: 0.6634\n",
      "Epoch 217/500\n",
      "2290/2290 [==============================] - 2s 767us/step - loss: 13.4999 - mae: 0.7569 - val_loss: 9.0317 - val_mae: 0.6599\n",
      "Epoch 218/500\n",
      "2290/2290 [==============================] - 2s 866us/step - loss: 15.0263 - mae: 0.8545 - val_loss: 9.0332 - val_mae: 0.6646\n",
      "Epoch 219/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 13.5714 - mae: 0.7658 - val_loss: 9.0328 - val_mae: 0.6635\n",
      "Epoch 220/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 14.0423 - mae: 0.8283 - val_loss: 9.0305 - val_mae: 0.6559\n",
      "Epoch 221/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 13.5185 - mae: 0.7817 - val_loss: 9.0303 - val_mae: 0.6551\n",
      "Epoch 222/500\n",
      "2290/2290 [==============================] - 2s 873us/step - loss: 13.6680 - mae: 0.7969 - val_loss: 9.0282 - val_mae: 0.6478\n",
      "Epoch 223/500\n",
      "2290/2290 [==============================] - 2s 844us/step - loss: 48.2334 - mae: 0.9038 - val_loss: 9.0343 - val_mae: 0.6684\n",
      "Epoch 224/500\n",
      "2290/2290 [==============================] - 2s 935us/step - loss: 13.5234 - mae: 0.7969 - val_loss: 9.0358 - val_mae: 0.6729\n",
      "Epoch 225/500\n",
      "2290/2290 [==============================] - 2s 878us/step - loss: 17.2871 - mae: 0.8234 - val_loss: 9.0376 - val_mae: 0.6782\n",
      "Epoch 226/500\n",
      "2290/2290 [==============================] - 2s 808us/step - loss: 13.5575 - mae: 0.7990 - val_loss: 9.0364 - val_mae: 0.6746\n",
      "Epoch 227/500\n",
      "2290/2290 [==============================] - 2s 813us/step - loss: 13.5161 - mae: 0.7855 - val_loss: 9.0348 - val_mae: 0.6699\n",
      "Epoch 228/500\n",
      "2290/2290 [==============================] - 2s 831us/step - loss: 13.6628 - mae: 0.8039 - val_loss: 9.0329 - val_mae: 0.6637\n",
      "Epoch 229/500\n",
      "2290/2290 [==============================] - 2s 852us/step - loss: 13.5499 - mae: 0.7893 - val_loss: 9.0326 - val_mae: 0.6628\n",
      "Epoch 230/500\n",
      "2290/2290 [==============================] - 2s 832us/step - loss: 52.7075 - mae: 0.9993 - val_loss: 9.0294 - val_mae: 0.6521\n",
      "Epoch 231/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 13.5307 - mae: 0.7731 - val_loss: 9.0289 - val_mae: 0.6503\n",
      "Epoch 232/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 13.5474 - mae: 0.7765 - val_loss: 9.0286 - val_mae: 0.6492\n",
      "Epoch 233/500\n",
      "2290/2290 [==============================] - 2s 874us/step - loss: 13.5193 - mae: 0.7718 - val_loss: 9.0288 - val_mae: 0.6499\n",
      "Epoch 234/500\n",
      "2290/2290 [==============================] - 2s 796us/step - loss: 13.6057 - mae: 0.8038 - val_loss: 9.0289 - val_mae: 0.6503\n",
      "Epoch 235/500\n",
      "2290/2290 [==============================] - 2s 876us/step - loss: 78.6604 - mae: 1.0090 - val_loss: 9.0318 - val_mae: 0.6604\n",
      "Epoch 236/500\n",
      "2290/2290 [==============================] - 2s 853us/step - loss: 13.5834 - mae: 0.8098 - val_loss: 9.0410 - val_mae: 0.6879\n",
      "Epoch 237/500\n",
      "2290/2290 [==============================] - 2s 832us/step - loss: 58.6100 - mae: 0.9409 - val_loss: 9.0371 - val_mae: 0.6768\n",
      "Epoch 238/500\n",
      "2290/2290 [==============================] - 2s 784us/step - loss: 13.5316 - mae: 0.8035 - val_loss: 9.0342 - val_mae: 0.6680\n",
      "Epoch 239/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 13.5338 - mae: 0.8184 - val_loss: 9.0328 - val_mae: 0.6636\n",
      "Epoch 240/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 13.5568 - mae: 0.7972 - val_loss: 9.0312 - val_mae: 0.6583\n",
      "Epoch 241/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 13.6165 - mae: 0.8176 - val_loss: 9.0290 - val_mae: 0.6506\n",
      "Epoch 242/500\n",
      "2290/2290 [==============================] - 2s 850us/step - loss: 13.5130 - mae: 0.8054 - val_loss: 9.0273 - val_mae: 0.6445\n",
      "Epoch 243/500\n",
      "2290/2290 [==============================] - 2s 830us/step - loss: 42.4000 - mae: 0.9281 - val_loss: 9.0344 - val_mae: 0.6685\n",
      "Epoch 244/500\n",
      "2290/2290 [==============================] - 2s 803us/step - loss: 13.4447 - mae: 0.7743 - val_loss: 9.0325 - val_mae: 0.6626\n",
      "Epoch 245/500\n",
      "2290/2290 [==============================] - 2s 827us/step - loss: 13.5019 - mae: 0.7841 - val_loss: 9.0325 - val_mae: 0.6625\n",
      "Epoch 246/500\n",
      "2290/2290 [==============================] - 2s 785us/step - loss: 13.5056 - mae: 0.7833 - val_loss: 9.0306 - val_mae: 0.6563\n",
      "Epoch 247/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 13.5460 - mae: 0.7910 - val_loss: 9.0298 - val_mae: 0.6534\n",
      "Epoch 248/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 16.2827 - mae: 0.8115 - val_loss: 9.0312 - val_mae: 0.6582\n",
      "Epoch 249/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 13.5408 - mae: 0.7833 - val_loss: 9.0297 - val_mae: 0.6531\n",
      "Epoch 250/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 15.8012 - mae: 0.8038 - val_loss: 9.0306 - val_mae: 0.6563\n",
      "Epoch 251/500\n",
      "2290/2290 [==============================] - 2s 713us/step - loss: 13.4669 - mae: 0.7716 - val_loss: 9.0303 - val_mae: 0.6552\n",
      "Epoch 252/500\n",
      "2290/2290 [==============================] - 2s 734us/step - loss: 13.5133 - mae: 0.7783 - val_loss: 9.0304 - val_mae: 0.6554\n",
      "Epoch 253/500\n",
      "2290/2290 [==============================] - 2s 713us/step - loss: 13.4992 - mae: 0.7726 - val_loss: 9.0297 - val_mae: 0.6533\n",
      "Epoch 254/500\n",
      "2290/2290 [==============================] - 2s 710us/step - loss: 13.4420 - mae: 0.7678 - val_loss: 9.0290 - val_mae: 0.6508\n",
      "Epoch 255/500\n",
      "2290/2290 [==============================] - 2s 713us/step - loss: 14.4126 - mae: 0.7951 - val_loss: 9.0305 - val_mae: 0.6558\n",
      "Epoch 256/500\n",
      "2290/2290 [==============================] - 2s 704us/step - loss: 13.5027 - mae: 0.7856 - val_loss: 9.0300 - val_mae: 0.6541\n",
      "Epoch 257/500\n",
      "2290/2290 [==============================] - 2s 766us/step - loss: 13.4878 - mae: 0.7770 - val_loss: 9.0283 - val_mae: 0.6480\n",
      "Epoch 258/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 13.5511 - mae: 0.7841 - val_loss: 9.0274 - val_mae: 0.6449\n",
      "Epoch 259/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 13.5175 - mae: 0.7875 - val_loss: 9.0266 - val_mae: 0.6418\n",
      "Epoch 260/500\n",
      "2290/2290 [==============================] - 2s 772us/step - loss: 13.5915 - mae: 0.7796 - val_loss: 9.0249 - val_mae: 0.6352\n",
      "Epoch 261/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 13.4815 - mae: 0.7634 - val_loss: 9.0252 - val_mae: 0.6364\n",
      "Epoch 262/500\n",
      "2290/2290 [==============================] - 2s 844us/step - loss: 13.4471 - mae: 0.7508 - val_loss: 9.0254 - val_mae: 0.6373\n",
      "Epoch 263/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 13.4568 - mae: 0.7538 - val_loss: 9.0249 - val_mae: 0.6350\n",
      "Epoch 264/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 13.4537 - mae: 0.7442 - val_loss: 9.0243 - val_mae: 0.6329\n",
      "Epoch 265/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 13.4485 - mae: 0.7528 - val_loss: 9.0255 - val_mae: 0.6375\n",
      "Epoch 266/500\n",
      "2290/2290 [==============================] - 2s 871us/step - loss: 13.4758 - mae: 0.7567 - val_loss: 9.0255 - val_mae: 0.6374\n",
      "Epoch 267/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 13.4950 - mae: 0.7478 - val_loss: 9.0247 - val_mae: 0.6346\n",
      "Epoch 268/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 13.8404 - mae: 0.7674 - val_loss: 9.0248 - val_mae: 0.6348\n",
      "Epoch 269/500\n",
      "2290/2290 [==============================] - 2s 908us/step - loss: 18.0548 - mae: 0.7899 - val_loss: 9.0237 - val_mae: 0.6302\n",
      "Epoch 270/500\n",
      "2290/2290 [==============================] - 2s 797us/step - loss: 13.4579 - mae: 0.7519 - val_loss: 9.0245 - val_mae: 0.6336\n",
      "Epoch 271/500\n",
      "2290/2290 [==============================] - 2s 872us/step - loss: 13.4640 - mae: 0.7472 - val_loss: 9.0247 - val_mae: 0.6344\n",
      "Epoch 272/500\n",
      "2290/2290 [==============================] - 2s 803us/step - loss: 13.4616 - mae: 0.7481 - val_loss: 9.0246 - val_mae: 0.6342\n",
      "Epoch 273/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 13.4458 - mae: 0.7488 - val_loss: 9.0256 - val_mae: 0.6378\n",
      "Epoch 274/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 13.8363 - mae: 0.7813 - val_loss: 9.0246 - val_mae: 0.6341\n",
      "Epoch 275/500\n",
      "2290/2290 [==============================] - 2s 861us/step - loss: 13.4609 - mae: 0.7574 - val_loss: 9.0252 - val_mae: 0.6365\n",
      "Epoch 276/500\n",
      "2290/2290 [==============================] - 2s 836us/step - loss: 13.4563 - mae: 0.7541 - val_loss: 9.0257 - val_mae: 0.6384\n",
      "Epoch 277/500\n",
      "2290/2290 [==============================] - 2s 862us/step - loss: 13.4575 - mae: 0.7534 - val_loss: 9.0251 - val_mae: 0.6358\n",
      "Epoch 278/500\n",
      "2290/2290 [==============================] - 2s 827us/step - loss: 13.4492 - mae: 0.7532 - val_loss: 9.0244 - val_mae: 0.6330\n",
      "Epoch 279/500\n",
      "2290/2290 [==============================] - 2s 862us/step - loss: 13.4640 - mae: 0.7451 - val_loss: 9.0243 - val_mae: 0.6327\n",
      "Epoch 280/500\n",
      "2290/2290 [==============================] - 2s 878us/step - loss: 13.4470 - mae: 0.7530 - val_loss: 9.0256 - val_mae: 0.6380\n",
      "Epoch 281/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 13.4436 - mae: 0.7546 - val_loss: 9.0251 - val_mae: 0.6360\n",
      "Epoch 282/500\n",
      "2290/2290 [==============================] - 2s 776us/step - loss: 13.5039 - mae: 0.7657 - val_loss: 9.0255 - val_mae: 0.6377\n",
      "Epoch 283/500\n",
      "2290/2290 [==============================] - 2s 774us/step - loss: 13.4492 - mae: 0.7528 - val_loss: 9.0245 - val_mae: 0.6335\n",
      "Epoch 284/500\n",
      "2290/2290 [==============================] - 2s 737us/step - loss: 13.4611 - mae: 0.7492 - val_loss: 9.0243 - val_mae: 0.6326\n",
      "Epoch 285/500\n",
      "2290/2290 [==============================] - 2s 774us/step - loss: 13.4234 - mae: 0.7469 - val_loss: 9.0246 - val_mae: 0.6338\n",
      "Epoch 286/500\n",
      "2290/2290 [==============================] - 2s 740us/step - loss: 13.4637 - mae: 0.7558 - val_loss: 9.0242 - val_mae: 0.6323\n",
      "Epoch 287/500\n",
      "2290/2290 [==============================] - 2s 766us/step - loss: 13.4497 - mae: 0.7490 - val_loss: 9.0250 - val_mae: 0.6356\n",
      "Epoch 288/500\n",
      "2290/2290 [==============================] - 2s 763us/step - loss: 13.4997 - mae: 0.7513 - val_loss: 9.0247 - val_mae: 0.6344\n",
      "Epoch 289/500\n",
      "2290/2290 [==============================] - 2s 748us/step - loss: 13.4596 - mae: 0.7534 - val_loss: 9.0251 - val_mae: 0.6361\n",
      "Epoch 290/500\n",
      "2290/2290 [==============================] - 2s 725us/step - loss: 13.4458 - mae: 0.7481 - val_loss: 9.0244 - val_mae: 0.6331\n",
      "Epoch 291/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 13.4539 - mae: 0.7433 - val_loss: 9.0241 - val_mae: 0.6319\n",
      "Epoch 292/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 16.6670 - mae: 0.8027 - val_loss: 9.0282 - val_mae: 0.6477\n",
      "Epoch 293/500\n",
      "2290/2290 [==============================] - 2s 842us/step - loss: 29.9660 - mae: 0.8559 - val_loss: 9.0326 - val_mae: 0.6630\n",
      "Epoch 294/500\n",
      "2290/2290 [==============================] - 2s 720us/step - loss: 13.4653 - mae: 0.7873 - val_loss: 9.0324 - val_mae: 0.6623\n",
      "Epoch 295/500\n",
      "2290/2290 [==============================] - 2s 825us/step - loss: 13.5223 - mae: 0.7851 - val_loss: 9.0316 - val_mae: 0.6595\n",
      "Epoch 296/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 13.4584 - mae: 0.7778 - val_loss: 9.0312 - val_mae: 0.6584\n",
      "Epoch 297/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 13.4627 - mae: 0.7734 - val_loss: 9.0299 - val_mae: 0.6537\n",
      "Epoch 298/500\n",
      "2290/2290 [==============================] - 2s 868us/step - loss: 13.5933 - mae: 0.7774 - val_loss: 9.0293 - val_mae: 0.6516\n",
      "Epoch 299/500\n",
      "2290/2290 [==============================] - 2s 884us/step - loss: 14.0940 - mae: 0.7890 - val_loss: 9.0299 - val_mae: 0.6539\n",
      "Epoch 300/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 13.4474 - mae: 0.7724 - val_loss: 9.0288 - val_mae: 0.6498\n",
      "Epoch 301/500\n",
      "2290/2290 [==============================] - 2s 790us/step - loss: 13.4468 - mae: 0.7683 - val_loss: 9.0285 - val_mae: 0.6487\n",
      "Epoch 302/500\n",
      "2290/2290 [==============================] - 2s 762us/step - loss: 13.4558 - mae: 0.7663 - val_loss: 9.0268 - val_mae: 0.6427\n",
      "Epoch 303/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 13.4546 - mae: 0.7553 - val_loss: 9.0260 - val_mae: 0.6394\n",
      "Epoch 304/500\n",
      "2290/2290 [==============================] - 2s 841us/step - loss: 13.4430 - mae: 0.7569 - val_loss: 9.0254 - val_mae: 0.6371\n",
      "Epoch 305/500\n",
      "2290/2290 [==============================] - 2s 831us/step - loss: 13.4393 - mae: 0.7560 - val_loss: 9.0262 - val_mae: 0.6404\n",
      "Epoch 306/500\n",
      "2290/2290 [==============================] - 2s 819us/step - loss: 13.4436 - mae: 0.7570 - val_loss: 9.0261 - val_mae: 0.6399\n",
      "Epoch 307/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 13.4459 - mae: 0.7598 - val_loss: 9.0260 - val_mae: 0.6396\n",
      "Epoch 308/500\n",
      "2290/2290 [==============================] - 2s 795us/step - loss: 13.5433 - mae: 0.7627 - val_loss: 9.0253 - val_mae: 0.6369\n",
      "Epoch 309/500\n",
      "2290/2290 [==============================] - 2s 848us/step - loss: 13.8463 - mae: 0.7725 - val_loss: 9.0261 - val_mae: 0.6400\n",
      "Epoch 310/500\n",
      "2290/2290 [==============================] - 2s 828us/step - loss: 13.4419 - mae: 0.7562 - val_loss: 9.0264 - val_mae: 0.6412\n",
      "Epoch 311/500\n",
      "2290/2290 [==============================] - 2s 889us/step - loss: 13.4429 - mae: 0.7569 - val_loss: 9.0258 - val_mae: 0.6387\n",
      "Epoch 312/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 13.4412 - mae: 0.7523 - val_loss: 9.0251 - val_mae: 0.6359\n",
      "Epoch 313/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 13.4415 - mae: 0.7510 - val_loss: 9.0249 - val_mae: 0.6354\n",
      "Epoch 314/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 13.4418 - mae: 0.7572 - val_loss: 9.0255 - val_mae: 0.6377\n",
      "Epoch 315/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 13.4831 - mae: 0.7566 - val_loss: 9.0256 - val_mae: 0.6379\n",
      "Epoch 316/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 13.4559 - mae: 0.7598 - val_loss: 9.0249 - val_mae: 0.6354\n",
      "Epoch 317/500\n",
      "2290/2290 [==============================] - 2s 860us/step - loss: 13.4448 - mae: 0.7560 - val_loss: 9.0255 - val_mae: 0.6375\n",
      "Epoch 318/500\n",
      "2290/2290 [==============================] - 2s 830us/step - loss: 13.4435 - mae: 0.7561 - val_loss: 9.0257 - val_mae: 0.6383\n",
      "Epoch 319/500\n",
      "2290/2290 [==============================] - 2s 794us/step - loss: 13.4435 - mae: 0.7560 - val_loss: 9.0252 - val_mae: 0.6364\n",
      "Epoch 320/500\n",
      "2290/2290 [==============================] - 2s 754us/step - loss: 13.4435 - mae: 0.7619 - val_loss: 9.0254 - val_mae: 0.6370\n",
      "Epoch 321/500\n",
      "2290/2290 [==============================] - 2s 848us/step - loss: 13.4423 - mae: 0.7551 - val_loss: 9.0257 - val_mae: 0.6385\n",
      "Epoch 322/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 13.4453 - mae: 0.7560 - val_loss: 9.0253 - val_mae: 0.6367\n",
      "Epoch 323/500\n",
      "2290/2290 [==============================] - 2s 740us/step - loss: 13.4423 - mae: 0.7499 - val_loss: 9.0245 - val_mae: 0.6336\n",
      "Epoch 324/500\n",
      "2290/2290 [==============================] - 2s 813us/step - loss: 13.4454 - mae: 0.7561 - val_loss: 9.0244 - val_mae: 0.6331\n",
      "Epoch 325/500\n",
      "2290/2290 [==============================] - 2s 827us/step - loss: 13.4423 - mae: 0.7527 - val_loss: 9.0244 - val_mae: 0.6331\n",
      "Epoch 326/500\n",
      "2290/2290 [==============================] - 2s 819us/step - loss: 13.4446 - mae: 0.7547 - val_loss: 9.0250 - val_mae: 0.6355\n",
      "Epoch 327/500\n",
      "2290/2290 [==============================] - 2s 848us/step - loss: 13.4431 - mae: 0.7508 - val_loss: 9.0236 - val_mae: 0.6297\n",
      "Epoch 328/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 13.4516 - mae: 0.7561 - val_loss: 9.0244 - val_mae: 0.6333\n",
      "Epoch 329/500\n",
      "2290/2290 [==============================] - 2s 765us/step - loss: 13.6981 - mae: 0.7630 - val_loss: 9.0250 - val_mae: 0.6356\n",
      "Epoch 330/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 13.4444 - mae: 0.7560 - val_loss: 9.0259 - val_mae: 0.6391\n",
      "Epoch 331/500\n",
      "2290/2290 [==============================] - 2s 770us/step - loss: 13.4426 - mae: 0.7508 - val_loss: 9.0250 - val_mae: 0.6356\n",
      "Epoch 332/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 13.4430 - mae: 0.7489 - val_loss: 9.0247 - val_mae: 0.6344\n",
      "Epoch 333/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 13.4433 - mae: 0.7565 - val_loss: 9.0246 - val_mae: 0.6341\n",
      "Epoch 334/500\n",
      "2290/2290 [==============================] - 2s 923us/step - loss: 13.4508 - mae: 0.7513 - val_loss: 9.0235 - val_mae: 0.6294\n",
      "Epoch 335/500\n",
      "2290/2290 [==============================] - 2s 871us/step - loss: 13.4433 - mae: 0.7595 - val_loss: 9.0257 - val_mae: 0.6385\n",
      "Epoch 336/500\n",
      "2290/2290 [==============================] - 2s 855us/step - loss: 13.4492 - mae: 0.7525 - val_loss: 9.0250 - val_mae: 0.6355\n",
      "Epoch 337/500\n",
      "2290/2290 [==============================] - 2s 838us/step - loss: 13.4445 - mae: 0.7501 - val_loss: 9.0239 - val_mae: 0.6313\n",
      "Epoch 338/500\n",
      "2290/2290 [==============================] - 2s 748us/step - loss: 13.4447 - mae: 0.7573 - val_loss: 9.0257 - val_mae: 0.6385\n",
      "Epoch 339/500\n",
      "2290/2290 [==============================] - 2s 875us/step - loss: 13.4429 - mae: 0.7541 - val_loss: 9.0249 - val_mae: 0.6350\n",
      "Epoch 340/500\n",
      "2290/2290 [==============================] - 2s 862us/step - loss: 13.4442 - mae: 0.7467 - val_loss: 9.0238 - val_mae: 0.6306\n",
      "Epoch 341/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 13.4481 - mae: 0.7574 - val_loss: 9.0255 - val_mae: 0.6375\n",
      "Epoch 342/500\n",
      "2290/2290 [==============================] - 2s 680us/step - loss: 13.4425 - mae: 0.7491 - val_loss: 9.0243 - val_mae: 0.6327\n",
      "Epoch 343/500\n",
      "2290/2290 [==============================] - 2s 659us/step - loss: 13.4429 - mae: 0.7572 - val_loss: 9.0249 - val_mae: 0.6353\n",
      "Epoch 344/500\n",
      "2290/2290 [==============================] - 2s 743us/step - loss: 13.4425 - mae: 0.7486 - val_loss: 9.0245 - val_mae: 0.6337\n",
      "Epoch 345/500\n",
      "2290/2290 [==============================] - 2s 745us/step - loss: 13.4709 - mae: 0.7519 - val_loss: 9.0243 - val_mae: 0.6327\n",
      "Epoch 346/500\n",
      "2290/2290 [==============================] - 2s 658us/step - loss: 13.5489 - mae: 0.7606 - val_loss: 9.0247 - val_mae: 0.6346\n",
      "Epoch 347/500\n",
      "2290/2290 [==============================] - 1s 637us/step - loss: 13.4434 - mae: 0.7467 - val_loss: 9.0235 - val_mae: 0.6296\n",
      "Epoch 348/500\n",
      "2290/2290 [==============================] - 2s 676us/step - loss: 13.4426 - mae: 0.7475 - val_loss: 9.0244 - val_mae: 0.6332\n",
      "Epoch 349/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 13.6450 - mae: 0.7637 - val_loss: 9.0253 - val_mae: 0.6367\n",
      "Epoch 350/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 15.4872 - mae: 0.7878 - val_loss: 9.0294 - val_mae: 0.6522\n",
      "Epoch 351/500\n",
      "2290/2290 [==============================] - 2s 853us/step - loss: 13.4429 - mae: 0.7718 - val_loss: 9.0297 - val_mae: 0.6532\n",
      "Epoch 352/500\n",
      "2290/2290 [==============================] - 2s 867us/step - loss: 13.4442 - mae: 0.7717 - val_loss: 9.0283 - val_mae: 0.6480\n",
      "Epoch 353/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 13.4432 - mae: 0.7678 - val_loss: 9.0281 - val_mae: 0.6472\n",
      "Epoch 354/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 13.4454 - mae: 0.7558 - val_loss: 9.0261 - val_mae: 0.6398\n",
      "Epoch 355/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 13.4431 - mae: 0.7585 - val_loss: 9.0263 - val_mae: 0.6408\n",
      "Epoch 356/500\n",
      "2290/2290 [==============================] - 2s 952us/step - loss: 13.4430 - mae: 0.7591 - val_loss: 9.0262 - val_mae: 0.6402\n",
      "Epoch 357/500\n",
      "2290/2290 [==============================] - 2s 873us/step - loss: 13.4432 - mae: 0.7609 - val_loss: 9.0266 - val_mae: 0.6417\n",
      "Epoch 358/500\n",
      "2290/2290 [==============================] - 2s 829us/step - loss: 13.4420 - mae: 0.7576 - val_loss: 9.0263 - val_mae: 0.6406\n",
      "Epoch 359/500\n",
      "2290/2290 [==============================] - 2s 812us/step - loss: 13.4426 - mae: 0.7571 - val_loss: 9.0262 - val_mae: 0.6401\n",
      "Epoch 360/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 13.4428 - mae: 0.7558 - val_loss: 9.0252 - val_mae: 0.6363\n",
      "Epoch 361/500\n",
      "2290/2290 [==============================] - 2s 835us/step - loss: 13.4483 - mae: 0.7557 - val_loss: 9.0249 - val_mae: 0.6353\n",
      "Epoch 362/500\n",
      "2290/2290 [==============================] - 2s 854us/step - loss: 13.4693 - mae: 0.7556 - val_loss: 9.0255 - val_mae: 0.6375\n",
      "Epoch 363/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 13.4424 - mae: 0.7545 - val_loss: 9.0251 - val_mae: 0.6359\n",
      "Epoch 364/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 13.4428 - mae: 0.7548 - val_loss: 9.0251 - val_mae: 0.6361\n",
      "Epoch 365/500\n",
      "2290/2290 [==============================] - 2s 817us/step - loss: 13.4435 - mae: 0.7467 - val_loss: 9.0243 - val_mae: 0.6328\n",
      "Epoch 366/500\n",
      "2290/2290 [==============================] - 2s 835us/step - loss: 13.4432 - mae: 0.7525 - val_loss: 9.0239 - val_mae: 0.6313\n",
      "Epoch 367/500\n",
      "2290/2290 [==============================] - 2s 883us/step - loss: 13.4429 - mae: 0.7470 - val_loss: 9.0229 - val_mae: 0.6268\n",
      "Epoch 368/500\n",
      "2290/2290 [==============================] - 2s 858us/step - loss: 13.4429 - mae: 0.7543 - val_loss: 9.0251 - val_mae: 0.6359\n",
      "Epoch 369/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 13.4428 - mae: 0.7514 - val_loss: 9.0251 - val_mae: 0.6359\n",
      "Epoch 370/500\n",
      "2290/2290 [==============================] - 2s 808us/step - loss: 13.4429 - mae: 0.7507 - val_loss: 9.0243 - val_mae: 0.6328\n",
      "Epoch 371/500\n",
      "2290/2290 [==============================] - 2s 869us/step - loss: 13.4429 - mae: 0.7583 - val_loss: 9.0254 - val_mae: 0.6372\n",
      "Epoch 372/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 13.4433 - mae: 0.7515 - val_loss: 9.0244 - val_mae: 0.6332\n",
      "Epoch 373/500\n",
      "2290/2290 [==============================] - 2s 871us/step - loss: 13.4426 - mae: 0.7506 - val_loss: 9.0254 - val_mae: 0.6371\n",
      "Epoch 374/500\n",
      "2290/2290 [==============================] - 2s 820us/step - loss: 13.4429 - mae: 0.7508 - val_loss: 9.0244 - val_mae: 0.6330\n",
      "Epoch 375/500\n",
      "2290/2290 [==============================] - 2s 839us/step - loss: 13.4428 - mae: 0.7563 - val_loss: 9.0250 - val_mae: 0.6355\n",
      "Epoch 376/500\n",
      "2290/2290 [==============================] - 2s 812us/step - loss: 13.4428 - mae: 0.7585 - val_loss: 9.0256 - val_mae: 0.6378\n",
      "Epoch 377/500\n",
      "2290/2290 [==============================] - 2s 842us/step - loss: 13.4428 - mae: 0.7550 - val_loss: 9.0250 - val_mae: 0.6358\n",
      "Epoch 378/500\n",
      "2290/2290 [==============================] - 2s 856us/step - loss: 13.4428 - mae: 0.7511 - val_loss: 9.0238 - val_mae: 0.6308\n",
      "Epoch 379/500\n",
      "2290/2290 [==============================] - 2s 856us/step - loss: 13.4424 - mae: 0.7513 - val_loss: 9.0247 - val_mae: 0.6346\n",
      "Epoch 380/500\n",
      "2290/2290 [==============================] - 2s 805us/step - loss: 13.4425 - mae: 0.7531 - val_loss: 9.0244 - val_mae: 0.6334\n",
      "Epoch 381/500\n",
      "2290/2290 [==============================] - 2s 831us/step - loss: 13.4441 - mae: 0.7491 - val_loss: 9.0243 - val_mae: 0.6328\n",
      "Epoch 382/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 13.4425 - mae: 0.7484 - val_loss: 9.0238 - val_mae: 0.6306\n",
      "Epoch 383/500\n",
      "2290/2290 [==============================] - 2s 801us/step - loss: 13.6401 - mae: 0.7585 - val_loss: 9.0255 - val_mae: 0.6374\n",
      "Epoch 384/500\n",
      "2290/2290 [==============================] - 2s 847us/step - loss: 13.4430 - mae: 0.7566 - val_loss: 9.0252 - val_mae: 0.6365\n",
      "Epoch 385/500\n",
      "2290/2290 [==============================] - 2s 868us/step - loss: 13.4427 - mae: 0.7553 - val_loss: 9.0245 - val_mae: 0.6335\n",
      "Epoch 386/500\n",
      "2290/2290 [==============================] - 2s 878us/step - loss: 13.4428 - mae: 0.7521 - val_loss: 9.0241 - val_mae: 0.6321\n",
      "Epoch 387/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 13.4425 - mae: 0.7510 - val_loss: 9.0245 - val_mae: 0.6335\n",
      "Epoch 388/500\n",
      "2290/2290 [==============================] - 2s 789us/step - loss: 13.4575 - mae: 0.7517 - val_loss: 9.0242 - val_mae: 0.6322\n",
      "Epoch 389/500\n",
      "2290/2290 [==============================] - 2s 780us/step - loss: 13.4426 - mae: 0.7539 - val_loss: 9.0246 - val_mae: 0.6341\n",
      "Epoch 390/500\n",
      "2290/2290 [==============================] - 2s 965us/step - loss: 13.4425 - mae: 0.7489 - val_loss: 9.0238 - val_mae: 0.6306\n",
      "Epoch 391/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 13.4431 - mae: 0.7495 - val_loss: 9.0242 - val_mae: 0.6324\n",
      "Epoch 392/500\n",
      "2290/2290 [==============================] - 2s 849us/step - loss: 13.4434 - mae: 0.7455 - val_loss: 9.0228 - val_mae: 0.6263\n",
      "Epoch 393/500\n",
      "2290/2290 [==============================] - 2s 842us/step - loss: 13.4431 - mae: 0.7542 - val_loss: 9.0249 - val_mae: 0.6351\n",
      "Epoch 394/500\n",
      "2290/2290 [==============================] - 2s 846us/step - loss: 13.4427 - mae: 0.7515 - val_loss: 9.0242 - val_mae: 0.6325\n",
      "Epoch 395/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 13.4424 - mae: 0.7519 - val_loss: 9.0240 - val_mae: 0.6317\n",
      "Epoch 396/500\n",
      "2290/2290 [==============================] - 2s 880us/step - loss: 13.4428 - mae: 0.7531 - val_loss: 9.0238 - val_mae: 0.6308\n",
      "Epoch 397/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 13.4427 - mae: 0.7546 - val_loss: 9.0251 - val_mae: 0.6358\n",
      "Epoch 398/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 13.4426 - mae: 0.7534 - val_loss: 9.0238 - val_mae: 0.6306\n",
      "Epoch 399/500\n",
      "2290/2290 [==============================] - 2s 899us/step - loss: 13.4429 - mae: 0.7533 - val_loss: 9.0241 - val_mae: 0.6321\n",
      "Epoch 400/500\n",
      "2290/2290 [==============================] - 2s 872us/step - loss: 13.4436 - mae: 0.7524 - val_loss: 9.0247 - val_mae: 0.6346\n",
      "Epoch 401/500\n",
      "2290/2290 [==============================] - 2s 887us/step - loss: 13.4428 - mae: 0.7489 - val_loss: 9.0241 - val_mae: 0.6318\n",
      "Epoch 402/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 13.4427 - mae: 0.7510 - val_loss: 9.0240 - val_mae: 0.6315\n",
      "Epoch 403/500\n",
      "2290/2290 [==============================] - 2s 737us/step - loss: 13.4441 - mae: 0.7423 - val_loss: 9.0233 - val_mae: 0.6287\n",
      "Epoch 404/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 13.4439 - mae: 0.7482 - val_loss: 9.0238 - val_mae: 0.6309\n",
      "Epoch 405/500\n",
      "2290/2290 [==============================] - 2s 843us/step - loss: 24.0464 - mae: 0.8262 - val_loss: 9.0299 - val_mae: 0.6536\n",
      "Epoch 406/500\n",
      "2290/2290 [==============================] - 2s 908us/step - loss: 13.4432 - mae: 0.7751 - val_loss: 9.0301 - val_mae: 0.6545\n",
      "Epoch 407/500\n",
      "2290/2290 [==============================] - 2s 787us/step - loss: 13.5438 - mae: 0.7757 - val_loss: 9.0292 - val_mae: 0.6514\n",
      "Epoch 408/500\n",
      "2290/2290 [==============================] - 2s 804us/step - loss: 13.4430 - mae: 0.7661 - val_loss: 9.0282 - val_mae: 0.6478\n",
      "Epoch 409/500\n",
      "2290/2290 [==============================] - 2s 790us/step - loss: 13.4430 - mae: 0.7635 - val_loss: 9.0271 - val_mae: 0.6438\n",
      "Epoch 410/500\n",
      "2290/2290 [==============================] - 2s 779us/step - loss: 13.4430 - mae: 0.7647 - val_loss: 9.0267 - val_mae: 0.6422\n",
      "Epoch 411/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 13.4432 - mae: 0.7590 - val_loss: 9.0267 - val_mae: 0.6421\n",
      "Epoch 412/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 13.4428 - mae: 0.7600 - val_loss: 9.0269 - val_mae: 0.6429\n",
      "Epoch 413/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 13.4433 - mae: 0.7553 - val_loss: 9.0260 - val_mae: 0.6394\n",
      "Epoch 414/500\n",
      "2290/2290 [==============================] - 2s 736us/step - loss: 13.4432 - mae: 0.7537 - val_loss: 9.0242 - val_mae: 0.6325\n",
      "Epoch 415/500\n",
      "2290/2290 [==============================] - 2s 739us/step - loss: 13.4438 - mae: 0.7618 - val_loss: 9.0253 - val_mae: 0.6368\n",
      "Epoch 416/500\n",
      "2290/2290 [==============================] - 2s 796us/step - loss: 13.4431 - mae: 0.7558 - val_loss: 9.0252 - val_mae: 0.6363\n",
      "Epoch 417/500\n",
      "2290/2290 [==============================] - 2s 885us/step - loss: 13.4437 - mae: 0.7564 - val_loss: 9.0259 - val_mae: 0.6391\n",
      "Epoch 418/500\n",
      "2290/2290 [==============================] - 2s 723us/step - loss: 13.4433 - mae: 0.7594 - val_loss: 9.0252 - val_mae: 0.6364\n",
      "Epoch 419/500\n",
      "2290/2290 [==============================] - 2s 784us/step - loss: 13.4430 - mae: 0.7544 - val_loss: 9.0247 - val_mae: 0.6345\n",
      "Epoch 420/500\n",
      "2290/2290 [==============================] - 2s 852us/step - loss: 13.4428 - mae: 0.7534 - val_loss: 9.0245 - val_mae: 0.6337\n",
      "Epoch 421/500\n",
      "2290/2290 [==============================] - 2s 752us/step - loss: 13.4432 - mae: 0.7512 - val_loss: 9.0236 - val_mae: 0.6297\n",
      "Epoch 422/500\n",
      "2290/2290 [==============================] - 2s 820us/step - loss: 13.4426 - mae: 0.7530 - val_loss: 9.0249 - val_mae: 0.6351\n",
      "Epoch 423/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 13.4429 - mae: 0.7562 - val_loss: 9.0250 - val_mae: 0.6355\n",
      "Epoch 424/500\n",
      "2290/2290 [==============================] - 2s 870us/step - loss: 13.4426 - mae: 0.7530 - val_loss: 9.0242 - val_mae: 0.6325\n",
      "Epoch 425/500\n",
      "2290/2290 [==============================] - 2s 827us/step - loss: 13.4726 - mae: 0.7604 - val_loss: 9.0249 - val_mae: 0.6353\n",
      "Epoch 426/500\n",
      "2290/2290 [==============================] - 2s 822us/step - loss: 13.4428 - mae: 0.7496 - val_loss: 9.0239 - val_mae: 0.6312\n",
      "Epoch 427/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 13.4426 - mae: 0.7470 - val_loss: 9.0232 - val_mae: 0.6283\n",
      "Epoch 428/500\n",
      "2290/2290 [==============================] - 2s 781us/step - loss: 13.4388 - mae: 0.7501 - val_loss: 9.0242 - val_mae: 0.6322\n",
      "Epoch 429/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 13.4457 - mae: 0.7533 - val_loss: 9.0248 - val_mae: 0.6349\n",
      "Epoch 430/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 13.4444 - mae: 0.7529 - val_loss: 9.0241 - val_mae: 0.6318\n",
      "Epoch 431/500\n",
      "2290/2290 [==============================] - 2s 813us/step - loss: 13.4461 - mae: 0.7570 - val_loss: 9.0244 - val_mae: 0.6334\n",
      "Epoch 432/500\n",
      "2290/2290 [==============================] - 2s 814us/step - loss: 13.4431 - mae: 0.7485 - val_loss: 9.0241 - val_mae: 0.6320\n",
      "Epoch 433/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 13.4427 - mae: 0.7527 - val_loss: 9.0240 - val_mae: 0.6316\n",
      "Epoch 434/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 13.4427 - mae: 0.7538 - val_loss: 9.0248 - val_mae: 0.6350\n",
      "Epoch 435/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 13.4428 - mae: 0.7526 - val_loss: 9.0240 - val_mae: 0.6314\n",
      "Epoch 436/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 13.4428 - mae: 0.7476 - val_loss: 9.0246 - val_mae: 0.6342\n",
      "Epoch 437/500\n",
      "2290/2290 [==============================] - 2s 895us/step - loss: 13.4429 - mae: 0.7485 - val_loss: 9.0237 - val_mae: 0.6301\n",
      "Epoch 438/500\n",
      "2290/2290 [==============================] - 2s 838us/step - loss: 13.4431 - mae: 0.7573 - val_loss: 9.0254 - val_mae: 0.6373\n",
      "Epoch 439/500\n",
      "2290/2290 [==============================] - 2s 821us/step - loss: 13.4428 - mae: 0.7506 - val_loss: 9.0246 - val_mae: 0.6341\n",
      "Epoch 440/500\n",
      "2290/2290 [==============================] - 2s 763us/step - loss: 13.4429 - mae: 0.7519 - val_loss: 9.0243 - val_mae: 0.6328\n",
      "Epoch 441/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 14.6374 - mae: 0.7848 - val_loss: 9.0272 - val_mae: 0.6443\n",
      "Epoch 442/500\n",
      "2290/2290 [==============================] - 2s 809us/step - loss: 13.4427 - mae: 0.7625 - val_loss: 9.0266 - val_mae: 0.6417\n",
      "Epoch 443/500\n",
      "2290/2290 [==============================] - 2s 884us/step - loss: 13.4429 - mae: 0.7597 - val_loss: 9.0269 - val_mae: 0.6429\n",
      "Epoch 444/500\n",
      "2290/2290 [==============================] - 2s 889us/step - loss: 13.4427 - mae: 0.7582 - val_loss: 9.0256 - val_mae: 0.6382\n",
      "Epoch 445/500\n",
      "2290/2290 [==============================] - 2s 823us/step - loss: 13.4432 - mae: 0.7568 - val_loss: 9.0254 - val_mae: 0.6374\n",
      "Epoch 446/500\n",
      "2290/2290 [==============================] - 2s 803us/step - loss: 13.4431 - mae: 0.7599 - val_loss: 9.0259 - val_mae: 0.6393\n",
      "Epoch 447/500\n",
      "2290/2290 [==============================] - 2s 798us/step - loss: 13.4426 - mae: 0.7563 - val_loss: 9.0251 - val_mae: 0.6362\n",
      "Epoch 448/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 13.4429 - mae: 0.7503 - val_loss: 9.0250 - val_mae: 0.6355\n",
      "Epoch 449/500\n",
      "2290/2290 [==============================] - 2s 845us/step - loss: 13.4434 - mae: 0.7492 - val_loss: 9.0253 - val_mae: 0.6369\n",
      "Epoch 450/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 13.4425 - mae: 0.7547 - val_loss: 9.0251 - val_mae: 0.6362\n",
      "Epoch 451/500\n",
      "2290/2290 [==============================] - 2s 837us/step - loss: 13.4430 - mae: 0.7497 - val_loss: 9.0239 - val_mae: 0.6311\n",
      "Epoch 452/500\n",
      "2290/2290 [==============================] - 2s 871us/step - loss: 13.4431 - mae: 0.7549 - val_loss: 9.0253 - val_mae: 0.6370\n",
      "Epoch 453/500\n",
      "2290/2290 [==============================] - 2s 850us/step - loss: 13.4432 - mae: 0.7482 - val_loss: 9.0242 - val_mae: 0.6324\n",
      "Epoch 454/500\n",
      "2290/2290 [==============================] - 2s 800us/step - loss: 13.4431 - mae: 0.7587 - val_loss: 9.0256 - val_mae: 0.6378\n",
      "Epoch 455/500\n",
      "2290/2290 [==============================] - 2s 824us/step - loss: 13.4696 - mae: 0.7518 - val_loss: 9.0245 - val_mae: 0.6334\n",
      "Epoch 456/500\n",
      "2290/2290 [==============================] - 2s 731us/step - loss: 13.4430 - mae: 0.7577 - val_loss: 9.0247 - val_mae: 0.6346\n",
      "Epoch 457/500\n",
      "2290/2290 [==============================] - 2s 753us/step - loss: 13.4432 - mae: 0.7465 - val_loss: 9.0241 - val_mae: 0.6318\n",
      "Epoch 458/500\n",
      "2290/2290 [==============================] - 2s 749us/step - loss: 13.4429 - mae: 0.7561 - val_loss: 9.0245 - val_mae: 0.6338\n",
      "Epoch 459/500\n",
      "2290/2290 [==============================] - 2s 751us/step - loss: 13.4431 - mae: 0.7551 - val_loss: 9.0238 - val_mae: 0.6307\n",
      "Epoch 460/500\n",
      "2290/2290 [==============================] - 2s 758us/step - loss: 13.4440 - mae: 0.7487 - val_loss: 9.0244 - val_mae: 0.6332\n",
      "Epoch 461/500\n",
      "2290/2290 [==============================] - 2s 806us/step - loss: 13.4493 - mae: 0.7551 - val_loss: 9.0244 - val_mae: 0.6332\n",
      "Epoch 462/500\n",
      "2290/2290 [==============================] - 2s 761us/step - loss: 13.4427 - mae: 0.7525 - val_loss: 9.0237 - val_mae: 0.6304\n",
      "Epoch 463/500\n",
      "2290/2290 [==============================] - 2s 788us/step - loss: 13.4431 - mae: 0.7498 - val_loss: 9.0241 - val_mae: 0.6322\n",
      "Epoch 464/500\n",
      "2290/2290 [==============================] - 2s 793us/step - loss: 13.4427 - mae: 0.7491 - val_loss: 9.0234 - val_mae: 0.6292\n",
      "Epoch 465/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 13.4571 - mae: 0.7522 - val_loss: 9.0239 - val_mae: 0.6311\n",
      "Epoch 466/500\n",
      "2290/2290 [==============================] - 2s 754us/step - loss: 13.4426 - mae: 0.7536 - val_loss: 9.0248 - val_mae: 0.6348\n",
      "Epoch 467/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 13.4427 - mae: 0.7546 - val_loss: 9.0241 - val_mae: 0.6319\n",
      "Epoch 468/500\n",
      "2290/2290 [==============================] - 2s 792us/step - loss: 13.4428 - mae: 0.7506 - val_loss: 9.0243 - val_mae: 0.6329\n",
      "Epoch 469/500\n",
      "2290/2290 [==============================] - 2s 778us/step - loss: 13.4431 - mae: 0.7554 - val_loss: 9.0248 - val_mae: 0.6349\n",
      "Epoch 470/500\n",
      "2290/2290 [==============================] - 2s 799us/step - loss: 13.4428 - mae: 0.7491 - val_loss: 9.0240 - val_mae: 0.6315\n",
      "Epoch 471/500\n",
      "2290/2290 [==============================] - 2s 872us/step - loss: 13.4429 - mae: 0.7513 - val_loss: 9.0236 - val_mae: 0.6300\n",
      "Epoch 472/500\n",
      "2290/2290 [==============================] - 2s 769us/step - loss: 13.4432 - mae: 0.7521 - val_loss: 9.0240 - val_mae: 0.6318\n",
      "Epoch 473/500\n",
      "2290/2290 [==============================] - 2s 777us/step - loss: 13.4427 - mae: 0.7500 - val_loss: 9.0237 - val_mae: 0.6304\n",
      "Epoch 474/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 13.4431 - mae: 0.7480 - val_loss: 9.0234 - val_mae: 0.6290\n",
      "Epoch 475/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 13.4426 - mae: 0.7491 - val_loss: 9.0235 - val_mae: 0.6294\n",
      "Epoch 476/500\n",
      "2290/2290 [==============================] - 2s 783us/step - loss: 13.4428 - mae: 0.7531 - val_loss: 9.0241 - val_mae: 0.6320\n",
      "Epoch 477/500\n",
      "2290/2290 [==============================] - 2s 774us/step - loss: 13.4425 - mae: 0.7482 - val_loss: 9.0232 - val_mae: 0.6284\n",
      "Epoch 478/500\n",
      "2290/2290 [==============================] - 2s 866us/step - loss: 13.4430 - mae: 0.7497 - val_loss: 9.0231 - val_mae: 0.6279\n",
      "Epoch 479/500\n",
      "2290/2290 [==============================] - 2s 786us/step - loss: 13.4430 - mae: 0.7509 - val_loss: 9.0235 - val_mae: 0.6297\n",
      "Epoch 480/500\n",
      "2290/2290 [==============================] - 2s 807us/step - loss: 13.4429 - mae: 0.7469 - val_loss: 9.0237 - val_mae: 0.6303\n",
      "Epoch 481/500\n",
      "2290/2290 [==============================] - 2s 818us/step - loss: 13.4428 - mae: 0.7483 - val_loss: 9.0240 - val_mae: 0.6314\n",
      "Epoch 482/500\n",
      "2290/2290 [==============================] - 2s 920us/step - loss: 13.4428 - mae: 0.7499 - val_loss: 9.0236 - val_mae: 0.6300\n",
      "Epoch 483/500\n",
      "2290/2290 [==============================] - 2s 791us/step - loss: 13.4431 - mae: 0.7500 - val_loss: 9.0229 - val_mae: 0.6269\n",
      "Epoch 484/500\n",
      "2290/2290 [==============================] - 2s 890us/step - loss: 13.5276 - mae: 0.7589 - val_loss: 9.0247 - val_mae: 0.6345\n",
      "Epoch 485/500\n",
      "2290/2290 [==============================] - 2s 885us/step - loss: 13.4429 - mae: 0.7486 - val_loss: 9.0237 - val_mae: 0.6303\n",
      "Epoch 486/500\n",
      "2290/2290 [==============================] - 2s 864us/step - loss: 13.4427 - mae: 0.7468 - val_loss: 9.0231 - val_mae: 0.6278\n",
      "Epoch 487/500\n",
      "2290/2290 [==============================] - 2s 886us/step - loss: 13.4427 - mae: 0.7490 - val_loss: 9.0244 - val_mae: 0.6330\n",
      "Epoch 488/500\n",
      "2290/2290 [==============================] - 2s 709us/step - loss: 13.4431 - mae: 0.7522 - val_loss: 9.0238 - val_mae: 0.6306\n",
      "Epoch 489/500\n",
      "2290/2290 [==============================] - 2s 750us/step - loss: 13.4425 - mae: 0.7485 - val_loss: 9.0239 - val_mae: 0.6312\n",
      "Epoch 490/500\n",
      "2290/2290 [==============================] - 2s 703us/step - loss: 13.4431 - mae: 0.7518 - val_loss: 9.0241 - val_mae: 0.6318\n",
      "Epoch 491/500\n",
      "2290/2290 [==============================] - 2s 684us/step - loss: 13.4428 - mae: 0.7528 - val_loss: 9.0242 - val_mae: 0.6323\n",
      "Epoch 492/500\n",
      "2290/2290 [==============================] - 1s 627us/step - loss: 13.4430 - mae: 0.7526 - val_loss: 9.0244 - val_mae: 0.6333\n",
      "Epoch 493/500\n",
      "2290/2290 [==============================] - 1s 625us/step - loss: 13.4439 - mae: 0.7565 - val_loss: 9.0239 - val_mae: 0.6312\n",
      "Epoch 494/500\n",
      "2290/2290 [==============================] - 2s 666us/step - loss: 16.7067 - mae: 0.8020 - val_loss: 9.0280 - val_mae: 0.6472\n",
      "Epoch 495/500\n",
      "2290/2290 [==============================] - 1s 633us/step - loss: 13.4430 - mae: 0.7691 - val_loss: 9.0282 - val_mae: 0.6477\n",
      "Epoch 496/500\n",
      "2290/2290 [==============================] - 2s 694us/step - loss: 13.4431 - mae: 0.7613 - val_loss: 9.0272 - val_mae: 0.6439\n",
      "Epoch 497/500\n",
      "2290/2290 [==============================] - 2s 687us/step - loss: 13.4434 - mae: 0.7707 - val_loss: 9.0280 - val_mae: 0.6470\n",
      "Epoch 498/500\n",
      "2290/2290 [==============================] - 2s 832us/step - loss: 13.4430 - mae: 0.7689 - val_loss: 9.0280 - val_mae: 0.6471\n",
      "Epoch 499/500\n",
      "2290/2290 [==============================] - 2s 842us/step - loss: 13.4439 - mae: 0.7589 - val_loss: 9.0263 - val_mae: 0.6408\n",
      "Epoch 500/500\n",
      "2290/2290 [==============================] - 2s 802us/step - loss: 13.4834 - mae: 0.7699 - val_loss: 9.0267 - val_mae: 0.6422\n",
      "Test Score: 3.00 RMSE\n"
     ]
    }
   ],
   "source": [
    "fifth = data[data['zip5'] == 53718]\n",
    "fifth = fifth.drop(['zip5', 'date_key'], axis =1)\n",
    "fifth_17_18 = fifth[fifth['year'] != 2019]\n",
    "y = fifth_17_18['impact_score']\n",
    "X = fifth_17_18.drop(['impact_score'], axis = 1)\n",
    "X = np.expand_dims(X, axis = 2)\n",
    "\n",
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=4, activation=\"relu\", filters=16),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=4, activation=\"relu\", filters=16),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),\n",
    "        Dense(1, activation='linear'),# For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "test_size = int(0.2 * 2862)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=25, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "#print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "#for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "#    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f7e36d34be0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAS2ElEQVR4nO3df5DcdX3H8edbAspwQoLINU1io0O0KCkIV4wy1juwHcBOQzsy1dKSMOlkplJrx9gxrZ3pdNqZYhVRRoYxI9bgWCOlUjIEf6SRq+OMUElBAkRJpAhH0qTUmHqA2ui7f9wncUn2snuX3b3cZ5+PmZ39fj/fz+6932zudd/73HeXyEwkSXV50UwXIEnqPMNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl5qIiNMj4o6IeDYivhcRvzfTNUlTMWemC5COUzcBPwEGgfOATRHxrcx8ZGbLktoTvkNVeqGIOAXYB5yTmY+Vsc8AT2fm2hktTmqTyzLSkV4N/PRgsBffAl43Q/VIU2a4S0caAPYfNrYfeOkM1CJNi+EuHWkcOPWwsVOBH85ALdK0GO7SkR4D5kTEkoaxcwH/mKpZwz+oSk1ExAYggT9k4mqZu4E3ebWMZgvP3KXm3gWcDOwFPgf8kcGu2cQzd0mqkGfuklShtsI9IuZGxO0R8e2I2B4Rbyxvz94cETvK/bwyNyLixojYGREPRcT53W1BknS4ds/cPwZ8KTN/mYmrBrYDa4EtmbkE2FL2AS4DlpTbauDmjlYsSWqp5Zp7RJzKxLvzXpUNkyPiO8BwZu6OiPnAaGa+JiI+UbY/d/i8rnUhSXqBdj447FXAfwP/EBHnAluB9wCDBwO7BPyZZf4C4KmGx4+VsUnD/YwzzsjFixdPvfrjyLPPPsspp5wy02X0TL/1C/bcL2ZTz1u3bn0mM1/e7Fg74T4HOB94d2beFxEf4+dLMM1Ek7Ejfj2IiNVMLNswODjIhz/84TZKOX6Nj48zMDAw02X0TL/1C/bcL2ZTzyMjI9+b7Fg74T4GjGXmfWX/dibCfU9EzG9YltnbMH9Rw+MXArsOf9LMXAesAxgaGsrh4eE2Sjl+jY6OMtt7mIp+6xfsuV/U0nPLP6hm5n8BT0XEa8rQJcCjwEZgRRlbAdxZtjcCV5erZpYB+11vl6Teavd/1vFu4LMRcRLwOHANEz8YbouIVcCTwJVl7t3A5cBO4LkyV5LUQ22Fe2Y+CAw1OXRJk7kJXHuMdUmSjoHvUJWkChnuklQhw12SKmS4S1KFDHdJqlC7l0JKfWnx2k2HttcsPcDKhv12PHHd2zpdktQWz9wlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KF2gr3iHgiIrZFxIMRcX8ZOz0iNkfEjnI/r4xHRNwYETsj4qGIOL+bDUiSjjSVM/eRzDwvM4fK/lpgS2YuAbaUfYDLgCXlthq4uVPFSpLacyzLMsuB9WV7PXBFw/itOeFeYG5EzD+GryNJmqLIzNaTIv4T2Ack8InMXBcRP8jMuQ1z9mXmvIi4C7guM79exrcA78/M+w97ztVMnNkzODh4wYYNGzrW1EwYHx9nYGBgpsvomX7pd9vT+w9tD54Me56f2uOXLjitwxX1Vr+8zo1mU88jIyNbG1ZTXmBOm89xUWbuiogzgc0R8e2jzI0mY0f8BMnMdcA6gKGhoRweHm6zlOPT6Ogos72HqeiXfleu3XRoe83SA1y/rd1vmQlPXDXc4Yp6q19e50a19NzWskxm7ir3e4E7gAuBPQeXW8r93jJ9DFjU8PCFwK5OFSxJaq1luEfEKRHx0oPbwG8ADwMbgRVl2grgzrK9Ebi6XDWzDNifmbs7XrkkaVLt/I45CNwREQfn/2NmfikivgncFhGrgCeBK8v8u4HLgZ3Ac8A1Ha9aknRULcM9Mx8Hzm0y/j/AJU3GE7i2I9VJkqbFd6hKUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRVqO9wj4oSIeCAi7ir7r4yI+yJiR0R8PiJOKuMvLvs7y/HF3SldkjSZqZy5vwfY3rD/QeCGzFwC7ANWlfFVwL7MPAu4ocyTJPVQW+EeEQuBtwGfLPsBXAzcXqasB64o28vLPuX4JWW+JKlHIjNbT4q4Hfg74KXA+4CVwL3l7JyIWAR8MTPPiYiHgUszc6wc+y7whsx85rDnXA2sBhgcHLxgw4YNHWtqJoyPjzMwMDDTZfRMv/S77en9h7YHT4Y9z0/t8UsXnNbhinqrX17nRrOp55GRka2ZOdTs2JxWD46I3wT2ZubWiBg+ONxkarZx7OcDmeuAdQBDQ0M5PDx8+JRZZXR0lNnew1T0S78r1246tL1m6QGu39byW+YFnrhquMMV9Va/vM6Naum5nX+pFwG/FRGXAy8BTgU+CsyNiDmZeQBYCOwq88eARcBYRMwBTgO+3/HKJUmTarnmnpl/npkLM3Mx8A7gq5l5FXAP8PYybQVwZ9neWPYpx7+a7az9SJI65liuc38/8N6I2Am8DLiljN8CvKyMvxdYe2wlSpKmakoLiJk5CoyW7ceBC5vM+RFwZQdqkyRNk+9QlaQKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIq1DLcI+IlEfHvEfGtiHgkIv66jL8yIu6LiB0R8fmIOKmMv7js7yzHF3e3BUnS4do5c/8xcHFmngucB1waEcuADwI3ZOYSYB+wqsxfBezLzLOAG8o8SVIPtQz3nDBedk8stwQuBm4v4+uBK8r28rJPOX5JRETHKpYktRSZ2XpSxAnAVuAs4CbgQ8C95eyciFgEfDEzz4mIh4FLM3OsHPsu8IbMfOaw51wNrAYYHBy8YMOGDZ3ragaMj48zMDAw02X0TL/0u+3p/Ye2B0+GPc9P7fFLF5zW4Yp6q19e50azqeeRkZGtmTnU7Nicdp4gM38KnBcRc4E7gLObTSv3zc7Sj/gJkpnrgHUAQ0NDOTw83E4px63R0VFmew9T0S/9rly76dD2mqUHuH5bW98yhzxx1XCHK+qtfnmdG9XS85SulsnMHwCjwDJgbkQc/Je+ENhVtseARQDl+GnA9ztRrCSpPe1cLfPycsZORJwMvBXYDtwDvL1MWwHcWbY3ln3K8a9mO2s/kqSOaed3zPnA+rLu/iLgtsy8KyIeBTZExN8CDwC3lPm3AJ+JiJ1MnLG/owt1S5KOomW4Z+ZDwOubjD8OXNhk/EfAlR2pTpI0Lb5DVZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqUMtwj4hFEXFPRGyPiEci4j1l/PSI2BwRO8r9vDIeEXFjROyMiIci4vxuNyFJeqF2ztwPAGsy82xgGXBtRLwWWAtsycwlwJayD3AZsKTcVgM3d7xqSdJRtQz3zNydmf9Rtn8IbAcWAMuB9WXaeuCKsr0cuDUn3AvMjYj5Ha9ckjSpyMz2J0csBr4GnAM8mZlzG47ty8x5EXEXcF1mfr2MbwHen5n3H/Zcq5k4s2dwcPCCDRs2HGMrM2t8fJyBgYGZLqNn+qXfbU/vP7Q9eDLseX5qj1+64LQOV9Rb/fI6N5pNPY+MjGzNzKFmx+a0+yQRMQD8M/Cnmfm/ETHp1CZjR/wEycx1wDqAoaGhHB4ebreU49Lo6CizvYep6Jd+V67ddGh7zdIDXL+t7W8ZAJ64arjDFfVWv7zOjWrpua2rZSLiRCaC/bOZ+YUyvOfgcku531vGx4BFDQ9fCOzqTLmSpHa0c7VMALcA2zPzIw2HNgIryvYK4M6G8avLVTPLgP2ZubuDNUuSWmjnd8yLgD8AtkXEg2XsL4DrgNsiYhXwJHBlOXY3cDmwE3gOuKajFUuSWmoZ7uUPo5MtsF/SZH4C1x5jXZKkY+A7VCWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekCrUM94j4VETsjYiHG8ZOj4jNEbGj3M8r4xERN0bEzoh4KCLO72bxkqTm2jlz/zRw6WFja4EtmbkE2FL2AS4DlpTbauDmzpQpSZqKluGemV8Dvn/Y8HJgfdleD1zRMH5rTrgXmBsR8ztVrCSpPdNdcx/MzN0A5f7MMr4AeKph3lgZkyT10JwOP180GcumEyNWM7F0w+DgIKOjox0upbfGx8dnfQ9T0S/9rll64ND24Mkv3G/HbP9v1C+vc6Naep5uuO+JiPmZubssu+wt42PAooZ5C4FdzZ4gM9cB6wCGhoZyeHh4mqUcH0ZHR5ntPUxFv/S7cu2mQ9trlh7g+m1T+5Z54qrhDlfUW/3yOjeqpefpLstsBFaU7RXAnQ3jV5erZpYB+w8u30iSeqflaUhEfA4YBs6IiDHgr4DrgNsiYhXwJHBlmX43cDmwE3gOuKYLNUuSWmgZ7pn5zkkOXdJkbgLXHmtRkqRj4ztUJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVKGuhHtEXBoR34mInRGxthtfQ5I0uY6He0ScANwEXAa8FnhnRLy2019HkjS5bpy5XwjszMzHM/MnwAZgeRe+jiRpEnO68JwLgKca9seANxw+KSJWA6vL7nhEfKcLtfTSGcAzM11ED/Vbv/zJNHqOD3apmN7pu9eZ2dXzL012oBvhHk3G8oiBzHXAui58/RkREfdn5tBM19Er/dYv2HO/qKXnbizLjAGLGvYXAru68HUkSZPoRrh/E1gSEa+MiJOAdwAbu/B1JEmT6PiyTGYeiIg/Br4MnAB8KjMf6fTXOQ5Vs8TUpn7rF+y5X1TRc2QesRwuSZrlfIeqJFXIcJekChnu0xQRV0bEIxHxs4iY9LKpWj6KISJOj4jNEbGj3M+bZN7fl/8u2yPixohodmnsrDCFnl8REV8pPT8aEYt7W2nntNtzmXtqRDwdER/vZY2d1k7PEXFeRHyj/Nt+KCJ+dyZqnQrDffoeBn4H+NpkEyr7KIa1wJbMXAJsKfsvEBFvAi4CfgU4B/hV4C29LLLDWvZc3Ap8KDPPZuId2nt7VF83tNszwN8A/9aTqrqrnZ6fA67OzNcBlwIfjYi5Paxxygz3acrM7ZnZ6l21NX0Uw3JgfdleD1zRZE4CLwFOAl4MnAjs6Ul13dGy5/LDek5mbgbIzPHMfK53JXZcO68zEXEBMAh8pUd1dVPLnjPzsczcUbZ3MfED/OU9q3AaDPfuavZRDAtmqJZjNZiZuwHK/ZmHT8jMbwD3ALvL7cuZub2nVXZWy56BVwM/iIgvRMQDEfGh8hvbbNWy54h4EXA98Gc9rq1b2nmdD4mIC5k4gfluD2qbtm58/EA1IuJfgV9ocugDmXlnO0/RZOy4vfb0aP22+fizgLOZeFcywOaI+LXMnHTpaqYda89MfA+9GXg98CTweWAlcEsn6uuGDvT8LuDuzHxqtvxJpQM9H3ye+cBngBWZ+bNO1NYthvtRZOZbj/EpZtVHMRyt34jYExHzM3N3+QfebF35t4F7M3O8POaLwDKO8neJmdaBnseABzLz8fKYf2Gi5+M23DvQ8xuBN0fEu4AB4KSIGM/M4/aCgQ70TEScCmwC/jIz7+1SqR3jskx31fRRDBuBFWV7BdDsN5cngbdExJyIOJGJP6bO5mWZdnr+JjAvIg6uv14MPNqD2rqlZc+ZeVVmviIzFwPvA249noO9DS17Lt+/dzDR6z/1sLbpy0xv07gxcZY6BvyYiT8afrmM/yITv7IenHc58BgT63MfmOm6j6HflzFxJcGOcn96GR8CPlm2TwA+wUSgPwp8ZKbr7nbPZf/XgYeAbcCngZNmuvZu99wwfyXw8Zmuu9s9A78P/B/wYMPtvJmu/Wg3P35AkirksowkVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRX6fxXgNgr14WJYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e37051f60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test.hist(bins = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
