{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import *\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('random_forest.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = list(data.zip5.unique())\n",
    "zips1 = zips[:23]\n",
    "zips2 = zips[23:46]\n",
    "zips3 = zips[46:69]\n",
    "zips4 = zips[69:]\n",
    "zipsx = [29172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[data['zip5'] == 17013]\n",
    "test = test.drop(['zip5', 'datetime'], axis =1)\n",
    "test_17_18 = test[test['year_2019'] != 1]\n",
    "y = test_17_18['impact_score']\n",
    "X = test_17_18.drop(['impact_score'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def reshape_X(X, time_interval):\n",
    "    XT = np.transpose(X.values)\n",
    "    reshaped_X = np.transpose(rolling_window(XT, time_interval), (1,2,0))\n",
    "    return reshaped_X\n",
    "\n",
    "X = reshape_X(X, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.3 * X.shape[0])\n",
    "X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 25, 4)             2772      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 4)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 9, 4)              68        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 4)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 4)              68        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2,915\n",
      "Trainable params: 2,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2026 samples, validate on 867 samples\n",
      "Epoch 1/150\n",
      "2026/2026 [==============================] - 3s 1ms/step - loss: 4830321.0854 - mse: 4830321.5000 - val_loss: 108922.4872 - val_mse: 108922.4531\n",
      "Epoch 2/150\n",
      "2026/2026 [==============================] - 2s 872us/step - loss: 1202509.3969 - mse: 1202509.5000 - val_loss: 8621.8638 - val_mse: 8621.8623\n",
      "Epoch 3/150\n",
      "2026/2026 [==============================] - 2s 807us/step - loss: 1150271.0132 - mse: 1150270.8750 - val_loss: 229.3859 - val_mse: 229.3859\n",
      "Epoch 4/150\n",
      "2026/2026 [==============================] - 2s 843us/step - loss: 734646.6373 - mse: 734646.4375 - val_loss: 237.4873 - val_mse: 237.4872\n",
      "Epoch 5/150\n",
      "2026/2026 [==============================] - 2s 897us/step - loss: 472915.0499 - mse: 472915.1562 - val_loss: 229.1502 - val_mse: 229.1502\n",
      "Epoch 6/150\n",
      "2026/2026 [==============================] - 1s 734us/step - loss: 281510.3574 - mse: 281510.3125 - val_loss: 229.0832 - val_mse: 229.0832\n",
      "Epoch 7/150\n",
      "2026/2026 [==============================] - 2s 786us/step - loss: 289197.7642 - mse: 289197.7500 - val_loss: 229.0338 - val_mse: 229.0338\n",
      "Epoch 8/150\n",
      "2026/2026 [==============================] - 2s 804us/step - loss: 103584.9081 - mse: 103584.9141 - val_loss: 228.9963 - val_mse: 228.9963\n",
      "Epoch 9/150\n",
      "2026/2026 [==============================] - 2s 780us/step - loss: 163850.9139 - mse: 163850.9844 - val_loss: 228.9054 - val_mse: 228.9054\n",
      "Epoch 10/150\n",
      "2026/2026 [==============================] - 2s 809us/step - loss: 151731.2845 - mse: 151731.2344 - val_loss: 228.6545 - val_mse: 228.6545\n",
      "Epoch 11/150\n",
      "2026/2026 [==============================] - 2s 816us/step - loss: 86810.1361 - mse: 86810.1562 - val_loss: 228.4885 - val_mse: 228.4884\n",
      "Epoch 12/150\n",
      "2026/2026 [==============================] - 2s 787us/step - loss: 115847.5653 - mse: 115847.6094 - val_loss: 228.2087 - val_mse: 228.2086\n",
      "Epoch 13/150\n",
      "2026/2026 [==============================] - 2s 798us/step - loss: 64400.3922 - mse: 64400.4141 - val_loss: 228.0851 - val_mse: 228.0851\n",
      "Epoch 14/150\n",
      "2026/2026 [==============================] - 2s 823us/step - loss: 60442.0483 - mse: 60442.0273 - val_loss: 227.9600 - val_mse: 227.9601\n",
      "Epoch 15/150\n",
      "2026/2026 [==============================] - 2s 796us/step - loss: 30003.5429 - mse: 30003.5332 - val_loss: 227.6587 - val_mse: 227.6587\n",
      "Epoch 16/150\n",
      "2026/2026 [==============================] - 2s 785us/step - loss: 64691.9171 - mse: 64691.9141 - val_loss: 227.3818 - val_mse: 227.3819\n",
      "Epoch 17/150\n",
      "2026/2026 [==============================] - 2s 813us/step - loss: 29377.4375 - mse: 29377.4492 - val_loss: 226.9641 - val_mse: 226.9642\n",
      "Epoch 18/150\n",
      "2026/2026 [==============================] - 2s 801us/step - loss: 34615.8309 - mse: 34615.8320 - val_loss: 226.7351 - val_mse: 226.7351\n",
      "Epoch 19/150\n",
      "2026/2026 [==============================] - 2s 770us/step - loss: 16171.0290 - mse: 16171.0342 - val_loss: 226.2940 - val_mse: 226.2941\n",
      "Epoch 20/150\n",
      "2026/2026 [==============================] - 2s 813us/step - loss: 16081.9725 - mse: 16081.9775 - val_loss: 225.8419 - val_mse: 225.8419\n",
      "Epoch 21/150\n",
      "2026/2026 [==============================] - 2s 800us/step - loss: 10047.2794 - mse: 10047.2764 - val_loss: 225.3763 - val_mse: 225.3763\n",
      "Epoch 22/150\n",
      "2026/2026 [==============================] - 2s 848us/step - loss: 5261.7066 - mse: 5261.7065 - val_loss: 224.8832 - val_mse: 224.8831\n",
      "Epoch 23/150\n",
      "2026/2026 [==============================] - 2s 963us/step - loss: 4597.9774 - mse: 4597.9756 - val_loss: 224.2178 - val_mse: 224.2178\n",
      "Epoch 24/150\n",
      "2026/2026 [==============================] - 2s 1ms/step - loss: 3836.4656 - mse: 3836.4656 - val_loss: 223.5174 - val_mse: 223.5173\n",
      "Epoch 25/150\n",
      "2026/2026 [==============================] - 2s 942us/step - loss: 2283.8281 - mse: 2283.8274 - val_loss: 222.8232 - val_mse: 222.8233\n",
      "Epoch 26/150\n",
      "2026/2026 [==============================] - 2s 876us/step - loss: 2454.3539 - mse: 2454.3538 - val_loss: 221.9282 - val_mse: 221.9283\n",
      "Epoch 27/150\n",
      "2026/2026 [==============================] - 2s 910us/step - loss: 1642.3403 - mse: 1642.3401 - val_loss: 220.9365 - val_mse: 220.9364\n",
      "Epoch 28/150\n",
      "2026/2026 [==============================] - 2s 1ms/step - loss: 924.7260 - mse: 924.7261 - val_loss: 219.8227 - val_mse: 219.8228\n",
      "Epoch 29/150\n",
      "2026/2026 [==============================] - 2s 861us/step - loss: 474.5390 - mse: 474.5390 - val_loss: 218.4893 - val_mse: 218.4893\n",
      "Epoch 30/150\n",
      "2026/2026 [==============================] - 2s 938us/step - loss: 270.5814 - mse: 270.5813 - val_loss: 217.0368 - val_mse: 217.0368\n",
      "Epoch 31/150\n",
      "2026/2026 [==============================] - 2s 913us/step - loss: 248.9862 - mse: 248.9861 - val_loss: 215.4375 - val_mse: 215.4375\n",
      "Epoch 32/150\n",
      "2026/2026 [==============================] - 2s 809us/step - loss: 224.6194 - mse: 224.6195 - val_loss: 213.7113 - val_mse: 213.7112\n",
      "Epoch 33/150\n",
      "2026/2026 [==============================] - 2s 809us/step - loss: 221.6414 - mse: 221.6414 - val_loss: 211.8573 - val_mse: 211.8573\n",
      "Epoch 34/150\n",
      "2026/2026 [==============================] - 2s 811us/step - loss: 219.7045 - mse: 219.7045 - val_loss: 209.9077 - val_mse: 209.9077\n",
      "Epoch 35/150\n",
      "2026/2026 [==============================] - 2s 804us/step - loss: 217.6826 - mse: 217.6826 - val_loss: 207.8880 - val_mse: 207.8881\n",
      "Epoch 36/150\n",
      "2026/2026 [==============================] - 2s 792us/step - loss: 215.5990 - mse: 215.5991 - val_loss: 205.8185 - val_mse: 205.8185\n",
      "Epoch 37/150\n",
      "2026/2026 [==============================] - 2s 785us/step - loss: 213.4753 - mse: 213.4753 - val_loss: 203.7184 - val_mse: 203.7184\n",
      "Epoch 38/150\n",
      "2026/2026 [==============================] - 2s 789us/step - loss: 211.3248 - mse: 211.3248 - val_loss: 201.5985 - val_mse: 201.5986\n",
      "Epoch 39/150\n",
      "2026/2026 [==============================] - 2s 789us/step - loss: 209.1605 - mse: 209.1607 - val_loss: 199.4674 - val_mse: 199.4674\n",
      "Epoch 40/150\n",
      "2026/2026 [==============================] - 2s 789us/step - loss: 206.9897 - mse: 206.9896 - val_loss: 197.3360 - val_mse: 197.3359\n",
      "Epoch 41/150\n",
      "2026/2026 [==============================] - 2s 806us/step - loss: 204.8184 - mse: 204.8184 - val_loss: 195.2061 - val_mse: 195.2060\n",
      "Epoch 42/150\n",
      "2026/2026 [==============================] - 2s 791us/step - loss: 202.6509 - mse: 202.6508 - val_loss: 193.0796 - val_mse: 193.0795\n",
      "Epoch 43/150\n",
      "2026/2026 [==============================] - 2s 783us/step - loss: 200.4885 - mse: 200.4886 - val_loss: 190.9610 - val_mse: 190.9610\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026/2026 [==============================] - 2s 769us/step - loss: 198.3345 - mse: 198.3344 - val_loss: 188.8504 - val_mse: 188.8505\n",
      "Epoch 45/150\n",
      "2026/2026 [==============================] - 2s 770us/step - loss: 196.1902 - mse: 196.1902 - val_loss: 186.7503 - val_mse: 186.7503\n",
      "Epoch 46/150\n",
      "2026/2026 [==============================] - 2s 793us/step - loss: 194.0532 - mse: 194.0533 - val_loss: 184.6582 - val_mse: 184.6582\n",
      "Epoch 47/150\n",
      "2026/2026 [==============================] - 2s 910us/step - loss: 191.9268 - mse: 191.9267 - val_loss: 182.5768 - val_mse: 182.5768\n",
      "Epoch 48/150\n",
      "2026/2026 [==============================] - 2s 891us/step - loss: 189.8086 - mse: 189.8086 - val_loss: 180.5020 - val_mse: 180.5020\n",
      "Epoch 49/150\n",
      "2026/2026 [==============================] - 2s 774us/step - loss: 187.6989 - mse: 187.6989 - val_loss: 178.4365 - val_mse: 178.4365\n",
      "Epoch 50/150\n",
      "2026/2026 [==============================] - 2s 825us/step - loss: 185.5970 - mse: 185.5968 - val_loss: 176.3778 - val_mse: 176.3777\n",
      "Epoch 51/150\n",
      "2026/2026 [==============================] - 2s 955us/step - loss: 183.5012 - mse: 183.5013 - val_loss: 174.3258 - val_mse: 174.3257\n",
      "Epoch 52/150\n",
      "2026/2026 [==============================] - 2s 819us/step - loss: 181.4109 - mse: 181.4109 - val_loss: 172.2793 - val_mse: 172.2793\n",
      "Epoch 53/150\n",
      "2026/2026 [==============================] - 2s 845us/step - loss: 179.3254 - mse: 179.3254 - val_loss: 170.2351 - val_mse: 170.2351\n",
      "Epoch 54/150\n",
      "2026/2026 [==============================] - 2s 773us/step - loss: 177.2388 - mse: 177.2388 - val_loss: 168.1887 - val_mse: 168.1887\n",
      "Epoch 55/150\n",
      "2026/2026 [==============================] - 2s 751us/step - loss: 175.1515 - mse: 175.1515 - val_loss: 166.1404 - val_mse: 166.1404\n",
      "Epoch 56/150\n",
      "2026/2026 [==============================] - 2s 748us/step - loss: 173.0590 - mse: 173.0589 - val_loss: 164.0851 - val_mse: 164.0851\n",
      "Epoch 57/150\n",
      "2026/2026 [==============================] - 1s 725us/step - loss: 170.9548 - mse: 170.9547 - val_loss: 162.0158 - val_mse: 162.0157\n",
      "Epoch 58/150\n",
      "2026/2026 [==============================] - 1s 733us/step - loss: 168.8321 - mse: 168.8321 - val_loss: 159.9252 - val_mse: 159.9251\n",
      "Epoch 59/150\n",
      "2026/2026 [==============================] - 1s 728us/step - loss: 166.6860 - mse: 166.6859 - val_loss: 157.8064 - val_mse: 157.8063\n",
      "Epoch 60/150\n",
      "2026/2026 [==============================] - 2s 750us/step - loss: 164.5057 - mse: 164.5057 - val_loss: 155.6504 - val_mse: 155.6504\n",
      "Epoch 61/150\n",
      "2026/2026 [==============================] - 2s 1ms/step - loss: 162.2787 - mse: 162.2787 - val_loss: 153.4407 - val_mse: 153.4407\n",
      "Epoch 62/150\n",
      "2026/2026 [==============================] - 3s 2ms/step - loss: 159.9904 - mse: 159.9904 - val_loss: 151.1648 - val_mse: 151.1649\n",
      "Epoch 63/150\n",
      "2026/2026 [==============================] - 2s 943us/step - loss: 157.6254 - mse: 157.6254 - val_loss: 148.8041 - val_mse: 148.8042\n",
      "Epoch 64/150\n",
      "2026/2026 [==============================] - 2s 1ms/step - loss: 155.1630 - mse: 155.1630 - val_loss: 146.3388 - val_mse: 146.3388\n",
      "Epoch 65/150\n",
      "2026/2026 [==============================] - 2s 921us/step - loss: 152.5836 - mse: 152.5835 - val_loss: 143.7516 - val_mse: 143.7516\n",
      "Epoch 66/150\n",
      "2026/2026 [==============================] - 2s 949us/step - loss: 149.8701 - mse: 149.8701 - val_loss: 141.0227 - val_mse: 141.0228\n",
      "Epoch 67/150\n",
      "2026/2026 [==============================] - 1s 736us/step - loss: 147.0037 - mse: 147.0038 - val_loss: 138.1414 - val_mse: 138.1414\n",
      "Epoch 68/150\n",
      "2026/2026 [==============================] - 2s 784us/step - loss: 143.9807 - mse: 143.9807 - val_loss: 135.1042 - val_mse: 135.1042\n",
      "Epoch 69/150\n",
      "2026/2026 [==============================] - 2s 933us/step - loss: 140.8012 - mse: 140.8011 - val_loss: 131.9233 - val_mse: 131.9233\n",
      "Epoch 70/150\n",
      "2026/2026 [==============================] - 2s 944us/step - loss: 137.4737 - mse: 137.4737 - val_loss: 128.6015 - val_mse: 128.6015\n",
      "Epoch 71/150\n",
      "2026/2026 [==============================] - 2s 779us/step - loss: 134.0215 - mse: 134.0215 - val_loss: 125.1750 - val_mse: 125.1750\n",
      "Epoch 72/150\n",
      "2026/2026 [==============================] - 2s 801us/step - loss: 130.4665 - mse: 130.4664 - val_loss: 121.6566 - val_mse: 121.6566\n",
      "Epoch 73/150\n",
      "2026/2026 [==============================] - 1s 738us/step - loss: 126.8311 - mse: 126.8311 - val_loss: 118.0759 - val_mse: 118.0759\n",
      "Epoch 74/150\n",
      "2026/2026 [==============================] - 1s 733us/step - loss: 123.1359 - mse: 123.1359 - val_loss: 114.4453 - val_mse: 114.4453\n",
      "Epoch 75/150\n",
      "2026/2026 [==============================] - 1s 730us/step - loss: 119.4029 - mse: 119.4030 - val_loss: 110.7884 - val_mse: 110.7884\n",
      "Epoch 76/150\n",
      "2026/2026 [==============================] - 1s 737us/step - loss: 115.6437 - mse: 115.6437 - val_loss: 107.1120 - val_mse: 107.1120\n",
      "Epoch 77/150\n",
      "2026/2026 [==============================] - 1s 735us/step - loss: 111.8693 - mse: 111.8693 - val_loss: 103.4263 - val_mse: 103.4263\n",
      "Epoch 78/150\n",
      "2026/2026 [==============================] - 1s 731us/step - loss: 108.0899 - mse: 108.0898 - val_loss: 99.7429 - val_mse: 99.7429\n",
      "Epoch 79/150\n",
      "2026/2026 [==============================] - 2s 753us/step - loss: 104.3122 - mse: 104.3122 - val_loss: 96.0683 - val_mse: 96.0683\n",
      "Epoch 80/150\n",
      "2026/2026 [==============================] - 1s 728us/step - loss: 100.5470 - mse: 100.5469 - val_loss: 92.4063 - val_mse: 92.4063\n",
      "Epoch 81/150\n",
      "2026/2026 [==============================] - 1s 729us/step - loss: 96.7936 - mse: 96.7936 - val_loss: 88.7641 - val_mse: 88.7642\n",
      "Epoch 82/150\n",
      "2026/2026 [==============================] - 1s 728us/step - loss: 93.0591 - mse: 93.0591 - val_loss: 85.1390 - val_mse: 85.1390\n",
      "Epoch 83/150\n",
      "2026/2026 [==============================] - 1s 725us/step - loss: 89.3495 - mse: 89.3495 - val_loss: 81.5450 - val_mse: 81.5450\n",
      "Epoch 84/150\n",
      "2026/2026 [==============================] - 1s 732us/step - loss: 85.6643 - mse: 85.6643 - val_loss: 77.9823 - val_mse: 77.9823\n",
      "Epoch 85/150\n",
      "2026/2026 [==============================] - 1s 726us/step - loss: 82.0132 - mse: 82.0132 - val_loss: 74.4555 - val_mse: 74.4555\n",
      "Epoch 86/150\n",
      "2026/2026 [==============================] - 1s 738us/step - loss: 78.3993 - mse: 78.3993 - val_loss: 70.9680 - val_mse: 70.9680\n",
      "Epoch 87/150\n",
      "2026/2026 [==============================] - 1s 735us/step - loss: 74.8278 - mse: 74.8278 - val_loss: 67.5251 - val_mse: 67.5251\n",
      "Epoch 88/150\n",
      "2026/2026 [==============================] - 1s 729us/step - loss: 71.3011 - mse: 71.3010 - val_loss: 64.1347 - val_mse: 64.1347\n",
      "Epoch 89/150\n",
      "2026/2026 [==============================] - 2s 747us/step - loss: 67.8228 - mse: 67.8228 - val_loss: 60.7938 - val_mse: 60.7938\n",
      "Epoch 90/150\n",
      "2026/2026 [==============================] - 1s 727us/step - loss: 64.3979 - mse: 64.3979 - val_loss: 57.5099 - val_mse: 57.5099\n",
      "Epoch 91/150\n",
      "2026/2026 [==============================] - 2s 748us/step - loss: 61.0321 - mse: 61.0321 - val_loss: 54.2875 - val_mse: 54.2875\n",
      "Epoch 92/150\n",
      "2026/2026 [==============================] - 1s 731us/step - loss: 57.7290 - mse: 57.7290 - val_loss: 51.1322 - val_mse: 51.1322\n",
      "Epoch 93/150\n",
      "2026/2026 [==============================] - 2s 764us/step - loss: 54.4894 - mse: 54.4894 - val_loss: 48.0421 - val_mse: 48.0421\n",
      "Epoch 94/150\n",
      "2026/2026 [==============================] - 1s 735us/step - loss: 51.3218 - mse: 51.3218 - val_loss: 45.0304 - val_mse: 45.0304\n",
      "Epoch 95/150\n",
      "2026/2026 [==============================] - 1s 736us/step - loss: 48.2267 - mse: 48.2267 - val_loss: 42.0865 - val_mse: 42.0865\n",
      "Epoch 96/150\n",
      "2026/2026 [==============================] - 2s 787us/step - loss: 45.2103 - mse: 45.2103 - val_loss: 39.2313 - val_mse: 39.2313\n",
      "Epoch 97/150\n",
      "2026/2026 [==============================] - 2s 932us/step - loss: 42.2759 - mse: 42.2760 - val_loss: 36.4617 - val_mse: 36.4617\n",
      "Epoch 98/150\n",
      "2026/2026 [==============================] - 2s 942us/step - loss: 39.4258 - mse: 39.4258 - val_loss: 33.7760 - val_mse: 33.7760\n",
      "Epoch 99/150\n",
      "2026/2026 [==============================] - 2s 872us/step - loss: 36.6679 - mse: 36.6679 - val_loss: 31.1854 - val_mse: 31.1854\n",
      "Epoch 100/150\n",
      "2026/2026 [==============================] - 2s 812us/step - loss: 34.0050 - mse: 34.0050 - val_loss: 28.6957 - val_mse: 28.6957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "2026/2026 [==============================] - 2s 773us/step - loss: 31.4368 - mse: 31.4368 - val_loss: 26.3025 - val_mse: 26.3025\n",
      "Epoch 102/150\n",
      "2026/2026 [==============================] - 2s 766us/step - loss: 28.9709 - mse: 28.9709 - val_loss: 24.0086 - val_mse: 24.0086\n",
      "Epoch 103/150\n",
      "2026/2026 [==============================] - 2s 768us/step - loss: 26.6110 - mse: 26.6110 - val_loss: 21.8274 - val_mse: 21.8274\n",
      "Epoch 104/150\n",
      "2026/2026 [==============================] - 2s 771us/step - loss: 24.3590 - mse: 24.3590 - val_loss: 19.7545 - val_mse: 19.7545\n",
      "Epoch 105/150\n",
      "2026/2026 [==============================] - 2s 811us/step - loss: 22.2135 - mse: 22.2135 - val_loss: 17.7944 - val_mse: 17.7944\n",
      "Epoch 106/150\n",
      "2026/2026 [==============================] - 2s 785us/step - loss: 20.1835 - mse: 20.1835 - val_loss: 15.9464 - val_mse: 15.9464\n",
      "Epoch 107/150\n",
      "2026/2026 [==============================] - 2s 835us/step - loss: 18.2734 - mse: 18.2734 - val_loss: 14.2214 - val_mse: 14.2214\n",
      "Epoch 108/150\n",
      "2026/2026 [==============================] - 2s 793us/step - loss: 16.4899 - mse: 16.4899 - val_loss: 12.6204 - val_mse: 12.6204\n",
      "Epoch 109/150\n",
      "2026/2026 [==============================] - 2s 818us/step - loss: 14.8194 - mse: 14.8194 - val_loss: 11.1343 - val_mse: 11.1343\n",
      "Epoch 110/150\n",
      "2026/2026 [==============================] - 2s 897us/step - loss: 13.2681 - mse: 13.2681 - val_loss: 9.7733 - val_mse: 9.7733\n",
      "Epoch 111/150\n",
      "2026/2026 [==============================] - 2s 943us/step - loss: 11.8437 - mse: 11.8437 - val_loss: 8.5300 - val_mse: 8.5300\n",
      "Epoch 112/150\n",
      "2026/2026 [==============================] - 2s 810us/step - loss: 10.5460 - mse: 10.5460 - val_loss: 7.4193 - val_mse: 7.4193\n",
      "Epoch 113/150\n",
      "2026/2026 [==============================] - 1s 718us/step - loss: 9.3717 - mse: 9.3717 - val_loss: 6.4282 - val_mse: 6.4282\n",
      "Epoch 114/150\n",
      "2026/2026 [==============================] - 2s 819us/step - loss: 8.3267 - mse: 8.3267 - val_loss: 5.5624 - val_mse: 5.5624\n",
      "Epoch 115/150\n",
      "2026/2026 [==============================] - 1s 736us/step - loss: 7.4031 - mse: 7.4031 - val_loss: 4.8163 - val_mse: 4.8163\n",
      "Epoch 116/150\n",
      "2026/2026 [==============================] - 1s 726us/step - loss: 6.6007 - mse: 6.6007 - val_loss: 4.1850 - val_mse: 4.1850\n",
      "Epoch 117/150\n",
      "2026/2026 [==============================] - 2s 748us/step - loss: 5.9133 - mse: 5.9133 - val_loss: 3.6625 - val_mse: 3.6625\n",
      "Epoch 118/150\n",
      "2026/2026 [==============================] - 1s 726us/step - loss: 5.3429 - mse: 5.3429 - val_loss: 3.2458 - val_mse: 3.2458\n",
      "Epoch 119/150\n",
      "2026/2026 [==============================] - 1s 730us/step - loss: 4.8765 - mse: 4.8765 - val_loss: 2.9256 - val_mse: 2.9256\n",
      "Epoch 120/150\n",
      "2026/2026 [==============================] - 2s 763us/step - loss: 4.5071 - mse: 4.5071 - val_loss: 2.6929 - val_mse: 2.6929\n",
      "Epoch 121/150\n",
      "2026/2026 [==============================] - 2s 786us/step - loss: 4.2252 - mse: 4.2252 - val_loss: 2.5308 - val_mse: 2.5308\n",
      "Epoch 122/150\n",
      "2026/2026 [==============================] - 2s 792us/step - loss: 4.0189 - mse: 4.0189 - val_loss: 2.4308 - val_mse: 2.4308\n",
      "Epoch 123/150\n",
      "2026/2026 [==============================] - 2s 788us/step - loss: 3.8736 - mse: 3.8736 - val_loss: 2.3732 - val_mse: 2.3732\n",
      "Epoch 124/150\n",
      "2026/2026 [==============================] - 2s 789us/step - loss: 3.7751 - mse: 3.7751 - val_loss: 2.3473 - val_mse: 2.3473\n",
      "Epoch 125/150\n",
      "2026/2026 [==============================] - 2s 797us/step - loss: 3.7111 - mse: 3.7111 - val_loss: 2.3420 - val_mse: 2.3420\n",
      "Epoch 126/150\n",
      "2026/2026 [==============================] - 2s 792us/step - loss: 3.6737 - mse: 3.6737 - val_loss: 2.3483 - val_mse: 2.3483\n",
      "Epoch 127/150\n",
      "2026/2026 [==============================] - 2s 749us/step - loss: 3.6525 - mse: 3.6525 - val_loss: 2.3588 - val_mse: 2.3588\n",
      "Epoch 128/150\n",
      "2026/2026 [==============================] - 1s 727us/step - loss: 3.6409 - mse: 3.6409 - val_loss: 2.3705 - val_mse: 2.3705\n",
      "Epoch 129/150\n",
      "2026/2026 [==============================] - 1s 737us/step - loss: 3.6359 - mse: 3.6359 - val_loss: 2.3780 - val_mse: 2.3780\n",
      "Epoch 130/150\n",
      "2026/2026 [==============================] - 2s 754us/step - loss: 3.6323 - mse: 3.6323 - val_loss: 2.3860 - val_mse: 2.3860\n",
      "Epoch 131/150\n",
      "2026/2026 [==============================] - 1s 739us/step - loss: 3.6304 - mse: 3.6304 - val_loss: 2.3930 - val_mse: 2.3930\n",
      "Epoch 132/150\n",
      "2026/2026 [==============================] - 1s 732us/step - loss: 3.6297 - mse: 3.6297 - val_loss: 2.3972 - val_mse: 2.3972\n",
      "Epoch 133/150\n",
      "2026/2026 [==============================] - 1s 738us/step - loss: 3.6294 - mse: 3.6294 - val_loss: 2.4003 - val_mse: 2.4003\n",
      "Epoch 134/150\n",
      "2026/2026 [==============================] - 2s 754us/step - loss: 3.6295 - mse: 3.6295 - val_loss: 2.4017 - val_mse: 2.4017\n",
      "Epoch 135/150\n",
      "2026/2026 [==============================] - 2s 767us/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4054 - val_mse: 2.4054\n",
      "Epoch 136/150\n",
      "2026/2026 [==============================] - 2s 748us/step - loss: 3.6293 - mse: 3.6293 - val_loss: 2.4066 - val_mse: 2.4066\n",
      "Epoch 137/150\n",
      "2026/2026 [==============================] - 1s 734us/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4077 - val_mse: 2.4077\n",
      "Epoch 138/150\n",
      "2026/2026 [==============================] - 2s 743us/step - loss: 3.6289 - mse: 3.6289 - val_loss: 2.4079 - val_mse: 2.4079\n",
      "Epoch 139/150\n",
      "2026/2026 [==============================] - 2s 748us/step - loss: 3.6289 - mse: 3.6289 - val_loss: 2.4066 - val_mse: 2.4066\n",
      "Epoch 140/150\n",
      "2026/2026 [==============================] - 2s 754us/step - loss: 3.6291 - mse: 3.6291 - val_loss: 2.4093 - val_mse: 2.4093\n",
      "Epoch 141/150\n",
      "2026/2026 [==============================] - 2s 752us/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4073 - val_mse: 2.4073\n",
      "Epoch 142/150\n",
      "2026/2026 [==============================] - 1s 735us/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4076 - val_mse: 2.4076\n",
      "Epoch 143/150\n",
      "2026/2026 [==============================] - 2s 839us/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4076 - val_mse: 2.4076\n",
      "Epoch 144/150\n",
      "2026/2026 [==============================] - 2s 765us/step - loss: 3.6290 - mse: 3.6290 - val_loss: 2.4074 - val_mse: 2.4074\n",
      "Epoch 145/150\n",
      "2026/2026 [==============================] - 1s 737us/step - loss: 3.6288 - mse: 3.6288 - val_loss: 2.4079 - val_mse: 2.4079\n",
      "Epoch 00145: early stopping\n",
      "Test Score: 1.55 RMSE\n"
     ]
    }
   ],
   "source": [
    "model = Sequential((\n",
    "        Convolution1D(input_shape=(28, 173), \n",
    "                      kernel_size=4, activation=\"relu\", filters=4),\n",
    "        MaxPooling1D(),    \n",
    "        Dropout(0.1),\n",
    "        Convolution1D(kernel_size=4, activation=\"relu\", filters=4),\n",
    "        MaxPooling1D(),\n",
    "        Dropout(0.1),\n",
    "        Convolution1D(kernel_size=4, activation=\"relu\", filters=4),\n",
    "        #Convolution1D(kernel_size=4, activation=\"relu\", filters=4),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),\n",
    "        Dense(1, activation='linear'),\n",
    "    ))\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mse'])\n",
    "model.summary()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "val = model.fit(X_train, y_train, epochs=150, batch_size=5, validation_data=(X_test, y_test), callbacks = [es])\n",
    "pred = model.predict(X_test)\n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_values = [ math.log(v) for v in val.history['val_loss'] ]\n",
    "list_values2 = [math.log(v) for v in val.history['loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#blue should be above orange\n",
    "plt.plot(list_values)\n",
    "plt.plot(list_values2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis = 2)\n",
    "\n",
    "model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Convolution1D(input_shape=(167,1), \n",
    "                      kernel_size=4, activation=\"relu\", filters=16),\n",
    "        MaxPooling1D(),     # Downsample the output of convolution by 2X.\n",
    "        Dropout(0.2),\n",
    "        #Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu'),\n",
    "        Convolution1D(kernel_size=4, activation=\"relu\", filters=16),\n",
    "        Dropout(0.2),\n",
    "        #MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='linear'),\n",
    "        Dense(1, activation='linear'),# For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()\n",
    "test_size = int(0.2 * 2862)           # In real life you'd want to use 0.2 - 0.5\n",
    "#impact = data['impact_score']\n",
    "#test = data.drop(['date_key', 'impact_score'], axis = 1)\n",
    "#test = np.expand_dims(test, axis = 2)\n",
    "X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(test, impact, test_size = 0.3)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=25, validation_data=(X_test, y_test))\n",
    "pred = model.predict(X_test)\n",
    "#print('\\n\\nactual', 'predicted', sep='\\t')\n",
    "#for actual, predicted in zip(y_test, pred.squeeze()):\n",
    "#    print(actual.squeeze(), predicted, sep='\\t')\n",
    "#print('next', model.predict(q).squeeze(), sep='\\t')\n",
    "    \n",
    "testScore = math.sqrt(mean_squared_error(y_test,pred))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
